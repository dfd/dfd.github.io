<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>
    <channel>
      <title>Implicit Assumptions - Statistics</title>
      <link>https://dfd.github.io</link>
      <description>Dave Decker&#x27;s blog</description>
      <generator>Zola</generator>
      <language>en</language>
      <atom:link href="https://dfd.github.io/categories/statistics/rss.xml" rel="self" type="application/rss+xml"/>
      <lastBuildDate>Sun, 02 Mar 2025 00:00:00 +0000</lastBuildDate>
      <item>
          <title>Don&#x27;t Conform Your Data to the Model. Do the Opposite.</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Sun, 02 Mar 2025 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          
          
          <category><![CDATA[ab testing]]></category>
          <category><![CDATA[bad practices]]></category>
          <category><![CDATA[winsorizing]]></category>
          
          <link>https://dfd.github.io/dont-conform-data-to-the-model/</link>
          <guid>https://dfd.github.io/dont-conform-data-to-the-model/</guid>
          <description><![CDATA[In this post, I'll be responding to an idea I saw on LinkedIn.  See my guidelines on naming names.
The Problem
I recently saw a post on LinkedIn which serves as a nice example of how the null hypothesis mindset leads people to poor statistical decision making.  The author works a…]]></description>
          <content:encoded><![CDATA[<p>In this post, I'll be responding to an idea I saw on LinkedIn.  See my guidelines on <a href="https://dfd.github.io/guidelines-on-naming-names/">naming names</a>.</p>
<h2 id="the-problem">The Problem</h2>
<p>I recently saw <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.linkedin.com/posts/tyler-buffington-82a1a212a_not-all-outlier-handling-approaches-are-created-activity-7285002769231425536-mty1/">a post on LinkedIn</a> which serves as a nice example of how the null hypothesis mindset leads people to poor statistical decision making.  The author works at an AB testing as a service company.  Here is the post:</p>
<p><img src="https://dfd.github.io/dont-conform-data-to-the-model/post.png" alt="png" /></p>
<p>I agree with most of the points listed, but I want to focus on the suggestion to Winsorize the data and the plots at the bottom of the post.</p>
<p>The issue here is that the author wants to be able to compare the effect of a treatment on a continuous variable.  The problem arises presumably because the author wants to conduct a two-sample t-test on the observations against a null hypothesis, but the skewed distribution of the observables will mean the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Student&#x27;s_t-test#Assumptions">assumption of normality on the sampling distribution of the means</a> won't be valid at lower sample sizes, and hence will decrease the power of the test.</p>
<h2 id="the-proposed-solution">The Proposed Solution</h2>
<p><a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Winsorizing">Winsorizing</a> is used to clip the highest values to reduce the skewness in the observed data; so instead of a long tail, the empirical distribution now has a small peak at whatever the threshold was.</p>
<p>If we suppose a frequentist null hypothesis test is what they want, there are problems with using a t-test here:</p>
<ol>
<li>If we don't Winsorize the data, then the normality assumptions on the means do not hold until larger sample sizes (using <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Central_limit_theorem">CLT</a>).  So for sample sizes at which CLT does not apply (and what is that cutoff exactly?), the &quot;frequentist guarantees&quot; don't apply.</li>
<li>If we do Winsorize, then we've changed our data so the frequentist guarantees also don't apply.  How could they?  We're throwing out data so we won't have accurate estimates in the difference of our means.</li>
</ol>
<p>The post frames this as a bias/variance trade-off.  While this is better than insisting on unbiasedness above all else, it seems to ignore the typical reasons given for using the null hypothesis test in the first place: those guarantees.</p>
<p>Even if the method of analysis isn't a null hypothesis test, Winsorizing data still throws out information that might change our conclusion, had we not capped it.  The value at which to cap it will necessarily be arbitrary. If we cap it ahead of time, then we may not know where a good cap is; if we cap it after the fact, how do we know our bias hasn't influenced the choice of cap?</p>
<p>In the <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.linkedin.com/posts/ronnyk_abtest-activity-7254731531049488384-hh6J/">Ron Kohavi post linked to in the screenshot</a>, he discusses using the technique to reduce skewness and increase power.  But increased power to compare what? If the continuous metric is what you care about, it's no longer clear what you're comparing with the new capped metric. He even gives an example of reducing required sample size by 50% through capping, and suggests capping at different levels to compare results.  But how to decide which results to use?  Maybe he explains it in his course that he links to, but it's hard to imagine a principled approach to deciding, and providing multiple options risks cherry-picking a convenient false positive.</p>
<h2 id="the-alternative-solution">The Alternative Solution</h2>
<p>Instead of trying to make our data conform to the assumption of our model (the t-test relies on a model of the null hypothesis), we should do the opposite and find a model that describes the data well.  Looking back at the post in the screenshot, we see an illustrative sample provided by the author.  It looks like it would be well approximated by a log-distribution, and in my experience, this is quite common in revenue or customer lifetime value metrics.  I agree with the author that we should not simply log the observations, but we could model the samples as log-normal distributions.  Here are some benefits of doing so:</p>
<ol>
<li>We don't throw out information.  Assuming our measurements aren't flawed, we should try to retain all available information from these tests.  Driving higher revenue value is valuable to the company.  If we use the same Winsor cutoff on both samples, then whichever sample drove more users above the cutoff will be punished in the comparison. </li>
<li>The log-normal distribution naturally discounts the impact of outliers in the sample.  It does this by estimating its parameters on the log scale.  So we don't need to worry if one sample &quot;got lucky&quot; with a random draw far out in its tail; we won't be using its raw value in a mean calculation.</li>
<li>But this is different than just comparing the logged values.  The mean of a <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Log-normal_distribution">log-normal distribution</a> is a function of its log mean and standard deviation parameters: $\exp(\mu + \frac{\sigma^2}{2})$. So we can take into account how both the mean and spread of the distribution on the log scale impact the metric we car about in its original scale.</li>
<li>If the log-normal assumptions are a good match for the data, then it is a sample-efficient way to estimate the parameters.</li>
</ol>
<p>Using Bayesian methods, we can estimate the parameters and create derived quantities using the equation above and have a posterior distribution over the difference, which we can use to estimate the probability of the treatment having a higher mean revenue or whatever we would like to know.</p>
<p>Of course, this may not work well in all situations; not all businesses have revenue distributions that are well modeled by log-normals.  But perhaps other distributions will work well in those cases.  When we conform our modeling assumptions to the nature of our data, we can make better comparisons about the things we care about in sample efficient ways.  No need to cap.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: Bernoulli&#x27;s Fallacy</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          <category><![CDATA[Probability]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/clayton-bernoullis-fallacy/</link>
          <guid>https://dfd.github.io/clayton-bernoullis-fallacy/</guid>
          <description><![CDATA[Overview
Aubrey Clayton's Bernoulli's Fallacy is perhaps the most accessible introduction to Jaynes' version of &quot;logical probability.&quot;  But it goes further than that: Clayton provides the essential elements of the development and historical debates on the interpretation…]]></description>
          <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>Aubrey Clayton's <a rel="noopener nofollow noreferrer" target="_blank" href="https://aubreyclayton.com/bernoulli"><em>Bernoulli's Fallacy</em></a> is perhaps the most accessible introduction to Jaynes' version of &quot;logical probability.&quot;  But it goes further than that: Clayton provides the essential elements of the development and historical debates on the interpretation of probability and statistical inference through Fisher; gives numerous examples of how frequentist methods can fail; connects these issues to the academic research replication crisis; and suggests a way forward.</p>
<h2 id="interpreting-probability">Interpreting probability</h2>
<p>After motivating self-reflection within the reader through some example probability problems, Clayton enumerates the most common interpretations, including ancient notions of chance, frequentist, subjective, and axiomatic, while dissecting the appeal and problems of each one.  Finally he presents &quot;probability as logic,&quot; which will serve as author's point of view throughout the book.</p>
<p>To summarize, <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Probability_axioms#Kolmogorov_axioms">Kolmogorov's axiomatic probability</a> had put probability on a rigorous foundation, but did not solve the interpretation problem; all modern interpretations could adhere to the axioms.  The logical point of view then evolved with contributions from <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/A_Treatise_on_Probability">Keynes</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://arxiv.org/pdf/0804.3173">Jeffries</a>; <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Cox%27s_theorem">Cox showed</a> how to break probability free from the notion of proportions and frequencies and to represent plausibilities while still adhering to the axioms; and Jaynes completed the evolution with his book, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712"><em>Probability Theory: The Logic of Science</em></a>.  Clayton walks the reader quickly through this evolution, and the final intuition is as follows:</p>
<p>Probability is an extension of logical deduction.  While logical deduction is process for calculating implications of interest from propositions that have true or false values, probability allows us to do the same with propositions of uncertain truth values.  In fact, as Clayton relays, probability reduces to logical deduction when working exclusively with probabilities of 0s and 1s.  Another key point is that all probability is conditional; we are always determining probability from <em>some</em> information, and that two rational people with the same information will calculate the same probabilities.  Jaynes viewed probability as a way to process information, not as proportions or long run frequency.  Finally, probability is epistemology, not ontology.  That is, probability represents our state of knowledge rather than reality itself, and confusing our uncertainty with reality is the <em>mind projection fallacy</em>.</p>
<p>Following this, Clayton walks the reader through some classic probability problems using this view of the probability, pointing out along the way that there is no need in any of them to define probability as long run frequency.</p>
<h2 id="bernoulli-s-blunder">Bernoulli's blunder</h2>
<p>Clayton next turns our attention to the history of probability and statistical inference, starting with Bernoulli.  In his telling of the history, Bernoulli was the first to make a mistake that would haunt statistics to the present day: concluding that a statistic derived from a particular sample is probably close to its true value for any sampled value, rather than the correct conclusion that a yet-to-be-drawn sample statistic will probably be close to the true value for any true value.  Clayton explains that Bernoulli's fallacy came from confusing the sampling probability (the likelihood), with the probability of a hypothesis.  These two are related through Bayes theorem, so Bernoulli needed to include more information to complete the calculation for his claim, and it's not necessarily true.</p>
<p>Clayton then goes on to illustrate <em>why</em> these two conclusions are not equivalent, despite appealing to people's intuition.  Specifically, he arms the reader with a particular framework to define hypotheses, assign prior probabilities, assign the sampling probability (or likelihood), calculate the &quot;pathway probability&quot; (the unnormalized posterior of multiplying the previous two numbers), and finally the posterior probability for each hypothesis through normalization.  This is done in a similar manner to Allen Downey's <a rel="noopener nofollow noreferrer" target="_blank" href="https://docs.google.com/drawings/d/1dHkIa-RdmnLPze84gkcmYflWwA7xwtwFhXPs_Rd0oRM/edit">Bayesian worksheet</a> (example use case <a rel="noopener nofollow noreferrer" target="_blank" href="https://allendowney.blogspot.com/2017/02/a-nice-bayes-theorem-problem-medical.html">here</a>).</p>
<p>To end the chapter, Clayton covers <em>base rate neglect</em>, first presenting the canonical medical test example, followed by the <em>prosecutor's fallacy</em> with the tragic story of <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Sally_Clark#Conviction_for_murder">Sally Clark's conviction</a>, before explaining that these are in fact the same error, confusing $P[A | B]$ for $P[B | A]$; the same one made by Bernoulli himself. </p>
<h2 id="wrong-turn">Wrong turn</h2>
<p>The next chapter covers the bridge from applying statistics in the hard sciences to softer ones.  This can be traced back to Gauss and Laplace, who developed and justified the method of least squares regression in astronomy, and Quetelet, who looked to apply the methods to the development of &quot;social physics.&quot; Around this time, the common interpretation of probability also starts to shift toward frequency.  This is due to both empirical data seeming to adhere to probability distributions, and because of academics questioning the origins of prior probabilities.</p>
<p>Chapter Four covers Galton, Pearson, and Fisher's influence on statistics.  These three advanced frequentist statistical theory and applied it to comparisons of groups of people, for the purposes of eugenics.  In this pursuit, they wanted to develop statistical methods that could be seen as &quot;objective&quot; to bolster their claims.  Clayton wrote <a rel="noopener nofollow noreferrer" target="_blank" href="https://nautil.us/how-eugenics-shaped-statistics-238014/">a related piece in 2020</a> that overlaps a bit with this chapter.</p>
<p>Chapter Five starts with an illustrative dialogue between a fictitious student and frequentist bot.  The humorous conversation conveys all the typical confusions one encounters while trying to navigate the logic and interpretation of frequentist methods.  It sits in stark contrast to the consistency of the previously presented Bayesian analyses.</p>
<p>Clayton then turns his attention back to Fisher, and the development of disparate tests for various situations, presenting <a href="https://dfd.github.io/mcelreath-statistical-rethinking/#connecting-methods-to-science">a flow chart similar to the one presented by McElreath</a>.  But Clayton goes beyond this sight gag in his critique of frequentist methods, calling out issues in both theory and practice.  He also cites evidence that Fisher, despite his vitriol toward Bayesian methods, actually drew inspiration from them in his derivation of maximum likelihood and grew a soft spot for them in his later writings.  And finally, his zig zagging during the development of <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Fiducial_inference">fiducial inference</a> eventually led him to argue against his own earlier concepts of inference since the new paradigm would not work for his new paradigm, which was trying to draw Bayesian conclusions without use of priors.</p>
<p>The chapter concludes with nine example problems that require Bayesian reasoning to obtain sensible solutions and for which frequentist methods fail miserably.</p>
<h2 id="frequently-wrong">Frequently wrong</h2>
<p>Clayton then turns his attention to the replication crisis in academia, and particularly in the social and medical sciences.  After citing objections to null hypothesis testing over time by those who recognized its irrelevance and misuse, he presents the theory and evidence of why so many modern studies have failed to replicate. Then he recounts the curious case of Daryl Bem, who turned from ESP skeptic to <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Daryl_Bem#%22Feeling_the_Future%22_controversy">publishing &quot;statistically significant&quot; research</a> on the subject.  Bem's claims were implausible to most and yet used standard methods of analysis.  Clayton then shares rumors that Bem had pulled this off as metacommentary on research practices.</p>
<p>In the final chapter, Clayton proposes reform to research practices.  It will come as no surprise that the proposal includes abandoning frequentist methods, including null hypothesis testing, and instead embracing Bayesian methods.  He even goes so far as to suggest that universities dissolve their statistics departments, since Bayesian inference relies on only one theorem.  That seems a bit far to me, as Bayesian models and computation can become quite complex, and there are still regular developments in Bayesian methods and practice.  Plus, even if researchers across fields used Bayesian methods, many won't master them at an advanced level, so it's helpful to have some centralized academics who can help or guide empirical work.</p>
<h2 id="recommendation">Recommendation</h2>
<p>This is a unique book in its scope: a straightforward presentation of Logical Probability; an overview of the historical debates; hands-on examples of where frequentist methods fail and how Bayesian methods help; and insights and proposed remedies related to the academic replication crisis.  It's not as math-heavy or dense as a textbook, but there is some math sprinkled throughout.  The notation includes probability, summations, factorials, binomial coefficients, and exponential functions.  So while I think the concepts are well-explained, a reader will want to feel comfortable with that notation and related concepts to get the most out of it.</p>
<p>Recommended for those looking for an introduction to Jaynes or the &quot;statistics wars.&quot;  I had first read this over two years ago, and it was fun to revisit and flip through.  If you feel inspired to move on to Jaynes' book itself afterward, Clayton also has a <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/watch?v=rfKS69cIwHc&amp;list=PL9v9IXDsJkktefQzX39wC2YG07vw7DsQ_">video lecture series</a> that covers the book from start to finish.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: Statistical Rethinking</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Wed, 24 Jul 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/mcelreath-statistical-rethinking/</link>
          <guid>https://dfd.github.io/mcelreath-statistical-rethinking/</guid>
          <description><![CDATA[<p>I'll start with the conclusion: McElreath's <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/036713991X/ref=pd_sim_d_sccl_3_5/141-3103872-8081264?psc=1">Staistical Rethinking</a> is my favorite textbook.  In the rest of this post I'll explain why and offer advice on how to get the most out of this book.</p>
]]></description>
          <content:encoded><![CDATA[<p>I'll start with the conclusion: McElreath's <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/036713991X/ref=pd_sim_d_sccl_3_5/141-3103872-8081264?psc=1">Staistical Rethinking</a> is my favorite textbook.  In the rest of this post I'll explain why and offer advice on how to get the most out of this book.</p>
<span id="continue-reading"></span><h2 id="my-background-before-reading-this-book">My background before reading this book</h2>
<p>By the time I started reading the first edition of this book in late 2017, I was already leading a sizable data science and analytics team and had a masters degree in statistics.  For that degree, most courses used frequentist methods, although I also took the one Bayesian statistics course that was offered.  So I wasn't exactly a beginner, and yet I still claim that I learned at least as much from this book as I did from that degree program.  I later used this book to teach a course at my old company, so I've also seen how well it works for those with non-stats backgrounds.</p>
<p>In this post, I'll refer to the 2nd edition.</p>
<h2 id="connecting-methods-to-science">Connecting methods to science</h2>
<p>McElreath starts off by presenting the <a rel="noopener nofollow noreferrer" target="_blank" href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-01?slide=28">typical maze of statistical tests</a> students are presented with in a statistics curriculum, and explains why this often causes confusion.  He then goes on to argue that these tests are not enough for research because they are difficult to adapt to unique contexts and fail in unpredictable ways.</p>
<p>Chapter one continues on to describe why deductive falsification, which is often used as the justification for applying frequentist null hypothesis tests, is impossible in most scientific contexts.  This is because &quot;many models correspond to the same hypothesis, and many hypotheses correspond to a single model,&quot; which &quot;makes strict falsification impossible.&quot;  This idea is <a rel="noopener nofollow noreferrer" target="_blank" href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-01?slide=34">illustrated here</a>, which shows that many processes are consistent with stated hypotheses, and many processes may be consistent with a single statistical model. </p>
<p>Measurement errors and continuous hypotheses also make falsification challenging in practice, and because of these issues, falsification is consensual rather than logical.  That is, scientists debate the merits of the evidence and come to a consensus over time.</p>
<p>The remainder of the chapter is spent outlining the methods for doing better science that are covered in the book: Bayesian data analysis, model comparison, multilevel models, and graphical casual models.</p>
<h2 id="illustrative-explanations">Illustrative explanations</h2>
<p>I think people like this book so much because the explanations bring the concepts to life, giving the reader an intuition that goes well beyond the definitions.  These are given throughout the book, but here are two examples that have always stuck with me:</p>
<p>Chapter 2 contains perhaps the best illustration of a likelihood function that I've seen.  It does so by presenting a simple case, in which a bag contains four marbles, each of which is blue or white.  Three marbles are drawn with replacement, and their colors are the observed data.  The unknown parameter of interest, $p$, is the proportion of marbles that are blue, so this sets up a discrete parameter, discrete outcome case.  The likelihood is calculated by enumerating all of the possible ways data can be generated for each conjecture (each possible value of parameter $p \in \{ 0,\frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1 \}$ ) by taking three draws from the bag, and highlighting which possible outcomes match the observed outcome.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2022-lecture-02?slide=5">You can see the example play out over consecutive slides starting here.</a>  The lesson is simple: the likelihood is the relative number of ways that the observed data can occur for each value of the parameter.  While such literal counting is only feasible in the discrete/discrete case, it gives the right intuition for continuous cases as well. </p>
<p>Another explanation that stands out is for KL Divergence and demonstrating why it is not symmetric like a distance metric.  The example given concerns predicting whether we will land on land or water, using Earth to set expectations for landing on Mars and then using Mars to set expectations for Earth.  Mars' surface is almost entirely land (we count the ice caps as water on 1% of the surface for the sake of the example), while Earth's surface is mostly water but with a significant amount of land.  So if you randomly land on Mars while using Earth to guide expectations, you'll be a little surprised but not shocked to (almost certainly) touch down on land.  If you randomly land on Earth using Mars to guide your expectations, there's a good chance you'll land in water, which will be shocking if you were picturing the 1% water on Mars.  The extra surprise means that using the proportions of land and water on Mars to guide expectations about Earth produces larger KL divergence than vice versa.</p>
<h2 id="insightful-plots">Insightful plots</h2>
<p>A strength of this book is the plots used for model checking.  It conveys good habits for plotting relationships and comparing the model to data.  As every experienced practitioner knows, plotting adds valuable information far beyond what model fit metrics can provide alone.</p>
<h2 id="bayesian-philosophy">Bayesian philosophy</h2>
<p>While he points out advantages of Bayesian methods over frequentist ones, McElreath avoids making sweeping claims for Bayesian methods.  In fact, he argues that no statistical approach by itself is sufficient, and there's a healthy humility as he discusses the modeling process.</p>
<p>The book takes the &quot;logical&quot; view of probability (<a rel="noopener nofollow noreferrer" target="_blank" href="https://bayes.wustl.edu/etj/prob/book.pdf">see here</a> for a long explanation), often associated with Jaynes, and also does a nice job delineating epistemology from ontology.  The simulation of tadpoles in tanks and the role of hyperparameters in the <em>Models with Memory</em> chapter is a good example of clarifying this difference, as is the discussion of the i.i.d. assumption and Jaynes' <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Mind_projection_fallacy"><em>mind projection fallacy</em></a>.</p>
<h2 id="working-up-to-multilevel-modeling-and-beyond">Working up to multilevel modeling and beyond</h2>
<p>The book is sensibly organized to gradually build up to multilevel models with partial pooling, which is one of the Bayesian super powers.  Along the way, the book introduces essential and useful topics for a practicing statistician, including spurious association, masked relationships, confounding variables, interactions, and causal DAGs.  It also covers modeling with Gaussian, binomial, Poisson, categorical, zero-inflated, and ordinal response variables.</p>
<p>It consistently presents the formal specification of each model before moving onto the code.  It's a great practice to get into and helps to clarify and communicate a model's intention.  Plus, modern Bayesian libraries use declarative model syntax that closely aligns with this format, bridging conceptual understanding and assumptions to code.</p>
<p>Continuing a bit beyond multilevel models to some other advanced topics, the book includes covariance, Gaussian Processes, missing data, measurement error, and instrument variables.  After I used it to teach the class at work, our team members were able to tackle some pretty sophisticated problems where quantifying uncertainty or leveraging a flexible model structure were crucially important.</p>
<h2 id="traditional-topics-missing">Traditional topics missing</h2>
<p>One thing the book skips over, which is normally covered in an introductory Bayesian course, is solving simple problems with conjugate priors analytically.  In practice, it's only applicable to a very small set of problems.  But I think doing the math by hand for a few examples will help some learners better understand what the MCMC sampling is approximating.  I don't think this belongs in McElreath's book; it's just a suggestion to find another resource to try that out separately, perhaps before reading this book, or sometime around chapter 3 or 4.</p>
<p>Continuing along those lines, it's generally not a math heavy book.  The math is usually presented, but not required, since most problems are solved computationally.  I think this serves the book and its scope well, but for moving on to more advanced material afterwards, I think a mathematical statistics course or book would be helpful to supplement.  <a href="https://dfd.github.io/probabilistic-machine-learning/">Probabilistic Machine Learning</a> (PML) actually covers this material quite well (and also includes a section on conjugate prior models).  Having that background ahead of time would probably help comprehend this book a bit better, but I don't think it's necessary.</p>
<p>Another traditional topic is implementing your own <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis Hastings</a> samplers.  This is again not particularly practical with modern libraries for Bayesian inference that do the hard work for you (and tend to use <a rel="noopener nofollow noreferrer" target="_blank" href="https://arxiv.org/abs/1701.02434">Hamiltonian Monte Carlo</a>).  I don't necessarily think a reader needs to supplement with this, although diving into the sampling algorithms may help with diagnosing problems and understanding when reparameterization is helpful in more complex models.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/book2.html">PML2</a> covers MCMC sampling.</p>
<p>And finally another traditional topic not found in the book is <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Jeffreys_prior">Jeffreys priors</a>.  These are intellectually interesting, but rarely use in applied work.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/book2.html">PML2</a> covers Jeffreys priors.</p>
<h2 id="evolving-editions">Evolving editions</h2>
<p>As mentioned, I originally read the first edition of the book, which was already great.  I also bought a copy of the second edition, which added several more topics, but the main difference may be how it stresses causal diagrams (DAGs) throughout.  The forthcoming 3rd edition <a rel="noopener nofollow noreferrer" target="_blank" href="https://elevanth.org/2024_01_02_third_edition.html">will apparently feature more on the Bayesian workflow</a>.</p>
<h2 id="recommendation">Recommendation</h2>
<p>McElreath thoughtfully weaves together philosophical, conceptual, and computational considerations through motivating examples and insightful illustrations.  If you want to learn (or relearn) statistics in a way that will clarify your thinking and prepare you for a wide variety of modeling problems, this is a must read.  I'll describe how I went through the book, which is what I'd recommend to get the most out of it.</p>
<p>When I read this book, I wanted to make a collection of the code examples from it in Python.  It's such a popular book that the author's code, which uses an <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.rdocumentation.org/packages/rethinking/">R package the author created</a>, has been translated to several other probabilistic programming languages (PPLs).  I was using PyMC3 at the time (now replaced by PyMC version 5), so I grabbed an available translation for it, and started making my own notebooks.  However, I noticed there were some differences between results from the resource I found and the book's results.  I think doing the full translation from scratch myself would have been tedious, but figuring out how to correct some examples was helpful.</p>
<p>By the way, I don't think which framework you choose is that important.  In fact, when I taught the class, I stressed that our chosen framework was somewhat arbitrary and that those will change over time; what's important is learning the concepts and practicing with modern MCMC samplers.  In practice, I've used <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.pymc.io/welcome.html">pymc</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://num.pyro.ai/en/stable/index.html">numpyro</a>, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://mc-stan.org/">stan</a> for various projects.</p>
<p>I also reproduced every plot in the book with code.  Much of McElreath's plotting code was not published in the book, so figuring that out also helped my understanding of the mounds of samples the models were generating.  Plotting is so important for model improvement.</p>
<p>And finally, as preparation for teaching a class with the book, I selected a small number of problems to do from each chapter.  I recommend choosing at least one conceptual question, and one computational question (and ideally one from each major topic) from each chapter to help get hands on experience with the material in the book.  McElreath also publishes homework problems and solutions from his course, which is a good option.</p>
<p>At the end of it, I had an amazing set of code examples in a framework I wanted to use, along with a solid understanding of how to approach a wide variety of problems.  This book on its own can bring an active learner up to an intermediate level.  It's also great conceptual preparation for more dense books like <a href="https://dfd.github.io/probabilistic-machine-learning/">PML</a> or <a rel="noopener nofollow noreferrer" target="_blank" href="http://www.stat.columbia.edu/~gelman/book/">BDA3</a>.</p>
<p>Take your time with this one.  You'll get out of it what you put into it.</p>
<h2 id="extra-resources">Extra Resources</h2>
<p>McElreath publishes <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/@rmcelreath/playlists">lectures for the book here</a>.  You can just find the latest (or whichever one matches your book's edition) and follow along.  Or you can look for the latest class <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/rmcelreath?tab=repositories">github repo</a> with lectures and assigned homework and solutions here (specific example <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/rmcelreath/stat_rethinking_2024">here</a>)</p>
<p>I mentioned translations for various PPLs.  Here are some for the 2nd edition of the book.</p>
<ul>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/pymc-devs/pymc-resources/tree/main/Rethinking_2">Translation for PyMC.</a></li>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://fehiepsi.github.io/rethinking-numpyro/">Translation for numpyro.</a></li>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://vincentarelbundock.github.io/rethinking2/">Translation for rstan.</a></li>
</ul>
<p>I haven't seen a complete translation for pystan, but stan itself is a language, so it should be relatively straightforward to convert to pystan for those with python data experience.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: The Theory That Would Not Die</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/mcgrayne-the-theory-that-would-not-die/</link>
          <guid>https://dfd.github.io/mcgrayne-the-theory-that-would-not-die/</guid>
          <description><![CDATA[Overview
I had put off reading Sharon Bertsch McGrayne's The Theory That Would Not Die for a long time, but I'm glad I finally got to it late last year.  Although I had previously read some books with &quot;history of statistics&quot; content, like Bernoulli's Fallacy and The Lad…]]></description>
          <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>I had put off reading Sharon Bertsch McGrayne's <a rel="noopener nofollow noreferrer" target="_blank" href="https://yalebooks.yale.edu/book/9780300188226/the-theory-that-would-not-die/">The Theory That Would Not Die</a> for a long time, but I'm glad I finally got to it late last year.  Although I had previously read some books with &quot;history of statistics&quot; content, like <a rel="noopener nofollow noreferrer" target="_blank" href="https://aubreyclayton.com/bernoulli">Bernoulli's Fallacy</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Lady-Tasting-Tea-Statistics-Revolutionized/dp/0805071342">The Lady Tasting Tea</a>, this book introduced a large number of new stories I hadn't heard before about the development and application of Bayesian methods as well as its detractors.</p>
<p>While <em>Bernoulli's Fallacy</em> primarily focused on a narrow lineage of academic thought for and against Bayesian methods up through Fisher, and <em>The Lady Tasting Tea</em> ran through all the big names in academic statistics in the 20th century without an overarching theme, McGrayne follows the thread of Bayesian statistics from its early development all the way through to its modern usage.  And whereas <em>Bernoulli's Fallacy</em> advocates and explains the Bayesian philosophy, McGrayne is focused on telling the full story with historical context, highlighting key figures, turning points, and historical use cases.</p>
<p>The book's long subtitle presents a nice synopsis: &quot;How Bayes' rule cracked the Enigma code, hunted down Russian submarines &amp; emerged triumphant from two centuries of controversy.&quot;</p>
<h2 id="new-to-me">New to me</h2>
<p>I learned something new in just about every chapter.  I was even embarrassed to realize I didn't know the motivating example of Bayes' original paper, using a Beta distribution to estimate the unknown location of a ball on a square table, using only information about whether other randomly thrown balls had landed to the left or right of it.  That really is clever, given the historical context.</p>
<p>There was some overlap between the sections on Laplace and Fisher from the aforementioned books, but many of the other stories I either hadn't seen at all or hadn't seen in as much depth.</p>
<p>For instance, I wasn't aware that Bayesian shrinkage had been imbued into actuarial practice as <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Credibility_theory">Credibility theory</a>, unbeknownst to many actuaries.  Or that there were proponents at Harvard Business School starting in the late 1950s.  There were many such stories that were completely new to me.</p>
<h2 id="world-war-ii">World War II</h2>
<p>I had long been familiar with Turing's work at Bletchley Park for cracking the Enigma Code with <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Banburismus">Banburismus</a> using early electro-mechanical computers, known as <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Bombe">bombes</a>, which enabled the Allies to navigate the Atlantic safely and evade U-boats.  McGrayne dedicates most of a chapter to this, as well as some other applications of Bayes to the war effort.  Helping to defeat the Nazis is a nice notch on Bayes' belt.</p>
<h2 id="nate-silver-s-prequel">Nate Silver's Prequel</h2>
<p>I hadn't previously heard the story of John Tukey, a well known statistician of Princeton and Bell Labs, joining NBC News to help call elections for 18 years with statistical methods, starting in 1960.  Some who helped with the programming claimed they were using empirical Bayes, although Tukey never admitted to using Bayesian methods.  The author shares speculation that Tukey's involvement with Cold War era national security, where Bayes' rule was widely used, may have been part of his reluctance to credit the method.</p>
<h2 id="biggest-surprise">Biggest surprise</h2>
<p>The most surprising thing I learned in this book was that the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman Filter</a> was <em>not</em> motivated by Bayes rule, and in fact Kalman disliked Bayesian methods.  I had previously assumed it was intended as an analytical solution to updating a Bayesian model with conjugate priors.  In any case, it does indeed have a <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Kalman_filter#Relationship_to_recursive_Bayesian_estimation">Bayesian interpretation</a>, like other filtering methods for localization.</p>
<h2 id="nitpicks">Nitpicks</h2>
<p>While McGrayne is not a mathematician or statistician herself, she seemed to have a pretty good handle on the Bayesian framework at a conceptual level.  However, there were two things that bothered me.  The first is that Bayes' rule is not a theory; it's a <em>theorem</em> that can be proven from the axioms of probability.  I think perhaps calling it a &quot;theory&quot; may have been in reference to using it for statistical inference, but it makes for a confusing book cover when &quot;theory&quot; appears to reference to Bayes' rule itself.</p>
<p>Second (and related to the first point), I kept waiting for her to state that Bayes' rule is <em>not at all controversial</em> in probability theory; it's only been debated in its application to estimating parameters in statistical models, where Bayesian practitioners apply probability to model parameters just as they would to any other uncertain quantity of interest.  I would also separate out the philosophical disagreements on the interpretation of probability from Bayes' rule itself.  Frequentist statisticians don't doubt the validity of Bayes' rule; it's a particular application of it they doubt.</p>
<h2 id="recommendation">Recommendation</h2>
<p>What I like about books like this and <em>Bernoulli's Fallacy</em> is that you can see the path dependence of history so clearly.  It's not as if people wake up each day and reevaluate what statistical methods they plan to use.  These things carry momentum and inertia with them, and wrong turns can last a long time.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Planck%27s_principle">&quot;Science progresses one funeral at a time,&quot;</a> as it were.</p>
<p>I've only covered a handful of stories told within this book; I was pleasantly surprised at its breadth and selectively chosen depth.  While it avoids the technical weeds, <em>The Theory That Would Not Die</em> does the most thorough job surveying the history of thought and application of Bayesian statistics as I've seen.  So if that interests you, I recommend it.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: Information Theory, Inference, and Learning Alogrithms</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Machine Learning]]></category>
          <category><![CDATA[Statistics]]></category>
          <category><![CDATA[Information Theory]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/mackay-itila/</link>
          <guid>https://dfd.github.io/mackay-itila/</guid>
          <description><![CDATA[Overview
David MacKay's classic textbook Information Theory, Inference, and Learning Algorithms was first published in 2003, and yet it holds up very well over 20 years later, seamlessly weaving together concepts across compression, noisy-channel coding, Bayesian statistics, and …]]></description>
          <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>David MacKay's classic textbook <a rel="noopener nofollow noreferrer" target="_blank" href="http://www.inference.org.uk/mackay/itila/book.html">Information Theory, Inference, and Learning Algorithms</a> was first published in 2003, and yet it holds up very well over 20 years later, seamlessly weaving together concepts across compression, noisy-channel coding, Bayesian statistics, and neural networks.  By the end, a diligent reader will have a very good understanding of fundamentals that are vital to more advanced work in Bayesian statistics and deep learning.</p>
<h2 id="a-unique-perspective">A unique perspective</h2>
<p>MacKay uniquely presents the material, drawing parallels between seemingly distinct topics.  As he states in Chapter 2, &quot;One of the themes of this book is  that data compression and data modeling are one and the same, and that they should both be addressed... using inverse probability.&quot;</p>
<p>One chapter presented a unique idea for model comparison using Occam's Razor.  He presents an &quot;Occam's Factor&quot; which &quot;provides the ratio of the poterior accessible volume of $\mathcal{H}_i$'s hypothesis space to the priori accessible volume, or the factor by which $\mathcal{H}_i$'s hypothesis space collapses when the data arrive.&quot;  It's logarithm &quot;is a measure of the amount of information we gain about the model's parameters when the data arrive.&quot;</p>
<p>So models with many parameters and few constraints will be penalized by a stronger Occam's factor than a simpler model because they had too much &quot;wiggle room&quot; to overfit the data.  It's similar in spirit to AIC or BIC, except it can be applied to the full distributions over parameters, as opposed to just the maximium likelihood point estimation models.  Later he goes so far as to say it can be used as a substitute for validation sets in Bayesian machine learning.  I'm not sure I'm ready to commit to that, but I'm interested to explore it further.  It seemes like it could be gamed.  For instance, if you know of an overfit model and its parameter values for a set of training data, and you then set your priors to be equal to these, then the parameters wouldn't change and the Occam's Factor of that model would be small.  So this seems to be relying on restricting oneself to truly using priors before seeing the data.</p>
<h2 id="diverse-topics">Diverse topics</h2>
<p>As if the diversity of high level topics weren't enough, perhaps the most interesting sequence in the book are chapters 18 and 19.  He applies the tools from the Information Theory section first to crossword puzzles, and then presents a simplified version of the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Cryptanalysis_of_the_Enigma">Enigma Code breaking done at Bletchley Park</a>.  Finally, he ends the section with an information theoretic point of view on the benefits of sexual reproduction with recombination using both analytical and simulation methods.</p>
<h2 id="relevance-to-nlp-and-deep-learning">Relevance to NLP and deep learning</h2>
<p>Even in Claude Shannon's original paper that introduced information theory, <a rel="noopener nofollow noreferrer" target="_blank" href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">A Mathemetical Theory of Communication</a>, he presented language models.  And if one studies natural language processing today, it's still filled with concepts from information theory.  Although this book came out 14 years before the transformer architecture was introduced in <a rel="noopener nofollow noreferrer" target="_blank" href="https://research.google/pubs/attention-is-all-you-need/">Attention is All You Need</a>, it will give readers an outstanding foundation from which to study modern approaches.  I worry too many practitioners just take the shortest path to the latest architectures without building the requisite knowledge first.</p>
<p>I also like how he distills neural networks into their architecture, activity rule, and learning rule.  The architecture specifies the variables and relationships within the network; the activity rule details how the outputs of neurons change in response to each other; the learning rule defines how to update weights during training.</p>
<p>One other topic I found particularly interesting was on the information capacity of a single neuron (and relating it back to VC dimension), as well as the capacity of Hopfield networks.  Walking through these analyses gives the reader building blocks for reasoning about the capacity of larger, more complex networks.</p>
<p>While it's true that the Bayesian methods for which MacKay advocates are not the dominate paradigm used in deep learning today, the probabilistic perspective is broad enough to interpret parameters with point estimates.  Regularization and drop out also have Bayesian interpretations.</p>
<h2 id="comparison-to-frequentist-methods">Comparison to frequentist methods</h2>
<p>MacKay mostly focuses on presenting the material at hand, but occasionally contrasts to frequentist methods.  One example comes in Chapter 24, where he discusses the estimators for $\sigma$ in a Gaussian distribution.  As he states:</p>
<p>Given data $D = \{x_n\}_{n=1}^N$, and 'estimator' of $\mu$ is</p>
<p>$$
\bar{x} \equiv \sum_{n=1}^N x_n / N,
$$
and two estimators of $\sigma$ are:
$$
\sigma_N = \sqrt{\frac{\sum_{n=1}^N (x_n - \bar{x})^2}{N}}
\;\text{and}\;
\sigma_{N-1} = \sqrt{\frac{\sum_{n=1}^N (x_n - \bar{x})^2}{N-1}}
$$</p>
<p>He discusses how they invent estimators in the frequentist paradigm and choose the one that best meets some criteria of sampling properties.  After pointing out that there is no clear principle for deciding which criterion to use, and given most criteria, there's no systematic way to produce an optimal estimator, he then explains the frequentist interpretation of these estimators.  The estimator $(\bar{x}, \sigma_N)$ is the maximum likelihood estimator, but $\sigma_N$ is biased.  That is, averaging over repeated sampling, $\sigma_N$ will not equal to $\sigma$.  The $\sigma_{N-1}$ version is unbiased.</p>
<p>In contrast, the Bayesian view arrives at these quantities as maximum a posteriori estimates of $\sigma$ with different conditioning.  The maximum of $P(\sigma | D)$ is at $\sigma_{N-1}$ and the maximum of $P(\sigma | D, \mu = \bar{x})$ is $\sigma_N$, using uninformative priors.  In other words, $\sigma_{N-1}$ is when we are jointly estimating our uncertainty in $\mu$ and $\sigma_N$ is when we hold $\mu$ fixed at $\bar{x}$.  There are nice supporting visuals on page 321.</p>
<p>One of the more lively and entertaining chapters is 37, <em>Bayesian Inference and Sampling Theory</em>.  In this short chapter, he offers very simple examples to demonstrate that frequentist methods calculate unhelpful quantities to the decision at hand or are sensitive to irrelevant information.</p>
<h2 id="nitpick">Nitpick</h2>
<p>Perhaps the one thing I didn't care for in the book was the early insistence on approximating $x!$ and ${n \choose x}$, which seemed like a bit of an unnecessary distraction.  This did not last long, however.</p>
<h2 id="problems">Problems</h2>
<p>The book offers a lot of problems, each with a difficulty rating between 1 and 3 next to it.  Some of the problems are quite challenging.  I recognized one problem as being a slight reframing of a <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/William_Lowell_Putnam_Mathematical_Competition">Putnam</a> A6 question, and it was only marked with a difficulty of 2.  Luckily, he also marked a small portion as recommended problems, and these problems were thoughtfully chosen to reinforce the content.</p>
<h2 id="recommendation">Recommendation</h2>
<p>Overall, I see this book as being a good one to read after Kevin Murphy's <a href="https://dfd.github.io/probabilistic-machine-learning/">Probabilistic Machine Learning</a>.  They draw from a similar perspective, but MacKay's goes deeper within what it covers.</p>
<p>So my recommendation is to read the whole book, after PML, and do all of the recommended problems.  The book is comprised of 50 relatively short chapters.  I think readers could alternate between reading a chapter and then doing its recommended problems, and complete the book in 100 days without much problem.  Very motivated readers could probably finish in 50 days by reading a chapter and completing the problems each day.  Consider it a &quot;must read&quot; if you want to go deep into NLP.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: Probably Overthinking It</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Sun, 17 Mar 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          <category><![CDATA[Causal Inference]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/probably-overthinking-it/</link>
          <guid>https://dfd.github.io/probably-overthinking-it/</guid>
          <description><![CDATA[Overview
Allen Downey’s Probably Overthinking It describes and resolves a number of statistical fallacies and paradoxes in an accessible way.  The subtitle is How to use data to answer questions, avoid statistical traps, and make better decisions, and an overarching theme is the …]]></description>
          <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>Allen Downey’s <a rel="noopener nofollow noreferrer" target="_blank" href="https://greenteapress.com/wp/probably-overthinking-it/">Probably Overthinking It</a> describes and resolves a number of statistical fallacies and paradoxes in an accessible way.  The subtitle is <em>How to use data to answer questions, avoid statistical traps, and make better decisions</em>, and an overarching theme is the various ways sampling bias can impact inference.  He illustrates the issues visually, demonstrates each with multiple examples, and shows adjustments that can overcome and explain them.</p>
<p>This book is well suited for data scientists, analysts, and mathematically inclined consumers of quantitative analyses.  While a background in probability and statistics would help get more out of the book, it’s not particularly technical; Downey does not dive deeply into the mathematical details, leaving much of the explanation to well designed plots and stories that accompany them.  It’s primarily a book on how to <em>reason</em> about observational data and evaluate models.</p>
<p>Having read <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/">Downey’s blog</a> (and <a rel="noopener nofollow noreferrer" target="_blank" href="http://allendowney.blogspot.com/">his previous blog</a> before that) for the past 12 years or so, I was familiar with many themes in the book, although some of the examples were new.  Topics include (among others) the <a rel="noopener nofollow noreferrer" target="_blank" href="http://allendowney.blogspot.com/2015/08/the-inspection-paradox-is-everywhere.html">Inspection Paradox</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2021/04/07/berkson-goes-to-college/">Berkson’s Paradox</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2021/05/25/in-search-of-simpsons-paradox/">Simpson’s Paradox</a>, and the <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/AllenDowney/BiteSizeBayes/blob/master/05_test.ipynb">base rate fallacy</a>.</p>
<h2 id="what-makes-the-book-unique">What makes the book unique</h2>
<p>One thing I like about the book is its emphasis on choosing appropriate probability distributions and plotting methods.  Students tend to come out of university statistics courses overemphasizing the Gaussian distribution, and a big lesson from this book is that the choice of distribution can matter a lot to a model’s quality and the decisions it informs. He also explains circumstances under which the Gaussian distribution <em>is</em> an appropriate choice and how to compare the fit of different modeling distributions.  He does quite a bit of modeling with the log-normal distribution, which I've used extensively in industry, and even the log-t distribution, which I've also applied in practice. </p>
<p>The book also focuses on comparing model predictions to observable data, rather than on parameters, standard errors, or hypothesis tests.  This is done with widespread use of ECDFs and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2022/10/03/the-long-tail-of-disaster/">log-log complementary CDFs when we want to emphasize tail behavior</a> (these are <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.cs.cmu.edu/~christos/courses/826-resources/PAPERS+BOOK/0412004-newman05.pdf">widely used for power law distributions</a>, often found in networks).  It also touches on survival analysis, which many analysts and data scientists don’t seem to encounter in school.</p>
<h2 id="most-interesting-chapters">Most interesting chapters</h2>
<p>Perhaps the most interesting topic is Berkson’s Paradox, for which the following example is given:</p>
<ul>
<li>Babies of mothers who smoked were about 6% lighter at birth</li>
<li>Smokers were about twice as likely to have babies lighter than 2,500 grams, which is considered “low birthweight”</li>
<li>Low-birthweight babies were much more likely to die within a month of birth</li>
<li>But the mortality rate of low-birthweight babies of smokers was lower than that of non-smokers</li>
</ul>
<p>This led some to conclude that smoking was protective for low-birthweight babies.  I won't spoil the solution here, but he posted <a rel="noopener nofollow noreferrer" target="_blank" href="https://docs.google.com/presentation/d/e/2PACX-1vRfp93z9-0v4d4xbhw73zgBO-oz0QMguXrBCUI3JQsmpv2RzUDlClyRS2kZMy-EMZTCYI-38HNMrPSP/pub?start=false&amp;loop=false&amp;delayms=3000">some slides on the subject</a>.  The chapter also introduces causal diagrams, but they are only covered to the extent needed by the chapter.</p>
<p>Another chapter I particularly enjoyed was <em>Chasing the Overton Window</em>, where he explains the following phenomena:</p>
<ul>
<li>People are more likely to identify as conservative as they get older.</li>
<li>Order people hold more conservative views, on average.</li>
<li>However, as people get older, their views do not become more conservative.</li>
</ul>
<p>Again, I won't spoil it, but you can see <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2023/04/24/the-overton-paradox/">some of his analysis here</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2022/11/09/chasing-the-overton-window/">watch a talk on it here</a>.</p>
<p>Although Downey is a <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2021/05/07/founded-upon-an-error/">card-carrying Bayesian</a>, the book doesn't emphasize Bayesian analysis, beyond the chapter on the base rate fallacy, which uses the <a rel="noopener nofollow noreferrer" target="_blank" href="https://allendowney.github.io/BiteSizeBayes/05_test.html">uncontroversial application of Bayes rule to diagnostic tests</a>, although I don't recall seeing the word &quot;Bayes&quot; at all.  An important point he makes is that information available before the test is important to establishing the relevant base rate, e.g. random screening implies a different base rate for a disease than someone being tested because of symptoms.  He then moves on to applying base rates to the <a rel="noopener nofollow noreferrer" target="_blank" href="https://arxiv.org/abs/1707.01195">fairness impossibility theorem</a>.</p>
<p>Another interesting chapter I'll mention was about the inspection paradox.  He gives several examples, but the most illustrative is about running a distance race.  A given runner will observe more runners who are either significantly faster or slower than themselves, but almost none who are running at about the same pace.  Not only does Downey do a nice job explaining why this occurs, but also shows how to adjust for the sampling bias that leads to the paradox.  You can see a <a rel="noopener nofollow noreferrer" target="_blank" href="http://allendowney.blogspot.com/2015/08/the-inspection-paradox-is-everywhere.html">version of the analysis here</a>.</p>
<h2 id="expanding-on-chapter-1">Expanding on Chapter 1</h2>
<p>The basic premise of chapter 1 is that no one is “normal” (defined as being within 0.3 standard deviations of the average) with respect to even a modest number of bodily measurements or personality traits.  He shows through a couple of examples that very few people are &quot;normal&quot; on numerous metrics.  One specific example uses the Big Five personality traits, which we are told are roughly independent, and pretty well approximated by Gaussian distributions.  For each trait, he defines normal as being in the middle 20-28% of the distribution.</p>
<p>He then presents data on the traits and normality within them and in combinations.  I'm combining some information from two tables here to show the percentage who are considered &quot;normal&quot; for each trait, followed by the counts and percentage of the total people who make it through each successive &quot;normal&quot; filter.</p>
<table><thead><tr><th><strong>Trait</strong></th><th style="text-align: right"><strong>Pct &quot;Normal&quot;</strong></th><th style="text-align: right"><strong>Count</strong></th><th style="text-align: right"><strong>Pct Remain</strong></th></tr></thead><tbody>
<tr><td>Extroversion</td><td style="text-align: right">23.4</td><td style="text-align: right">204,077</td><td style="text-align: right">23.4</td></tr>
<tr><td>Emotional stability</td><td style="text-align: right">20.9</td><td style="text-align: right">46,988</td><td style="text-align: right">5.4</td></tr>
<tr><td>Conscientiousness</td><td style="text-align: right">20.2</td><td style="text-align: right">10,976</td><td style="text-align: right">1.3</td></tr>
<tr><td>Agreeableness</td><td style="text-align: right">21.1</td><td style="text-align: right">2,981</td><td style="text-align: right">0.3</td></tr>
<tr><td>Openness</td><td style="text-align: right">28.3</td><td style="text-align: right">926</td><td style="text-align: right">0.1</td></tr>
</tbody></table>
<p>In other words, only 0.1% of those in the sample data are in a &quot;normal&quot; range in each trait.  There are at least two ways to explain why this is an expected result.</p>
<p>The first is <a rel="noopener nofollow noreferrer" target="_blank" href="https://juanitorduz.github.io/exploring-the-curse-of-dimensionality-part-ii./">the curse of dimensionality</a>, which offers a geometric explanation for this.  Suppose each of $d$ traits were an independent standard Gaussian distribution.  The mean squared distance from the origin for points in the distribution is $d$, which follows from the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Chi-squared_distribution">chi-squared distribution</a>.  In other words, the more independent traits we add, the further from the mean we expect a sampled point in the distribution to be.  And more to the point, as $d$ grows, the probability increases that at least one of the draws from the independent Gaussian traits will be far away from the mean.</p>
<p>From a geometric point of view, if a point is far from the mean in any dimension, then we would say it is on the &quot;edge&quot; of the distribution.  The curse of dimensionality tells us that as $d$ grows large, eventually most of our data will be found on the edge of the data set, i.e. a value in at least one dimension for each point will be far from the mean (not &quot;normal&quot; aka &quot;weird&quot;).  And with enough dimensions, all data will be at an edge in approximately the same number of attributes.  Downey alludes to this idea when he writes, &quot;In the limit, if we consider the nearly infinite ways people vary, we find we are all equally weird,&quot; but without naming the concept directly.  So there may have been an opportunity to connect the chapter 1 lesson back to a more broadly applicable concept that readers may have heard about in other contexts of statistics or machine learning. See <a rel="noopener nofollow noreferrer" target="_blank" href="https://hastie.su.domains/ElemStatLearn/">section 2.5 of Elements of Statistical Learning</a> for more on the idea.</p>
<p>A simpler reason this result is expected is that if you multiply roughly independent probabilities between 0.2 and 0.3, you can get a small probability quickly.  And the product gets smaller faster when the events are independent.  For instance, suppose we treat each of those percent normal for each trait as probabilities of independent events and just multiply them.  We would get the results in the far right of the following table, which isn't too much different from the actual results.</p>
<table><thead><tr><th><strong>Trait</strong></th><th style="text-align: right"><strong>Pct &quot;Normal&quot;</strong></th><th style="text-align: right"><strong>Pct Remain</strong></th><th style="text-align: right"><strong>Pct if ⫫</strong></th></tr></thead><tbody>
<tr><td>Extroversion</td><td style="text-align: right">23.4</td><td style="text-align: right">23.4</td><td style="text-align: right">23.4</td></tr>
<tr><td>Emotional stability</td><td style="text-align: right">20.9</td><td style="text-align: right">5.4</td><td style="text-align: right">4.9</td></tr>
<tr><td>Conscientiousness</td><td style="text-align: right">20.2</td><td style="text-align: right">1.3</td><td style="text-align: right">1.0</td></tr>
<tr><td>Agreeableness</td><td style="text-align: right">21.1</td><td style="text-align: right">0.3</td><td style="text-align: right">0.2</td></tr>
<tr><td>Openness</td><td style="text-align: right">28.3</td><td style="text-align: right">0.1</td><td style="text-align: right">0.1</td></tr>
</tbody></table>
<p>Notice this result does not have anything to do with being &quot;normal&quot; or any concept of distance at all.  We could have been talking about the probabilities of being in the farthest left portions of the trait distributions, or simply assigned the probabilities to a set of independent Bernoulli trials.  To simplify further, let's suppose the probability of being &quot;normal&quot; for any trait is $p$.  Then the probability that an instance is normal in all $d$ independent traits is just $p^d$, which will become very small as $d$ grows large, if $p \lt 1$.  Here, we can get roughly the same result by setting $p=0.234$ for each of 5 indepdent traits.</p>
<h2 id="verdict">Verdict</h2>
<p>Overall, I recommend this book to producers and consumers of data analysis to learn a useful set of concepts through examples.  At the very least, readers will learn to ask good questions with respect to common issues in observational data analysis, and readers with the right background may learn some new methods to apply.  While it eschews the technical density of a textbook, it demands more intellectual engagement than a typical pop science book, drawing readers in with its broad scope of topics and colorful storytelling.</p>
<p>All the notebooks used to generate the analyses and plots are <a rel="noopener nofollow noreferrer" target="_blank" href="https://allendowney.github.io/ProbablyOverthinkingIt/intro.html">available here</a>, which can aid practitioners looking to apply the lessons to their own work.  You can <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Probably-Overthinking-Questions-Statistical-Decisions/dp/0226822583">buy the book here</a>, and he also has a number of high quality technical books <a rel="noopener nofollow noreferrer" target="_blank" href="https://greenteapress.com/wp/">available for free here</a>.</p>
]]></content:encoded>
      </item>
    </channel>
</rss> 
