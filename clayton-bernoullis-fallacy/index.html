<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content=#ffffff name=theme-color><meta content=#da532c name=msapplication-TileColor><link href=https://dfd.github.io/rss.xml rel=alternate title=RSS type=application/rss+xml><link href=/icons/site.webmanifest rel=manifest><link color=#5bbad5 href=/icons/safari-pinned-tab.svg rel=mask-icon><link href=/icons/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/icons/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/icons/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link crossorigin href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.1/css/academicons.min.css integrity=sha384-FIue+PI4SsI9XfHCz8dBLg33b0c1fMJgNU3X//L26FYbGnlSEfWmNT7zgWc2N9b6 rel=stylesheet><link integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css rel=stylesheet><link href=https://dfd.github.io/deep-thought.css rel=stylesheet><title>
    
Implicit Assumptions | Book Review: Bernoulli's Fallacy

  </title><script async data-beampipe-domain=dfd.github.io defer src=https://beampipe.io/js/tracker.js></script><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs rel=stylesheet><script crossorigin defer integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js></script><script crossorigin defer integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mathtex-script-type.min.js></script><script crossorigin defer integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js></script><body class=has-background-white><nav aria-label="section navigation" class="navbar is-light" role=navigation><div class=container><div class=navbar-brand><a class="navbar-item is-size-5 has-text-weight-bold" href=https://dfd.github.io>Implicit Assumptions</a><a class="navbar-burger burger" aria-expanded=false aria-label=menu data-target=navMenu role=button> <span aria-hidden=true></span> <span aria-hidden=true></span> <span aria-hidden=true></span> </a></div><div class=navbar-menu id=navMenu><div class="navbar-end has-text-centered"><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/> Posts </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/tags> Tags </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/categories> Categories </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/about> About </a><a title="Switch to dark theme" class=navbar-item id=dark-mode> <span class=icon> <i class="fas fa-adjust"></i> </span> </a></div></div></div></nav><section class=section><div class=container><div class=columns><div class="column is-8 is-offset-2"><article class=box><h1 class=title>Book Review: Bernoulli's Fallacy</h1><div class="columns is-multiline is-gapless"><div class="column is-8"><span class="icon-text has-text-grey"> <span class=icon-special> <i class="far fa-calendar-alt" title="publish date"></i> </span> <span><time datetime=2024-08-15>August 15, 2024</time></span> <span class=icon-special> <i class="fas fa-user" title=author></i> </span> <span>Dave</span> </span></div><div class="column is-4 has-text-right-desktop"></div></div><div class="content mt-2"><h2 id=overview>Overview</h2><p>Aubrey Clayton's <a rel="noopener nofollow noreferrer" href=https://aubreyclayton.com/bernoulli target=_blank><em>Bernoulli's Fallacy</em></a> is perhaps the most accessible introduction to Jaynes' version of "logical probability." But it goes further than that: Clayton provides the essential elements of the development and historical debates on the interpretation of probability and statistical inference through Fisher; gives numerous examples of how frequentist methods can fail; connects these issues to the academic research replication crisis; and suggests a way forward.<h2 id=interpreting-probability>Interpreting probability</h2><p>After motivating self-reflection within the reader through some example probability problems, Clayton enumerates the most common interpretations, including ancient notions of chance, frequentist, subjective, and axiomatic, while dissecting the appeal and problems of each one. Finally he presents "probability as logic," which will serve as author's point of view throughout the book.<p>To summarize, <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Probability_axioms#Kolmogorov_axioms target=_blank>Kolmogorov's axiomatic probability</a> had put probability on a rigorous foundation, but did not solve the interpretation problem; all modern interpretations could adhere to the axioms. The logical point of view then evolved with contributions from <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/A_Treatise_on_Probability target=_blank>Keynes</a> and <a rel="noopener nofollow noreferrer" href=https://arxiv.org/pdf/0804.3173 target=_blank>Jeffries</a>; <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Cox%27s_theorem target=_blank>Cox showed</a> how to break probability free from the notion of proportions and frequencies and to represent plausibilities while still adhering to the axioms; and Jaynes completed the evolution with his book, <a rel="noopener nofollow noreferrer" href=https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712 target=_blank><em>Probability Theory: The Logic of Science</em></a>. Clayton walks the reader quickly through this evolution, and the final intuition is as follows:<p>Probability is an extension of logical deduction. While logical deduction is process for calculating implications of interest from propositions that have true or false values, probability allows us to do the same with propositions of uncertain truth values. In fact, as Clayton relays, probability reduces to logical deduction when working exclusively with probabilities of 0s and 1s. Another key point is that all probability is conditional; we are always determining probability from <em>some</em> information, and that two rational people with the same information will calculate the same probabilities. Jaynes viewed probability as a way to process information, not as proportions or long run frequency. Finally, probability is epistemology, not ontology. That is, probability represents our state of knowledge rather than reality itself, and confusing our uncertainty with reality is the <em>mind projection fallacy</em>.<p>Following this, Clayton walks the reader through some classic probability problems using this view of the probability, pointing out along the way that there is no need in any of them to define probability as long run frequency.<h2 id=bernoulli-s-blunder>Bernoulli's blunder</h2><p>Clayton next turns our attention to the history of probability and statistical inference, starting with Bernoulli. In his telling of the history, Bernoulli was the first to make a mistake that would haunt statistics to the present day: concluding that a statistic derived from a particular sample is probably close to its true value for any sampled value, rather than the correct conclusion that a yet-to-be-drawn sample statistic will probably be close to the true value for any true value. Clayton explains that Bernoulli's fallacy came from confusing the sampling probability (the likelihood), with the probability of a hypothesis. These two are related through Bayes theorem, so Bernoulli needed to include more information to complete the calculation for his claim, and it's not necessarily true.<p>Clayton then goes on to illustrate <em>why</em> these two conclusions are not equivalent, despite appealing to people's intuition. Specifically, he arms the reader with a particular framework to define hypotheses, assign prior probabilities, assign the sampling probability (or likelihood), calculate the "pathway probability" (the unnormalized posterior of multiplying the previous two numbers), and finally the posterior probability for each hypothesis through normalization. This is done in a similar manner to Allen Downey's <a rel="noopener nofollow noreferrer" href=https://docs.google.com/drawings/d/1dHkIa-RdmnLPze84gkcmYflWwA7xwtwFhXPs_Rd0oRM/edit target=_blank>Bayesian worksheet</a> (example use case <a rel="noopener nofollow noreferrer" href=https://allendowney.blogspot.com/2017/02/a-nice-bayes-theorem-problem-medical.html target=_blank>here</a>).<p>To end the chapter, Clayton covers <em>base rate neglect</em>, first presenting the canonical medical test example, followed by the <em>prosecutor's fallacy</em> with the tragic story of <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Sally_Clark#Conviction_for_murder target=_blank>Sally Clark's conviction</a>, before explaining that these are in fact the same error, confusing $P[A | B]$ for $P[B | A]$; the same one made by Bernoulli himself.<h2 id=wrong-turn>Wrong turn</h2><p>The next chapter covers the bridge from applying statistics in the hard sciences to softer ones. This can be traced back to Gauss and Laplace, who developed and justified the method of least squares regression in astronomy, and Quetelet, who looked to apply the methods to the development of "social physics." Around this time, the common interpretation of probability also starts to shift toward frequency. This is due to both empirical data seeming to adhere to probability distributions, and because of academics questioning the origins of prior probabilities.<p>Chapter Four covers Galton, Pearson, and Fisher's influence on statistics. These three advanced frequentist statistical theory and applied it to comparisons of groups of people, for the purposes of eugenics. In this pursuit, they wanted to develop statistical methods that could be seen as "objective" to bolster their claims. Clayton wrote <a rel="noopener nofollow noreferrer" href=https://nautil.us/how-eugenics-shaped-statistics-238014/ target=_blank>a related piece in 2020</a> that overlaps a bit with this chapter.<p>Chapter Five starts with an illustrative dialogue between a fictitious student and frequentist bot. The humorous conversation conveys all the typical confusions one encounters while trying to navigate the logic and interpretation of frequentist methods. It sits in stark contrast to the consistency of the previously presented Bayesian analyses.<p>Clayton then turns his attention back to Fisher, and the development of disparate tests for various situations, presenting <a href=https://dfd.github.io/mcelreath-statistical-rethinking/#connecting-methods-to-science>a flow chart similar to the one presented by McElreath</a>. But Clayton goes beyond this sight gag in his critique of frequentist methods, calling out issues in both theory and practice. He also cites evidence that Fisher, despite his vitriol toward Bayesian methods, actually drew inspiration from them in his derivation of maximum likelihood and grew a soft spot for them in his later writings. And finally, his zig zagging during the development of <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Fiducial_inference target=_blank>fiducial inference</a> eventually led him to argue against his own earlier concepts of inference since the new paradigm would not work for his new paradigm, which was trying to draw Bayesian conclusions without use of priors.<p>The chapter concludes with nine example problems that require Bayesian reasoning to obtain sensible solutions and for which frequentist methods fail miserably.<h2 id=frequently-wrong>Frequently wrong</h2><p>Clayton then turns his attention to the replication crisis in academia, and particularly in the social and medical sciences. After citing objections to null hypothesis testing over time by those who recognized its irrelevance and misuse, he presents the theory and evidence of why so many modern studies have failed to replicate. Then he recounts the curious case of Daryl Bem, who turned from ESP skeptic to <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Daryl_Bem#%22Feeling_the_Future%22_controversy target=_blank>publishing "statistically significant" research</a> on the subject. Bem's claims were implausible to most and yet used standard methods of analysis. Clayton then shares rumors that Bem had pulled this off as metacommentary on research practices.<p>In the final chapter, Clayton proposes reform to research practices. It will come as no surprise that the proposal includes abandoning frequentist methods, including null hypothesis testing, and instead embracing Bayesian methods. He even goes so far as to suggest that universities dissolve their statistics departments, since Bayesian inference relies on only one theorem. That seems a bit far to me, as Bayesian models and computation can become quite complex, and there are still regular developments in Bayesian methods and practice. Plus, even if researchers across fields used Bayesian methods, many won't master them at an advanced level, so it's helpful to have some centralized academics who can help or guide empirical work.<h2 id=recommendation>Recommendation</h2><p>This is a unique book in its scope: a straightforward presentation of Logical Probability; an overview of the historical debates; hands-on examples of where frequentist methods fail and how Bayesian methods help; and insights and proposed remedies related to the academic replication crisis. It's not as math-heavy or dense as a textbook, but there is some math sprinkled throughout. The notation includes probability, summations, factorials, binomial coefficients, and exponential functions. So while I think the concepts are well-explained, a reader will want to feel comfortable with that notation and related concepts to get the most out of it.<p>Recommended for those looking for an introduction to Jaynes or the "statistics wars." I had first read this over two years ago, and it was fun to revisit and flip through. If you feel inspired to move on to Jaynes' book itself afterward, Clayton also has a <a href="https://www.youtube.com/watch?v=rfKS69cIwHc&list=PL9v9IXDsJkktefQzX39wC2YG07vw7DsQ_" rel="noopener nofollow noreferrer" target=_blank>video lecture series</a> that covers the book from start to finish.</div><div class="columns box-footer is-12"><div class="column is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-cubes" title=categories></i> </span> <span> <a class=has-text-weight-semibold href=https://dfd.github.io/categories/statistics/>Statistics</a>,Â <a class=has-text-weight-semibold href=https://dfd.github.io/categories/probability/>Probability</a> </span> </span></div><div class="column has-text-right-desktop is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-tags" title=tags></i> </span> <a class=has-text-weight-semibold href=https://dfd.github.io/tags/book-review/>book review</a> </span></div></div></article></div><div class="column is-2 is-hidden-mobile"><aside class=menu style=position:sticky;top:48px><p class="heading has-text-weight-bold">Contents<ul class=menu-list><li><a class="toc is-size-7 is-active" href=https://dfd.github.io/clayton-bernoullis-fallacy/#overview id=link-overview> Overview </a><li><a class="toc is-size-7" href=https://dfd.github.io/clayton-bernoullis-fallacy/#interpreting-probability id=link-interpreting-probability> Interpreting probability </a><li><a class="toc is-size-7" href=https://dfd.github.io/clayton-bernoullis-fallacy/#bernoulli-s-blunder id=link-bernoulli-s-blunder> Bernoulli's blunder </a><li><a class="toc is-size-7" href=https://dfd.github.io/clayton-bernoullis-fallacy/#wrong-turn id=link-wrong-turn> Wrong turn </a><li><a class="toc is-size-7" href=https://dfd.github.io/clayton-bernoullis-fallacy/#frequently-wrong id=link-frequently-wrong> Frequently wrong </a><li><a class="toc is-size-7" href=https://dfd.github.io/clayton-bernoullis-fallacy/#recommendation id=link-recommendation> Recommendation </a></ul></aside></div></div></div></section><section class=section><div class=container><div class="columns is-centered"><div class="column is-8"><nav class=level><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/mcelreath-statistical-rethinking/> <span class="icon mr-2"> <i class="fas fa-arrow-circle-left"></i> </span> Book Review: Statistical Rethinking </a></div><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/linux-distro/> Choosing a Linux Distribution<span class="icon ml-2"> <i class="fas fa-arrow-circle-right"></i> </span> </a></div></nav></div></div></div></section><footer class="footer py-4"><div class="content has-text-centered"><p><span class=icon> <i class="fas fa-power-off"></i> </span> Powered by <span class=icon-text> <span><a href=https://www.getzola.org/>zola</a></span> </span></div></footer><script src=https://dfd.github.io/js/site.js></script><script>const menuBarHeight=document.querySelector("nav.navbar").clientHeight;const tocItems=document.querySelectorAll(".toc");const navSections=new Array(tocItems.length);tocItems.forEach((a,b)=>{let c=a.getAttribute("id").substring(5);navSections[b]=document.getElementById(c)});function isVisible(a){const b=navSections[a];const c=a<tocItems.length- 1?navSections[a+ 1]:document.querySelectorAll("section.section").item(1);const d=b.getBoundingClientRect();const e=c.getBoundingClientRect();const f=window.innerHeight||document.documentElement.clientHeight;return d.top<=f&&e.top- menuBarHeight>=0}function activateIfVisible(){for(b=true,i=0;i<tocItems.length;i++){if(b&&isVisible(i)){tocItems[i].classList.add('is-active');b=false}else tocItems[i].classList.remove('is-active')}}var isTicking=null;window.addEventListener('scroll',()=>{if(!isTicking){window.requestAnimationFrame(()=>{activateIfVisible();isTicking=false});isTicking=true}},false)</script>