<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content=#ffffff name=theme-color><meta content=#da532c name=msapplication-TileColor><link href=https://dfd.github.io/rss.xml rel=alternate title=RSS type=application/rss+xml><link href=/icons/site.webmanifest rel=manifest><link color=#5bbad5 href=/icons/safari-pinned-tab.svg rel=mask-icon><link href=/icons/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/icons/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/icons/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link crossorigin href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.1/css/academicons.min.css integrity=sha384-FIue+PI4SsI9XfHCz8dBLg33b0c1fMJgNU3X//L26FYbGnlSEfWmNT7zgWc2N9b6 rel=stylesheet><link integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css rel=stylesheet><link href=https://dfd.github.io/deep-thought.css rel=stylesheet><title>
    
Implicit Assumptions | Don't Conform Your Data to the Model. Do the Opposite.

  </title><script async data-beampipe-domain=dfd.github.io defer src=https://beampipe.io/js/tracker.js></script><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs rel=stylesheet><script crossorigin defer integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js></script><script crossorigin defer integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mathtex-script-type.min.js></script><script crossorigin defer integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js></script><body class=has-background-white><nav aria-label="section navigation" class="navbar is-light" role=navigation><div class=container><div class=navbar-brand><a class="navbar-item is-size-5 has-text-weight-bold" href=https://dfd.github.io>Implicit Assumptions</a><a class="navbar-burger burger" aria-expanded=false aria-label=menu data-target=navMenu role=button> <span aria-hidden=true></span> <span aria-hidden=true></span> <span aria-hidden=true></span> </a></div><div class=navbar-menu id=navMenu><div class="navbar-end has-text-centered"><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/> Posts </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/tags> Tags </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/categories> Categories </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/about> About </a><a title="Switch to dark theme" class=navbar-item id=dark-mode> <span class=icon> <i class="fas fa-adjust"></i> </span> </a></div></div></div></nav><section class=section><div class=container><div class=columns><div class="column is-8 is-offset-2"><article class=box><h1 class=title>Don't Conform Your Data to the Model. Do the Opposite.</h1><div class="columns is-multiline is-gapless"><div class="column is-8"><span class="icon-text has-text-grey"> <span class=icon-special> <i class="far fa-calendar-alt" title="publish date"></i> </span> <span><time datetime=2025-03-02>March 02, 2025</time></span> <span class=icon-special> <i class="fas fa-user" title=author></i> </span> <span>Dave</span> </span></div><div class="column is-4 has-text-right-desktop"></div></div><div class="content mt-2"><p>In this post, I'll be responding to an idea I saw on LinkedIn. See my guidelines on <a href=https://dfd.github.io/guidelines-on-naming-names/>naming names</a>.<h2 id=the-problem>The Problem</h2><p>I recently saw <a rel="noopener nofollow noreferrer" href=https://www.linkedin.com/posts/tyler-buffington-82a1a212a_not-all-outlier-handling-approaches-are-created-activity-7285002769231425536-mty1/ target=_blank>a post on LinkedIn</a> which serves as a nice example of how the null hypothesis mindset leads people to poor statistical decision making. The author works at an AB testing as a service company. Here is the post:<p><img alt=png src=https://dfd.github.io/dont-conform-data-to-the-model/post.png><p>I agree with most of the points listed, but I want to focus on the suggestion to Winsorize the data and the plots at the bottom of the post.<p>The issue here is that the author wants to be able to compare the effect of a treatment on a continuous variable. The problem arises presumably because the author wants to conduct a two-sample t-test on the observations against a null hypothesis, but the skewed distribution of the observables will mean the <a href="https://en.wikipedia.org/wiki/Student's_t-test#Assumptions" rel="noopener nofollow noreferrer" target=_blank>assumption of normality on the sampling distribution of the means</a> won't be valid at lower sample sizes, and hence will decrease the power of the test.<h2 id=the-proposed-solution>The Proposed Solution</h2><p><a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Winsorizing target=_blank>Winsorizing</a> is used to clip the highest values to reduce the skewness in the observed data; so instead of a long tail, the empirical distribution now has a small peak at whatever the threshold was.<p>If we suppose a frequentist null hypothesis test is what they want, there are problems with using a t-test here:<ol><li>If we don't Winsorize the data, then the normality assumptions on the means do not hold until larger sample sizes (using <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Central_limit_theorem target=_blank>CLT</a>). So for sample sizes at which CLT does not apply (and what is that cutoff exactly?), the "frequentist guarantees" don't apply.<li>If we do Winsorize, then we've changed our data so the frequentist guarantees also don't apply. How could they? We're throwing out data so we won't have accurate estimates in the difference of our means.</ol><p>The post frames this as a bias/variance trade-off. While this is better than insisting on unbiasedness above all else, it seems to ignore the typical reasons given for using the null hypothesis test in the first place: those guarantees.<p>Even if the method of analysis isn't a null hypothesis test, Winsorizing data still throws out information that might change our conclusion, had we not capped it. The value at which to cap it will necessarily be arbitrary. If we cap it ahead of time, then we may not know where a good cap is; if we cap it after the fact, how do we know our bias hasn't influenced the choice of cap?<p>In the <a rel="noopener nofollow noreferrer" href=https://www.linkedin.com/posts/ronnyk_abtest-activity-7254731531049488384-hh6J/ target=_blank>Ron Kohavi post linked to in the screenshot</a>, he discusses using the technique to reduce skewness and increase power. But increased power to compare what? If the continuous metric is what you care about, it's no longer clear what you're comparing with the new capped metric. He even gives an example of reducing required sample size by 50% through capping, and suggests capping at different levels to compare results. But how to decide which results to use? Maybe he explains it in his course that he links to, but it's hard to imagine a principled approach to deciding, and providing multiple options risks cherry-picking a convenient false positive.<h2 id=the-alternative-solution>The Alternative Solution</h2><p>Instead of trying to make our data conform to the assumption of our model (the t-test relies on a model of the null hypothesis), we should do the opposite and find a model that describes the data well. Looking back at the post in the screenshot, we see an illustrative sample provided by the author. It looks like it would be well approximated by a log-distribution, and in my experience, this is quite common in revenue or customer lifetime value metrics. I agree with the author that we should not simply log the observations, but we could model the samples as log-normal distributions. Here are some benefits of doing so:<ol><li>We don't throw out information. Assuming our measurements aren't flawed, we should try to retain all available information from these tests. Driving higher revenue value is valuable to the company. If we use the same Winsor cutoff on both samples, then whichever sample drove more users above the cutoff will be punished in the comparison.<li>The log-normal distribution naturally discounts the impact of outliers in the sample. It does this by estimating its parameters on the log scale. So we don't need to worry if one sample "got lucky" with a random draw far out in its tail; we won't be using its raw value in a mean calculation.<li>But this is different than just comparing the logged values. The mean of a <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Log-normal_distribution target=_blank>log-normal distribution</a> is a function of its log mean and standard deviation parameters: $\exp(\mu + \frac{\sigma^2}{2})$. So we can take into account how both the mean and spread of the distribution on the log scale impact the metric we car about in its original scale.<li>If the log-normal assumptions are a good match for the data, then it is a sample-efficient way to estimate the parameters.</ol><p>Using Bayesian methods, we can estimate the parameters and create derived quantities using the equation above and have a posterior distribution over the difference, which we can use to estimate the probability of the treatment having a higher mean revenue or whatever we would like to know.<p>Of course, this may not work well in all situations; not all businesses have revenue distributions that are well modeled by log-normals. But perhaps other distributions will work well in those cases. When we conform our modeling assumptions to the nature of our data, we can make better comparisons about the things we care about in sample efficient ways. No need to cap.</div><div class="columns box-footer is-12"><div class="column is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-cubes" title=categories></i> </span> <span> <a class=has-text-weight-semibold href=https://dfd.github.io/categories/statistics/>Statistics</a> </span> </span></div><div class="column has-text-right-desktop is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-tags" title=tags></i> </span> <a class=has-text-weight-semibold href=https://dfd.github.io/tags/ab-testing/>ab testing</a> , <a class=has-text-weight-semibold href=https://dfd.github.io/tags/bad-practices/>bad practices</a> , <a class=has-text-weight-semibold href=https://dfd.github.io/tags/winsorizing/>winsorizing</a> </span></div></div></article></div><div class="column is-2 is-hidden-mobile"><aside class=menu style=position:sticky;top:48px><p class="heading has-text-weight-bold">Contents<ul class=menu-list><li><a class="toc is-size-7 is-active" href=https://dfd.github.io/dont-conform-data-to-the-model/#the-problem id=link-the-problem> The Problem </a><li><a class="toc is-size-7" href=https://dfd.github.io/dont-conform-data-to-the-model/#the-proposed-solution id=link-the-proposed-solution> The Proposed Solution </a><li><a class="toc is-size-7" href=https://dfd.github.io/dont-conform-data-to-the-model/#the-alternative-solution id=link-the-alternative-solution> The Alternative Solution </a></ul></aside></div></div></div></section><section class=section><div class=container><div class="columns is-centered"><div class="column is-8"><nav class=level><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/guidelines-on-naming-names/> <span class="icon mr-2"> <i class="fas fa-arrow-circle-left"></i> </span> Blog Guidelines on Naming Names </a></div><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/solution-ai-slop/> A Solution to the Deluge of Bug Bounty and FOSS AI Slop Submissions<span class="icon ml-2"> <i class="fas fa-arrow-circle-right"></i> </span> </a></div></nav></div></div></div></section><footer class="footer py-4"><div class="content has-text-centered"><p><span class=icon> <i class="fas fa-power-off"></i> </span> Powered by <span class=icon-text> <span><a href=https://www.getzola.org/>zola</a></span> </span></div></footer><script src=https://dfd.github.io/js/site.js></script><script>const menuBarHeight=document.querySelector("nav.navbar").clientHeight;const tocItems=document.querySelectorAll(".toc");const navSections=new Array(tocItems.length);tocItems.forEach((a,b)=>{let c=a.getAttribute("id").substring(5);navSections[b]=document.getElementById(c)});function isVisible(a){const b=navSections[a];const c=a<tocItems.length- 1?navSections[a+ 1]:document.querySelectorAll("section.section").item(1);const d=b.getBoundingClientRect();const e=c.getBoundingClientRect();const f=window.innerHeight||document.documentElement.clientHeight;return d.top<=f&&e.top- menuBarHeight>=0}function activateIfVisible(){for(b=true,i=0;i<tocItems.length;i++){if(b&&isVisible(i)){tocItems[i].classList.add('is-active');b=false}else tocItems[i].classList.remove('is-active')}}var isTicking=null;window.addEventListener('scroll',()=>{if(!isTicking){window.requestAnimationFrame(()=>{activateIfVisible();isTicking=false});isTicking=true}},false)</script>