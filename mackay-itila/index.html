<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content=#ffffff name=theme-color><meta content=#da532c name=msapplication-TileColor><link href=https://dfd.github.io/rss.xml rel=alternate title=RSS type=application/rss+xml><link href=/icons/site.webmanifest rel=manifest><link color=#5bbad5 href=/icons/safari-pinned-tab.svg rel=mask-icon><link href=/icons/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/icons/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/icons/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link crossorigin href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.1/css/academicons.min.css integrity=sha384-FIue+PI4SsI9XfHCz8dBLg33b0c1fMJgNU3X//L26FYbGnlSEfWmNT7zgWc2N9b6 rel=stylesheet><link integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css rel=stylesheet><link href=https://dfd.github.io/deep-thought.css rel=stylesheet><title>
    
Implicit Assumptions | Book Review: Information Theory, Inference, and Learning Alogrithms

  </title><script async data-beampipe-domain=dfd.github.io defer src=https://beampipe.io/js/tracker.js></script><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs rel=stylesheet><script crossorigin defer integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js></script><script crossorigin defer integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mathtex-script-type.min.js></script><script crossorigin defer integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js></script><body class=has-background-white><nav aria-label="section navigation" class="navbar is-light" role=navigation><div class=container><div class=navbar-brand><a class="navbar-item is-size-5 has-text-weight-bold" href=https://dfd.github.io>Implicit Assumptions</a><a class="navbar-burger burger" aria-expanded=false aria-label=menu data-target=navMenu role=button> <span aria-hidden=true></span> <span aria-hidden=true></span> <span aria-hidden=true></span> </a></div><div class=navbar-menu id=navMenu><div class="navbar-end has-text-centered"><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/> Posts </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/tags> Tags </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/categories> Categories </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/about> About </a><a title="Switch to dark theme" class=navbar-item id=dark-mode> <span class=icon> <i class="fas fa-adjust"></i> </span> </a></div></div></div></nav><section class=section><div class=container><div class=columns><div class="column is-8 is-offset-2"><article class=box><h1 class=title>Book Review: Information Theory, Inference, and Learning Alogrithms</h1><div class="columns is-multiline is-gapless"><div class="column is-8"><span class="icon-text has-text-grey"> <span class=icon-special> <i class="far fa-calendar-alt" title="publish date"></i> </span> <span><time datetime=2024-07-15>July 15, 2024</time></span> <span class=icon-special> <i class="fas fa-user" title=author></i> </span> <span>Dave</span> </span></div><div class="column is-4 has-text-right-desktop"></div></div><div class="content mt-2"><h2 id=overview>Overview</h2><p>David MacKay's classic textbook <a rel="noopener nofollow noreferrer" href=http://www.inference.org.uk/mackay/itila/book.html target=_blank>Information Theory, Inference, and Learning Algorithms</a> was first published in 2003, and yet it holds up very well over 20 years later, seamlessly weaving together concepts across compression, noisy-channel coding, Bayesian statistics, and neural networks. By the end, a diligent reader will have a very good understanding of fundamentals that are vital to more advanced work in Bayesian statistics and deep learning.<h2 id=a-unique-perspective>A unique perspective</h2><p>MacKay uniquely presents the material, drawing parallels between seemingly distinct topics. As he states in Chapter 2, "One of the themes of this book is that data compression and data modeling are one and the same, and that they should both be addressed... using inverse probability."<p>One chapter presented a unique idea for model comparison using Occam's Razor. He presents an "Occam's Factor" which "provides the ratio of the poterior accessible volume of $\mathcal{H}_i$'s hypothesis space to the priori accessible volume, or the factor by which $\mathcal{H}_i$'s hypothesis space collapses when the data arrive." It's logarithm "is a measure of the amount of information we gain about the model's parameters when the data arrive."<p>So models with many parameters and few constraints will be penalized by a stronger Occam's factor than a simpler model because they had too much "wiggle room" to overfit the data. It's similar in spirit to AIC or BIC, except it can be applied to the full distributions over parameters, as opposed to just the maximium likelihood point estimation models. Later he goes so far as to say it can be used as a substitute for validation sets in Bayesian machine learning. I'm not sure I'm ready to commit to that, but I'm interested to explore it further. It seemes like it could be gamed. For instance, if you know of an overfit model and its parameter values for a set of training data, and you then set your priors to be equal to these, then the parameters wouldn't change and the Occam's Factor of that model would be small. So this seems to be relying on restricting oneself to truly using priors before seeing the data.<h2 id=diverse-topics>Diverse topics</h2><p>As if the diversity of high level topics weren't enough, perhaps the most interesting sequence in the book are chapters 18 and 19. He applies the tools from the Information Theory section first to crossword puzzles, and then presents a simplified version of the <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Cryptanalysis_of_the_Enigma target=_blank>Enigma Code breaking done at Bletchley Park</a>. Finally, he ends the section with an information theoretic point of view on the benefits of sexual reproduction with recombination using both analytical and simulation methods.<h2 id=relevance-to-nlp-and-deep-learning>Relevance to NLP and deep learning</h2><p>Even in Claude Shannon's original paper that introduced information theory, <a rel="noopener nofollow noreferrer" href=https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf target=_blank>A Mathemetical Theory of Communication</a>, he presented language models. And if one studies natural language processing today, it's still filled with concepts from information theory. Although this book came out 14 years before the transformer architecture was introduced in <a rel="noopener nofollow noreferrer" href=https://research.google/pubs/attention-is-all-you-need/ target=_blank>Attention is All You Need</a>, it will give readers an outstanding foundation from which to study modern approaches. I worry too many practitioners just take the shortest path to the latest architectures without building the requisite knowledge first.<p>I also like how he distills neural networks into their architecture, activity rule, and learning rule. The architecture specifies the variables and relationships within the network; the activity rule details how the outputs of neurons change in response to each other; the learning rule defines how to update weights during training.<p>One other topic I found particularly interesting was on the information capacity of a single neuron (and relating it back to VC dimension), as well as the capacity of Hopfield networks. Walking through these analyses gives the reader building blocks for reasoning about the capacity of larger, more complex networks.<p>While it's true that the Bayesian methods for which MacKay advocates are not the dominate paradigm used in deep learning today, the probabilistic perspective is broad enough to interpret parameters with point estimates. Regularization and drop out also have Bayesian interpretations.<h2 id=comparison-to-frequentist-methods>Comparison to frequentist methods</h2><p>MacKay mostly focuses on presenting the material at hand, but occasionally contrasts to frequentist methods. One example comes in Chapter 24, where he discusses the estimators for $\sigma$ in a Gaussian distribution. As he states:<p>Given data $D = \{x_n\}_{n=1}^N$, and 'estimator' of $\mu$ is<p>$$ \bar{x} \equiv \sum_{n=1}^N x_n / N, $$ and two estimators of $\sigma$ are: $$ \sigma_N = \sqrt{\frac{\sum_{n=1}^N (x_n - \bar{x})^2}{N}} \;\text{and}\; \sigma_{N-1} = \sqrt{\frac{\sum_{n=1}^N (x_n - \bar{x})^2}{N-1}} $$<p>He discusses how they invent estimators in the frequentist paradigm and choose the one that best meets some criteria of sampling properties. After pointing out that there is no clear principle for deciding which criterion to use, and given most criteria, there's no systematic way to produce an optimal estimator, he then explains the frequentist interpretation of these estimators. The estimator $(\bar{x}, \sigma_N)$ is the maximum likelihood estimator, but $\sigma_N$ is biased. That is, averaging over repeated sampling, $\sigma_N$ will not equal to $\sigma$. The $\sigma_{N-1}$ version is unbiased.<p>In contrast, the Bayesian view arrives at these quantities as maximum a posteriori estimates of $\sigma$ with different conditioning. The maximum of $P(\sigma | D)$ is at $\sigma_{N-1}$ and the maximum of $P(\sigma | D, \mu = \bar{x})$ is $\sigma_N$, using uninformative priors. In other words, $\sigma_{N-1}$ is when we are jointly estimating our uncertainty in $\mu$ and $\sigma_N$ is when we hold $\mu$ fixed at $\bar{x}$. There are nice supporting visuals on page 321.<p>One of the more lively and entertaining chapters is 37, <em>Bayesian Inference and Sampling Theory</em>. In this short chapter, he offers very simple examples to demonstrate that frequentist methods calculate unhelpful quantities to the decision at hand or are sensitive to irrelevant information.<h2 id=nitpick>Nitpick</h2><p>Perhaps the one thing I didn't care for in the book was the early insistence on approximating $x!$ and ${n \choose x}$, which seemed like a bit of an unnecessary distraction. This did not last long, however.<h2 id=problems>Problems</h2><p>The book offers a lot of problems, each with a difficulty rating between 1 and 3 next to it. Some of the problems are quite challenging. I recognized one problem as being a slight reframing of a <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/William_Lowell_Putnam_Mathematical_Competition target=_blank>Putnam</a> A6 question, and it was only marked with a difficulty of 2. Luckily, he also marked a small portion as recommended problems, and these problems were thoughtfully chosen to reinforce the content.<h2 id=recommendation>Recommendation</h2><p>Overall, I see this book as being a good one to read after Kevin Murphy's <a href=https://dfd.github.io/probabilistic-machine-learning/>Probabilistic Machine Learning</a>. They draw from a similar perspective, but MacKay's goes deeper within what it covers.<p>So my recommendation is to read the whole book, after PML, and do all of the recommended problems. The book is comprised of 50 relatively short chapters. I think readers could alternate between reading a chapter and then doing its recommended problems, and complete the book in 100 days without much problem. Very motivated readers could probably finish in 50 days by reading a chapter and completing the problems each day. Consider it a "must read" if you want to go deep into NLP.</div><div class="columns box-footer is-12"><div class="column is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-cubes" title=categories></i> </span> <span> <a class=has-text-weight-semibold href=https://dfd.github.io/categories/machine-learning/>Machine Learning</a>, <a class=has-text-weight-semibold href=https://dfd.github.io/categories/statistics/>Statistics</a>, <a class=has-text-weight-semibold href=https://dfd.github.io/categories/information-theory/>Information Theory</a> </span> </span></div><div class="column has-text-right-desktop is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-tags" title=tags></i> </span> <a class=has-text-weight-semibold href=https://dfd.github.io/tags/book-review/>book review</a> </span></div></div></article></div><div class="column is-2 is-hidden-mobile"><aside class=menu style=position:sticky;top:48px><p class="heading has-text-weight-bold">Contents<ul class=menu-list><li><a class="toc is-size-7 is-active" href=https://dfd.github.io/mackay-itila/#overview id=link-overview> Overview </a><li><a class="toc is-size-7" href=https://dfd.github.io/mackay-itila/#a-unique-perspective id=link-a-unique-perspective> A unique perspective </a><li><a class="toc is-size-7" href=https://dfd.github.io/mackay-itila/#diverse-topics id=link-diverse-topics> Diverse topics </a><li><a class="toc is-size-7" href=https://dfd.github.io/mackay-itila/#relevance-to-nlp-and-deep-learning id=link-relevance-to-nlp-and-deep-learning> Relevance to NLP and deep learning </a><li><a class="toc is-size-7" href=https://dfd.github.io/mackay-itila/#comparison-to-frequentist-methods id=link-comparison-to-frequentist-methods> Comparison to frequentist methods </a><li><a class="toc is-size-7" href=https://dfd.github.io/mackay-itila/#nitpick id=link-nitpick> Nitpick </a><li><a class="toc is-size-7" href=https://dfd.github.io/mackay-itila/#problems id=link-problems> Problems </a><li><a class="toc is-size-7" href=https://dfd.github.io/mackay-itila/#recommendation id=link-recommendation> Recommendation </a></ul></aside></div></div></div></section><section class=section><div class=container><div class="columns is-centered"><div class="column is-8"><nav class=level><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/probabilistic-machine-learning/> <span class="icon mr-2"> <i class="fas fa-arrow-circle-left"></i> </span> Book Review: Probabilistic Machine Learning: An Introduction </a></div><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/mcgrayne-the-theory-that-would-not-die/> Book Review: The Theory That Would Not Die<span class="icon ml-2"> <i class="fas fa-arrow-circle-right"></i> </span> </a></div></nav></div></div></div></section><footer class="footer py-4"><div class="content has-text-centered"><p><span class=icon> <i class="fas fa-power-off"></i> </span> Powered by <span class=icon-text> <span><a href=https://www.getzola.org/>zola</a></span> </span></div></footer><script src=https://dfd.github.io/js/site.js></script><script>const menuBarHeight=document.querySelector("nav.navbar").clientHeight;const tocItems=document.querySelectorAll(".toc");const navSections=new Array(tocItems.length);tocItems.forEach((a,b)=>{let c=a.getAttribute("id").substring(5);navSections[b]=document.getElementById(c)});function isVisible(a){const b=navSections[a];const c=a<tocItems.length- 1?navSections[a+ 1]:document.querySelectorAll("section.section").item(1);const d=b.getBoundingClientRect();const e=c.getBoundingClientRect();const f=window.innerHeight||document.documentElement.clientHeight;return d.top<=f&&e.top- menuBarHeight>=0}function activateIfVisible(){for(b=true,i=0;i<tocItems.length;i++){if(b&&isVisible(i)){tocItems[i].classList.add('is-active');b=false}else tocItems[i].classList.remove('is-active')}}var isTicking=null;window.addEventListener('scroll',()=>{if(!isTicking){window.requestAnimationFrame(()=>{activateIfVisible();isTicking=false});isTicking=true}},false)</script>