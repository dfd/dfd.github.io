<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content=#ffffff name=theme-color><meta content=#da532c name=msapplication-TileColor><link href=https://dfd.github.io/rss.xml rel=alternate title=RSS type=application/rss+xml><link href=/icons/site.webmanifest rel=manifest><link color=#5bbad5 href=/icons/safari-pinned-tab.svg rel=mask-icon><link href=/icons/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/icons/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/icons/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link crossorigin href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.1/css/academicons.min.css integrity=sha384-FIue+PI4SsI9XfHCz8dBLg33b0c1fMJgNU3X//L26FYbGnlSEfWmNT7zgWc2N9b6 rel=stylesheet><link integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css rel=stylesheet><link href=https://dfd.github.io/deep-thought.css rel=stylesheet><title>
    
Implicit Assumptions | Book Review: Statistical Rethinking

  </title><script async data-beampipe-domain=dfd.github.io defer src=https://beampipe.io/js/tracker.js></script><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs rel=stylesheet><script crossorigin defer integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js></script><script crossorigin defer integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mathtex-script-type.min.js></script><script crossorigin defer integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js></script><body class=has-background-white><nav aria-label="section navigation" class="navbar is-light" role=navigation><div class=container><div class=navbar-brand><a class="navbar-item is-size-5 has-text-weight-bold" href=https://dfd.github.io>Implicit Assumptions</a><a class="navbar-burger burger" aria-expanded=false aria-label=menu data-target=navMenu role=button> <span aria-hidden=true></span> <span aria-hidden=true></span> <span aria-hidden=true></span> </a></div><div class=navbar-menu id=navMenu><div class="navbar-end has-text-centered"><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/> Posts </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/tags> Tags </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/categories> Categories </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/about> About </a><a title="Switch to dark theme" class=navbar-item id=dark-mode> <span class=icon> <i class="fas fa-adjust"></i> </span> </a></div></div></div></nav><section class=section><div class=container><div class=columns><div class="column is-8 is-offset-2"><article class=box><h1 class=title>Book Review: Statistical Rethinking</h1><div class="columns is-multiline is-gapless"><div class="column is-8"><span class="icon-text has-text-grey"> <span class=icon-special> <i class="far fa-calendar-alt" title="publish date"></i> </span> <span><time datetime=2024-07-24>July 24, 2024</time></span> <span class=icon-special> <i class="fas fa-user" title=author></i> </span> <span>Dave</span> </span></div><div class="column is-4 has-text-right-desktop"></div></div><div class="content mt-2"><p>I'll start with the conclusion: McElreath's <a href="https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/036713991X/ref=pd_sim_d_sccl_3_5/141-3103872-8081264?psc=1" rel="noopener nofollow noreferrer" target=_blank>Staistical Rethinking</a> is my favorite textbook. In the rest of this post I'll explain why and offer advice on how to get the most out of this book.</p><span id=continue-reading></span><h2 id=my-background-before-reading-this-book>My background before reading this book</h2><p>By the time I started reading the first edition of this book in late 2017, I was already leading a sizable data science and analytics team and had a masters degree in statistics. For that degree, most courses used frequentist methods, although I also took the one Bayesian statistics course that was offered. So I wasn't exactly a beginner, and yet I still claim that I learned at least as much from this book as I did from that degree program. I later used this book to teach a course at my old company, so I've also seen how well it works for those with non-stats backgrounds.<p>In this post, I'll refer to the 2nd edition.<h2 id=connecting-methods-to-science>Connecting methods to science</h2><p>McElreath starts off by presenting the <a href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-01?slide=28" rel="noopener nofollow noreferrer" target=_blank>typical maze of statistical tests</a> students are presented with in a statistics curriculum, and explains why this often causes confusion. He then goes on to argue that these tests are not enough for research because they are difficult to adapt to unique contexts and fail in unpredictable ways.<p>Chapter one continues on to describe why deductive falsification, which is often used as the justification for applying frequentist null hypothesis tests, is impossible in most scientific contexts. This is because "many models correspond to the same hypothesis, and many hypotheses correspond to a single model," which "makes strict falsification impossible." This idea is <a href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-01?slide=34" rel="noopener nofollow noreferrer" target=_blank>illustrated here</a>, which shows that many processes are consistent with stated hypotheses, and many processes may be consistent with a single statistical model.<p>Measurement errors and continuous hypotheses also make falsification challenging in practice, and because of these issues, falsification is consensual rather than logical. That is, scientists debate the merits of the evidence and come to a consensus over time.<p>The remainder of the chapter is spent outlining the methods for doing better science that are covered in the book: Bayesian data analysis, model comparison, multilevel models, and graphical casual models.<h2 id=illustrative-explanations>Illustrative explanations</h2><p>I think people like this book so much because the explanations bring the concepts to life, giving the reader an intuition that goes well beyond the definitions. These are given throughout the book, but here are two examples that have always stuck with me:<p>Chapter 2 contains perhaps the best illustration of a likelihood function that I've seen. It does so by presenting a simple case, in which a bag contains four marbles, each of which is blue or white. Three marbles are drawn with replacement, and their colors are the observed data. The unknown parameter of interest, $p$, is the proportion of marbles that are blue, so this sets up a discrete parameter, discrete outcome case. The likelihood is calculated by enumerating all of the possible ways data can be generated for each conjecture (each possible value of parameter $p \in \{ 0,\frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1 \}$ ) by taking three draws from the bag, and highlighting which possible outcomes match the observed outcome. <a href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2022-lecture-02?slide=5" rel="noopener nofollow noreferrer" target=_blank>You can see the example play out over consecutive slides starting here.</a> The lesson is simple: the likelihood is the relative number of ways that the observed data can occur for each value of the parameter. While such literal counting is only feasible in the discrete/discrete case, it gives the right intuition for continuous cases as well.<p>Another explanation that stands out is for KL Divergence and demonstrating why it is not symmetric like a distance metric. The example given concerns predicting whether we will land on land or water, using Earth to set expectations for landing on Mars and then using Mars to set expectations for Earth. Mars' surface is almost entirely land (we count the ice caps as water on 1% of the surface for the sake of the example), while Earth's surface is mostly water but with a significant amount of land. So if you randomly land on Mars while using Earth to guide expectations, you'll be a little surprised but not shocked to (almost certainly) touch down on land. If you randomly land on Earth using Mars to guide your expectations, there's a good chance you'll land in water, which will be shocking if you were picturing the 1% water on Mars. The extra surprise means that using the proportions of land and water on Mars to guide expectations about Earth produces larger KL divergence than vice versa.<h2 id=insightful-plots>Insightful plots</h2><p>A strength of this book is the plots used for model checking. It conveys good habits for plotting relationships and comparing the model to data. As every experienced practitioner knows, plotting adds valuable information far beyond what model fit metrics can provide alone.<h2 id=bayesian-philosophy>Bayesian philosophy</h2><p>While he points out advantages of Bayesian methods over frequentist ones, McElreath avoids making sweeping claims for Bayesian methods. In fact, he argues that no statistical approach by itself is sufficient, and there's a healthy humility as he discusses the modeling process.<p>The book takes the "logical" view of probability (<a rel="noopener nofollow noreferrer" href=https://bayes.wustl.edu/etj/prob/book.pdf target=_blank>see here</a> for a long explanation), often associated with Jaynes, and also does a nice job delineating epistemology from ontology. The simulation of tadpoles in tanks and the role of hyperparameters in the <em>Models with Memory</em> chapter is a good example of clarifying this difference, as is the discussion of the i.i.d. assumption and Jaynes' <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Mind_projection_fallacy target=_blank><em>mind projection fallacy</em></a>.<h2 id=working-up-to-multilevel-modeling-and-beyond>Working up to multilevel modeling and beyond</h2><p>The book is sensibly organized to gradually build up to multilevel models with partial pooling, which is one of the Bayesian super powers. Along the way, the book introduces essential and useful topics for a practicing statistician, including spurious association, masked relationships, confounding variables, interactions, and causal DAGs. It also covers modeling with Gaussian, binomial, Poisson, categorical, zero-inflated, and ordinal response variables.<p>It consistently presents the formal specification of each model before moving onto the code. It's a great practice to get into and helps to clarify and communicate a model's intention. Plus, modern Bayesian libraries use declarative model syntax that closely aligns with this format, bridging conceptual understanding and assumptions to code.<p>Continuing a bit beyond multilevel models to some other advanced topics, the book includes covariance, Gaussian Processes, missing data, measurement error, and instrument variables. After I used it to teach the class at work, our team members were able to tackle some pretty sophisticated problems where quantifying uncertainty or leveraging a flexible model structure were crucially important.<h2 id=traditional-topics-missing>Traditional topics missing</h2><p>One thing the book skips over, which is normally covered in an introductory Bayesian course, is solving simple problems with conjugate priors analytically. In practice, it's only applicable to a very small set of problems. But I think doing the math by hand for a few examples will help some learners better understand what the MCMC sampling is approximating. I don't think this belongs in McElreath's book; it's just a suggestion to find another resource to try that out separately, perhaps before reading this book, or sometime around chapter 3 or 4.<p>Continuing along those lines, it's generally not a math heavy book. The math is usually presented, but not required, since most problems are solved computationally. I think this serves the book and its scope well, but for moving on to more advanced material afterwards, I think a mathematical statistics course or book would be helpful to supplement. <a href=https://dfd.github.io/probabilistic-machine-learning/>Probabilistic Machine Learning</a> (PML) actually covers this material quite well (and also includes a section on conjugate prior models). Having that background ahead of time would probably help comprehend this book a bit better, but I don't think it's necessary.<p>Another traditional topic is implementing your own <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Gibbs_sampling target=_blank>Gibbs</a> and <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm target=_blank>Metropolis Hastings</a> samplers. This is again not particularly practical with modern libraries for Bayesian inference that do the hard work for you (and tend to use <a rel="noopener nofollow noreferrer" href=https://arxiv.org/abs/1701.02434 target=_blank>Hamiltonian Monte Carlo</a>). I don't necessarily think a reader needs to supplement with this, although diving into the sampling algorithms may help with diagnosing problems and understanding when reparameterization is helpful in more complex models. <a rel="noopener nofollow noreferrer" href=https://probml.github.io/pml-book/book2.html target=_blank>PML2</a> covers MCMC sampling.<p>And finally another traditional topic not found in the book is <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Jeffreys_prior target=_blank>Jeffreys priors</a>. These are intellectually interesting, but rarely use in applied work. <a rel="noopener nofollow noreferrer" href=https://probml.github.io/pml-book/book2.html target=_blank>PML2</a> covers Jeffreys priors.<h2 id=evolving-editions>Evolving editions</h2><p>As mentioned, I originally read the first edition of the book, which was already great. I also bought a copy of the second edition, which added several more topics, but the main difference may be how it stresses causal diagrams (DAGs) throughout. The forthcoming 3rd edition <a rel="noopener nofollow noreferrer" href=https://elevanth.org/2024_01_02_third_edition.html target=_blank>will apparently feature more on the Bayesian workflow</a>.<h2 id=recommendation>Recommendation</h2><p>McElreath thoughtfully weaves together philosophical, conceptual, and computational considerations through motivating examples and insightful illustrations. If you want to learn (or relearn) statistics in a way that will clarify your thinking and prepare you for a wide variety of modeling problems, this is a must read. I'll describe how I went through the book, which is what I'd recommend to get the most out of it.<p>When I read this book, I wanted to make a collection of the code examples from it in Python. It's such a popular book that the author's code, which uses an <a rel="noopener nofollow noreferrer" href=https://www.rdocumentation.org/packages/rethinking/ target=_blank>R package the author created</a>, has been translated to several other probabilistic programming languages (PPLs). I was using PyMC3 at the time (now replaced by PyMC version 5), so I grabbed an available translation for it, and started making my own notebooks. However, I noticed there were some differences between results from the resource I found and the book's results. I think doing the full translation from scratch myself would have been tedious, but figuring out how to correct some examples was helpful.<p>By the way, I don't think which framework you choose is that important. In fact, when I taught the class, I stressed that our chosen framework was somewhat arbitrary and that those will change over time; what's important is learning the concepts and practicing with modern MCMC samplers. In practice, I've used <a rel="noopener nofollow noreferrer" href=https://www.pymc.io/welcome.html target=_blank>pymc</a>, <a rel="noopener nofollow noreferrer" href=https://num.pyro.ai/en/stable/index.html target=_blank>numpyro</a>, and <a rel="noopener nofollow noreferrer" href=https://mc-stan.org/ target=_blank>stan</a> for various projects.<p>I also reproduced every plot in the book with code. Much of McElreath's plotting code was not published in the book, so figuring that out also helped my understanding of the mounds of samples the models were generating. Plotting is so important for model improvement.<p>And finally, as preparation for teaching a class with the book, I selected a small number of problems to do from each chapter. I recommend choosing at least one conceptual question, and one computational question (and ideally one from each major topic) from each chapter to help get hands on experience with the material in the book. McElreath also publishes homework problems and solutions from his course, which is a good option.<p>At the end of it, I had an amazing set of code examples in a framework I wanted to use, along with a solid understanding of how to approach a wide variety of problems. This book on its own can bring an active learner up to an intermediate level. It's also great conceptual preparation for more dense books like <a href=https://dfd.github.io/probabilistic-machine-learning/>PML</a> or <a rel="noopener nofollow noreferrer" href=http://www.stat.columbia.edu/~gelman/book/ target=_blank>BDA3</a>.<p>Take your time with this one. You'll get out of it what you put into it.<h2 id=extra-resources>Extra Resources</h2><p>McElreath publishes <a rel="noopener nofollow noreferrer" href=https://www.youtube.com/@rmcelreath/playlists target=_blank>lectures for the book here</a>. You can just find the latest (or whichever one matches your book's edition) and follow along. Or you can look for the latest class <a href="https://github.com/rmcelreath?tab=repositories" rel="noopener nofollow noreferrer" target=_blank>github repo</a> with lectures and assigned homework and solutions here (specific example <a rel="noopener nofollow noreferrer" href=https://github.com/rmcelreath/stat_rethinking_2024 target=_blank>here</a>)<p>I mentioned translations for various PPLs. Here are some for the 2nd edition of the book.<ul><li><a rel="noopener nofollow noreferrer" href=https://github.com/pymc-devs/pymc-resources/tree/main/Rethinking_2 target=_blank>Translation for PyMC.</a><li><a rel="noopener nofollow noreferrer" href=https://fehiepsi.github.io/rethinking-numpyro/ target=_blank>Translation for numpyro.</a><li><a rel="noopener nofollow noreferrer" href=https://vincentarelbundock.github.io/rethinking2/ target=_blank>Translation for rstan.</a></ul><p>I haven't seen a complete translation for pystan, but stan itself is a language, so it should be relatively straightforward to convert to pystan for those with python data experience.</div><div class="columns box-footer is-12"><div class="column is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-cubes" title=categories></i> </span> <span> <a class=has-text-weight-semibold href=https://dfd.github.io/categories/statistics/>Statistics</a> </span> </span></div><div class="column has-text-right-desktop is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-tags" title=tags></i> </span> <a class=has-text-weight-semibold href=https://dfd.github.io/tags/book-review/>book review</a> </span></div></div></article></div><div class="column is-2 is-hidden-mobile"><aside class=menu style=position:sticky;top:48px><p class="heading has-text-weight-bold">Contents<ul class=menu-list><li><a class="toc is-size-7 is-active" href=https://dfd.github.io/mcelreath-statistical-rethinking/#my-background-before-reading-this-book id=link-my-background-before-reading-this-book> My background before reading this book </a><li><a class="toc is-size-7" href=https://dfd.github.io/mcelreath-statistical-rethinking/#connecting-methods-to-science id=link-connecting-methods-to-science> Connecting methods to science </a><li><a class="toc is-size-7" href=https://dfd.github.io/mcelreath-statistical-rethinking/#illustrative-explanations id=link-illustrative-explanations> Illustrative explanations </a><li><a class="toc is-size-7" href=https://dfd.github.io/mcelreath-statistical-rethinking/#insightful-plots id=link-insightful-plots> Insightful plots </a><li><a class="toc is-size-7" href=https://dfd.github.io/mcelreath-statistical-rethinking/#bayesian-philosophy id=link-bayesian-philosophy> Bayesian philosophy </a><li><a class="toc is-size-7" href=https://dfd.github.io/mcelreath-statistical-rethinking/#working-up-to-multilevel-modeling-and-beyond id=link-working-up-to-multilevel-modeling-and-beyond> Working up to multilevel modeling and beyond </a><li><a class="toc is-size-7" href=https://dfd.github.io/mcelreath-statistical-rethinking/#traditional-topics-missing id=link-traditional-topics-missing> Traditional topics missing </a><li><a class="toc is-size-7" href=https://dfd.github.io/mcelreath-statistical-rethinking/#evolving-editions id=link-evolving-editions> Evolving editions </a><li><a class="toc is-size-7" href=https://dfd.github.io/mcelreath-statistical-rethinking/#recommendation id=link-recommendation> Recommendation </a><li><a class="toc is-size-7" href=https://dfd.github.io/mcelreath-statistical-rethinking/#extra-resources id=link-extra-resources> Extra Resources </a></ul></aside></div></div></div></section><section class=section><div class=container><div class="columns is-centered"><div class="column is-8"><nav class=level><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/mcgrayne-the-theory-that-would-not-die/> <span class="icon mr-2"> <i class="fas fa-arrow-circle-left"></i> </span> Book Review: The Theory That Would Not Die </a></div><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/clayton-bernoullis-fallacy/> Book Review: Bernoulli's Fallacy<span class="icon ml-2"> <i class="fas fa-arrow-circle-right"></i> </span> </a></div></nav></div></div></div></section><footer class="footer py-4"><div class="content has-text-centered"><p><span class=icon> <i class="fas fa-power-off"></i> </span> Powered by <span class=icon-text> <span><a href=https://www.getzola.org/>zola</a></span> </span></div></footer><script src=https://dfd.github.io/js/site.js></script><script>const menuBarHeight=document.querySelector("nav.navbar").clientHeight;const tocItems=document.querySelectorAll(".toc");const navSections=new Array(tocItems.length);tocItems.forEach((a,b)=>{let c=a.getAttribute("id").substring(5);navSections[b]=document.getElementById(c)});function isVisible(a){const b=navSections[a];const c=a<tocItems.length- 1?navSections[a+ 1]:document.querySelectorAll("section.section").item(1);const d=b.getBoundingClientRect();const e=c.getBoundingClientRect();const f=window.innerHeight||document.documentElement.clientHeight;return d.top<=f&&e.top- menuBarHeight>=0}function activateIfVisible(){for(b=true,i=0;i<tocItems.length;i++){if(b&&isVisible(i)){tocItems[i].classList.add('is-active');b=false}else tocItems[i].classList.remove('is-active')}}var isTicking=null;window.addEventListener('scroll',()=>{if(!isTicking){window.requestAnimationFrame(()=>{activateIfVisible();isTicking=false});isTicking=true}},false)</script>