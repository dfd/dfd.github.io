<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content=#ffffff name=theme-color><meta content=#da532c name=msapplication-TileColor><link href=https://dfd.github.io/rss.xml rel=alternate title=RSS type=application/rss+xml><link href=/icons/site.webmanifest rel=manifest><link color=#5bbad5 href=/icons/safari-pinned-tab.svg rel=mask-icon><link href=/icons/favicon-16x16.png rel=icon sizes=16x16 type=image/png><link href=/icons/favicon-32x32.png rel=icon sizes=32x32 type=image/png><link href=/icons/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link crossorigin href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.1/css/academicons.min.css integrity=sha384-FIue+PI4SsI9XfHCz8dBLg33b0c1fMJgNU3X//L26FYbGnlSEfWmNT7zgWc2N9b6 rel=stylesheet><link integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css rel=stylesheet><link href=https://dfd.github.io/deep-thought.css rel=stylesheet><title>
    
Implicit Assumptions | Book Review: Probabilistic Machine Learning: An Introduction

  </title><script async data-beampipe-domain=dfd.github.io defer src=https://beampipe.io/js/tracker.js></script><link crossorigin href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs rel=stylesheet><script crossorigin defer integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js></script><script crossorigin defer integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mathtex-script-type.min.js></script><script crossorigin defer integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js></script><body class=has-background-white><nav aria-label="section navigation" class="navbar is-light" role=navigation><div class=container><div class=navbar-brand><a class="navbar-item is-size-5 has-text-weight-bold" href=https://dfd.github.io>Implicit Assumptions</a><a class="navbar-burger burger" aria-expanded=false aria-label=menu data-target=navMenu role=button> <span aria-hidden=true></span> <span aria-hidden=true></span> <span aria-hidden=true></span> </a></div><div class=navbar-menu id=navMenu><div class="navbar-end has-text-centered"><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/> Posts </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/tags> Tags </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/categories> Categories </a><a class="navbar-item has-text-weight-semibold" href=https://dfd.github.io/about> About </a><a title="Switch to dark theme" class=navbar-item id=dark-mode> <span class=icon> <i class="fas fa-adjust"></i> </span> </a></div></div></div></nav><section class=section><div class=container><div class=columns><div class="column is-8 is-offset-2"><article class=box><h1 class=title>Book Review: Probabilistic Machine Learning: An Introduction</h1><div class="columns is-multiline is-gapless"><div class="column is-8"><span class="icon-text has-text-grey"> <span class=icon-special> <i class="far fa-calendar-alt" title="publish date"></i> </span> <span><time datetime=2024-03-30>March 30, 2024</time></span> <span class=icon-special> <i class="fas fa-user" title=author></i> </span> <span>Dave</span> </span></div><div class="column is-4 has-text-right-desktop"></div></div><div class="content mt-2"><p>I'm surprised people aren't making a bigger deal about Kevin Murphy's new textbooks, the first of which I'll review here.</p><span id=continue-reading></span><h2 id=overview>Overview</h2><p><a rel="noopener nofollow noreferrer" href=https://probml.github.io/pml-book/book1.html target=_blank>Probabilistic Machine Learning: An Introduction</a> covers an incredible breadth and surprising depth of machine learning and statistics topics. It can be thought of as a "best of" from <a rel="noopener nofollow noreferrer" href=https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf target=_blank>Bishop</a>, <a rel="noopener nofollow noreferrer" href=http://www.inference.org.uk/mackay/itila/ target=_blank>MacKay</a>, <a rel="noopener nofollow noreferrer" href=https://hastie.su.domains/ElemStatLearn/ target=_blank>The Elements of Statistical Learning</a>, and others, along with a view of recent, relevant research, all of which is pulled together under a probabilistic persepctive and consistent notation. The book is almost entirely self-contained, with an extensive <em>Foundations</em> section covering most prerequisite topics<sup class=footnote-reference><a href=#prereqs>1</a></sup> before moving on to linear models, deep neural networks, nonparametric models, unsupervised learning, and a few other topics.<h2 id=clear-exposition>Clear exposition</h2><p>The book takes a Bayesian view of probability (that we can treat all unknown quantities, such as future outcomes and parameters, as random variables and model them with probability distributions) and applies it to statistical and machine learning models. In doing so, Murphy is able to apply a consistent framework and common concepts across a very wide range of topics, regardless of how they were originally motivated or conceived. This approach turns the exploration of various models into a seamless composition of building blocks, rather than a series of jumps between the origin stories of each model.<p>Perhaps surprisingly, the book manages to cover the most important<sup class=footnote-reference><a href=#important>2</a></sup> concepts one would encounter in graduate statistics coursework. In fact, even after exiting the aforementioned <em>Foundations</em> section, it spends a good chunk of the book on linear models, much of which would overlap with a statistics curriculum. This is helpful for two reasons:<ol><li>First, it's important to apply the right tool for the job. While complex ML models increasingly do amazing things, there are still many applications and contexts when we need the imposed structure of GLMs and other simpler models.<li>More importantly, it introduces the structure of Murphy's approach, which extends naturally from the <em>Foundations</em> section, to relatively simple models before moving on to more complex topics. This allows the reader to more easily adapt and generalize the framework.</ol><p>A common pattern is:<ol><li>Motivate the model and define the likeliihood, such as $$ p(y| \mathbf{x}; \mathbf{\theta}) = \text{Ber}(y|\mathbf{\sigma}(\mathbf{w}^\intercal \mathbf{x} + b)) $$ in the case of logistic regression, adding definitions, priors, or nested functions if applicable.<li>Define and derive the objective function, such as the negative log likelihood.<li>Show how the optimization of this objective is done in practice.</ol><p>The book extensively borrows from earlier textbooks and papers for explanations, figures, and problems, always with proper attribution. In fact, it may have the most citations per page I've encountered. Murphy has meticulously adapted the clearest plots and explanations for each subject and selected problems that provoke deep insights into the material. Additionally, the extensive use of self-references within the book help to facilitate refreshing an earlier topics as needed.<h2 id=practical-theory>Practical theory</h2><p>In practice, I find the the probabilistic perspective to be much more useful than the theory emphasized in some other ML books. For instance, while I understand the role of <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension target=_blank>VC-dimension</a> and <a rel="noopener nofollow noreferrer" href=https://en.wikipedia.org/wiki/Probably_approximately_correct_learning target=_blank>PAC learning</a> in the development of statistical learning theory, I'm rarely utilizing those ideas in applied work.<sup class=footnote-reference><a href=#SLT>3</a></sup> Murphy's book explains them in under two pages, while some other books devote entire chapters to each of these concepts.<p>Also, as he notes, the probabilistic perspective lends itself to optimal decision making and is shared across science and engineering disciplines, which is what helps fuel the breadth of the two books.<h2 id=breadth-and-depth>Breadth and depth</h2><p>Have a look at the <a rel="noopener nofollow noreferrer" href=https://probml.github.io/pml-book/toc1.pdf target=_blank>table of contents</a>. I don't know of another textbook that covers linear Gaussian systems, GLMs, and transformers, not to mention Gaussian processes, factorization machines, and graph embeddings (plus the <a rel="noopener nofollow noreferrer" href=https://github.com/probml/pml2-book/blob/main/toc2-long-2023-01-19.pdf target=_blank>even longer sequel book on "advanced topics"</a> <em>and</em> the <a rel="noopener nofollow noreferrer" href=https://github.com/probml/pml-book/blob/main/supp2.md target=_blank>supplemental material</a> to that!). The power of his approach is that it allows him to write succintly, yet clearly and precisely, on a variety of subjects, adding color and intuition when needed.<p>The book examines the most crucial and fundamental topics, offering detailed proofs and derivations, unveiling important results, and comparing the performance of competing methods across various problems. For example, linear regression methods are thoroughly explored over forty pages. Furthermore, the text dedicates three chapters to deep neural networks, addressing structured data, images, and sequences specifically. In contrast, it touches briefly on many specialized and niche topics, subsequently directing readers to one or more citations for further investigation.<h2 id=source-code>Source code</h2><p>While the text focuses on the theory, concepts, and math, Murphy makes available the <a rel="noopener nofollow noreferrer" href=https://github.com/probml/pyprobml/tree/master/notebooks/book1 target=_blank>code to reproduce most of the figures</a>. This often includes experiments and model fitting, for those who want to try out models from the book. I have not used the code much myself, although I did find <a rel="noopener nofollow noreferrer" href=https://github.com/probml/pyprobml/issues/1113 target=_blank>an issue with the mixture of linear experts code</a>.<h2 id=corrections>Corrections</h2><p>I preordered a hard copy of the first printing and, while meticulously working through it, encountered several typos, some even within the mathematical content. For each typo discovered, I referred to the <a rel="noopener nofollow noreferrer" href=https://github.com/probml/pml-book/releases/latest/download/book1.pdf target=_blank>latest draft version</a> to verify if it had already been corrected. Many had been previously identified and fixed, yet I was still able to report <a href="https://github.com/probml/pml-book/issues?q=is%3Aissue+author%3Adfd+" rel="noopener nofollow noreferrer" target=_blank>several new ones</a>, all of which were subsequently addressed. Contributing in this way to such an important book was an unexpected perk.<p>In a weird way, the typos actually enhanced my experience, since catching them was a nice way to validate I understood the material well (your mileage may vary). Collecting corrections via github probably increased reader participation, and I would assume the current draft is in great shape as a result.<h2 id=recommendation>Recommendation</h2><p>Probabilistic Machine Learning is one of two introductory ML textbooks I would recommend. I think its perspective is the perfect complement to the inductive bias<sup class=footnote-reference><a href=#inductive>4</a></sup> lens found in Tom Mitchell's classic <a rel="noopener nofollow noreferrer" href=http://www.cs.cmu.edu/~tom/mlbook.html target=_blank>Machine Learning</a> textbook.<p>If you're looking to develop expertise in machine learning, I recommend going through this book cover to cover and attempting each problem provided. You might not be able to solve them all, so time box each attempt. You can check <a rel="noopener nofollow noreferrer" href=https://probml.github.io/pml-book/solns-public.pdf target=_blank>solutions to many of the problems</a>, but give each of them a shot on your own first.<p>I was able to move through it relatively quickly, but it helped that I already had graduate degrees focused on statistics and machine learning.<sup class=footnote-reference><a href=#experience>5</a></sup> If you find some of your math background is shaky or that the <em>Foundations</em> section is too dense, there are plenty of online resources available to help out. I particularly like the <a rel="noopener nofollow noreferrer" href=https://www.coursera.org/specializations/mathematics-machine-learning target=_blank>Mathematics for Machine Learning</a> course from Imperial College London for linear algebra and multivariate calculus. If you need to supplement with such outside sources, I would still use the structure within PML to guide what specific topics to brush up on, rather than committing to an endless math curriculum before moving on to ML. This would also be an ideal book for readers with a Bayesian statistics background to dive into machine learning, since they will be familiar with the framing.<p>If you find a particular subject interesting, grab the <a rel="noopener nofollow noreferrer" href=https://github.com/probml/pyprobml/tree/master/notebooks/book1 target=_blank>associated code</a> and apply it to some new problems or run experiments. If you feel like you've mastered a chapter of the book, try to generate data sets that deliberately demonstrate the comparative strengths or weaknesses of the models within it.<h2 id=conclusion>Conclusion</h2><p>This has become my go-to reference on ML. Whenever I encounter a question about ML or need to refresh my memory on a topic, I usually go straight to this book (or <a rel="noopener nofollow noreferrer" href=https://probml.github.io/pml-book/book2.html target=_blank>its sequel</a>) over google, wikipedia, or other books.<p>For a long time, it's been common for study groups to read through <a rel="noopener nofollow noreferrer" href=https://hastie.su.domains/ElemStatLearn/ target=_blank>ESL</a> together while learning ML. I think some of its sections are good, but it never appealed to me in the same way and relies too much on null hypothesis testing and p-values for my taste. <em>Probabilistic Machine Learning</em> now offers a better, modern alternative to cultivate a way of thinking that extends well beyond what many people narrowly think of as "machine learning."<p>The book is <a rel="noopener nofollow noreferrer" href=https://probml.github.io/pml-book/book1.html target=_blank>freely available online</a>; however, considering its extensive length, opting for a hard copy allows for a much-needed respite from computer screens. You can <a rel="noopener nofollow noreferrer" href=https://www.amazon.com/Probabilistic-Machine-Learning-Introduction-Computation/dp/0262046822 target=_blank>purchase it here</a>. And if you want to hear more about the author's perspective, you <a rel="noopener nofollow noreferrer" href=https://learnbayesstats.com/episode/68-probabilistic-machine-learning-generative-models-kevin-murphy/ target=_blank>can listen to a podcast interview here</a>.<p>I also have the <a rel="noopener nofollow noreferrer" href=https://probml.github.io/pml-book/book2.html target=_blank>second book</a>, but haven't yet had time to read it (although I've already used it as a reference on several topics). Hopefully I will start it later this year.<h2 id=footnotes>Footnotes</h2><div class=footnote-definition id=prereqs><sup class=footnote-definition-label>1</sup><p>The book assumes knowledge of basic set theory and calculus, and comfort with math notation will help, although there is an appendix for that. It <em>very briefly</em> covers derivatives and matrix calculus, so that might be challenging for someone new to those subjects. Integration is not covered within the book, so I think it's fair to say that a reader should know calculus well before starting.</div><div class=footnote-definition id=important><sup class=footnote-definition-label>2</sup><p>While it briefly describes null hypothesis testing, it does not <a href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-01?slide=28" rel="noopener nofollow noreferrer" target=_blank>devolve into various statistical tests</a>. I said <em>important</em>, not <em>common</em>.</div><div class=footnote-definition id=SLT><sup class=footnote-definition-label>3</sup><p>These concepts may help develop an intuition about machine learning, but they are certainly not the concepts I start with when facing a new prediction problem.</div><div class=footnote-definition id=inductive><sup class=footnote-definition-label>4</sup><p>Inductive bias describes how a learning algorithm conducts its search over a hypothesis space, and can be further broken down into restriction bias (which hypotheses are considered) and preference bias (which hypotheses are preferred) for analysis. Mitchell's book covers much more than this, of course, but it's the unique aspect I find most useful.</div><div class=footnote-definition id=experience><sup class=footnote-definition-label>5</sup><p>Even with this background, I still learned quite a bit from the book.</div></div><div class="columns box-footer is-12"><div class="column is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-cubes" title=categories></i> </span> <span> <a class=has-text-weight-semibold href=https://dfd.github.io/categories/machine-learning/>Machine Learning</a> </span> </span></div><div class="column has-text-right-desktop is-6"><p><span class=icon-text> <span class=icon-special> <i class="fas fa-tags" title=tags></i> </span> <a class=has-text-weight-semibold href=https://dfd.github.io/tags/book-review/>book review</a> </span></div></div></article></div><div class="column is-2 is-hidden-mobile"><aside class=menu style=position:sticky;top:48px><p class="heading has-text-weight-bold">Contents<ul class=menu-list><li><a class="toc is-size-7 is-active" href=https://dfd.github.io/probabilistic-machine-learning/#overview id=link-overview> Overview </a><li><a class="toc is-size-7" href=https://dfd.github.io/probabilistic-machine-learning/#clear-exposition id=link-clear-exposition> Clear exposition </a><li><a class="toc is-size-7" href=https://dfd.github.io/probabilistic-machine-learning/#practical-theory id=link-practical-theory> Practical theory </a><li><a class="toc is-size-7" href=https://dfd.github.io/probabilistic-machine-learning/#breadth-and-depth id=link-breadth-and-depth> Breadth and depth </a><li><a class="toc is-size-7" href=https://dfd.github.io/probabilistic-machine-learning/#source-code id=link-source-code> Source code </a><li><a class="toc is-size-7" href=https://dfd.github.io/probabilistic-machine-learning/#corrections id=link-corrections> Corrections </a><li><a class="toc is-size-7" href=https://dfd.github.io/probabilistic-machine-learning/#recommendation id=link-recommendation> Recommendation </a><li><a class="toc is-size-7" href=https://dfd.github.io/probabilistic-machine-learning/#conclusion id=link-conclusion> Conclusion </a><li><a class="toc is-size-7" href=https://dfd.github.io/probabilistic-machine-learning/#footnotes id=link-footnotes> Footnotes </a></ul></aside></div></div></div></section><section class=section><div class=container><div class="columns is-centered"><div class="column is-8"><nav class=level><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/probably-overthinking-it/> <span class="icon mr-2"> <i class="fas fa-arrow-circle-left"></i> </span> Book Review: Probably Overthinking It </a></div><div class="level-item has-text-centered"><a class="button is-dark is-outlined" href=https://dfd.github.io/mackay-itila/> Book Review: Information Theory, Inference, and Learning Alogrithms<span class="icon ml-2"> <i class="fas fa-arrow-circle-right"></i> </span> </a></div></nav></div></div></div></section><footer class="footer py-4"><div class="content has-text-centered"><p><span class=icon> <i class="fas fa-power-off"></i> </span> Powered by <span class=icon-text> <span><a href=https://www.getzola.org/>zola</a></span> </span></div></footer><script src=https://dfd.github.io/js/site.js></script><script>const menuBarHeight=document.querySelector("nav.navbar").clientHeight;const tocItems=document.querySelectorAll(".toc");const navSections=new Array(tocItems.length);tocItems.forEach((a,b)=>{let c=a.getAttribute("id").substring(5);navSections[b]=document.getElementById(c)});function isVisible(a){const b=navSections[a];const c=a<tocItems.length- 1?navSections[a+ 1]:document.querySelectorAll("section.section").item(1);const d=b.getBoundingClientRect();const e=c.getBoundingClientRect();const f=window.innerHeight||document.documentElement.clientHeight;return d.top<=f&&e.top- menuBarHeight>=0}function activateIfVisible(){for(b=true,i=0;i<tocItems.length;i++){if(b&&isVisible(i)){tocItems[i].classList.add('is-active');b=false}else tocItems[i].classList.remove('is-active')}}var isTicking=null;window.addEventListener('scroll',()=>{if(!isTicking){window.requestAnimationFrame(()=>{activateIfVisible();isTicking=false});isTicking=true}},false)</script>