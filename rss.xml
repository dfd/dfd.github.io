<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>
    <channel>
      <title>Implicit Assumptions</title>
      <link>https://dfd.github.io</link>
      <description>Dave Decker&#x27;s blog</description>
      <generator>Zola</generator>
      <language>en</language>
      <atom:link href="https://dfd.github.io/rss.xml" rel="self" type="application/rss+xml"/>
      <lastBuildDate>Wed, 18 Feb 2026 00:00:00 +0000</lastBuildDate>
      <item>
          <title>A Solution to the Deluge of Bug Bounty and FOSS AI Slop Submissions</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Wed, 18 Feb 2026 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Development]]></category>
          <category><![CDATA[Economics]]></category>
          
          
          <category><![CDATA[solutions]]></category>
          
          <link>https://dfd.github.io/solution-ai-slop/</link>
          <guid>https://dfd.github.io/solution-ai-slop/</guid>
          <description><![CDATA[It seems many maintainers are drowning in AI submissions to bug bounties and FOSS contributions.  Many are frustrated by the volume of low quality PRs or submitters who don't seem to understand what they are submitting.
Some maintainers are resorting to extreme measures or crying…]]></description>
          <content:encoded><![CDATA[<p>It seems many maintainers are drowning in AI submissions to bug bounties and FOSS contributions.  Many are frustrated by the volume of low quality PRs or submitters who don't seem to understand what they are submitting.</p>
<p>Some maintainers are resorting to <a rel="noopener nofollow noreferrer" target="_blank" href="https://etn.se/index.php/nyheter/72808-curl-removes-bug-bounties.html">extreme measures</a> or <a rel="noopener nofollow noreferrer" target="_blank" href="https://bsky.app/profile/akien.bsky.social/post/3meyerixvhs2p">crying out for help</a>.  One is attempting to set up a whole new system of <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/mitchellh/vouch">social trust</a> to address the issue.</p>
<p>I'm happy to see experimentation in this area, and hope something shows efficacy and traction soon.  I want to throw another idea out into the ether that I think could help solve the problem.</p>
<p>But first, let's analyze the problem:</p>
<p>Generative AI coding has become very cheap.  Depending on how people use it, they either pay a flat fee, which gives them a marginal cost of zero to produce AI code (up to some usage limit) or they pay a very small “per token” fee.  In either case, the marginal cost of producing bug bounty submissions and open source pull requests has fallen to near zero, so people are doing much more of it.  It's not hard for someone to submit hundreds or thousands of submissions quickly with one of these subscriptions.  There was no cost to submit PRs or bug reports in the past, but it took a person's time, which is valuable.  But with LLMs, even that cost has shrunk dramatically.</p>
<p>The potential benefits to submitters can vary.  For bug bounties, it's for a potential financial reward, and for PRs, it may be to help their reputation, resume, or to test a new AI bot they whipped up.</p>
<p>Maintainers are annoyed at the low quality of submissions, in part because it's a cost (opportunity cost of their time) to review the PRs or bug bounty submissions.  It also crowds out reviews of new contributors who cannot be trusted by default.  For the purpose of this post, let's just take the maintainer's complaints as a given, and put aside any debate about whether AI could usefully help out.</p>
<p>I believe there's a relatively easy solution to this: the maintainers can charge a small fee to submit PRs or bug bounties to their projects.  If the maintainers then believe the submission was made in good faith (whether they accept the submission or not), they should return the fee.  If they don't believe it was made in good faith, they keep it.  Maintainers have an incentive to return the fee to good faith contributors so that they continue to receive good quality submissions.  Submissions not addressed in a timely manner could be automatically refunded, and merged PRs could automatically result in a refund.</p>
<p>That's it<sup class="footnote-reference"><a href="#integration">1</a></sup>.  That alone should greatly reduce AI slop submissions.  It takes something with a marginal cost near 0 and increases it enough to stop mass submissions of slop, because the submitters will risk losing money proportionally to the number of their slop submissions.  I don't know exactly where that price should be, maybe $1.  Maybe it varies by project.  It's hard to know what price will be enough to stop the slop deluge, but I don't think it would need to be too high to stop mass offenders, and I'm confident such a price exists.</p>
<p>To avoid discouraging good submissions, the result of each submission should be transparent, and the display of the result of returning the fee or not should be outside the control of the maintainers.  This way, would-be contributors could audit their track record of returning the fees to good faith efforts.  Having some friction in the PR or bug bounty process will still risk lowering good faith submissions when the submitters are worried about losing their fee.  But we should expect that to mostly reduce lower quality good faith submissions (which have always been there, just not in large quantities), because they're the ones least confident in having their submissions viewed as good faith.</p>
<p>One real life drag on such a system is that financial transactions normally come with small transaction costs.  So each project would need to decide who eats that cost.  I would guess that large, important projects would probably ask the submitters to eat it, and small projects who are desperate for help might volunteer to eat it.  Between that and the pricing, there's plenty of room to experiment.  Transaction costs are typically very small, and if it solves the problem, then it could easily be worth it.  To be clear, if a solution without transaction costs, like the <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/mitchellh/vouch">vouch</a>, can work, than that is preferred.  I'm most confident my proposed solution would work well for bug bounties, where there are already financial transactions involved, and rewards at stake.</p>
<p>No need to detect AI with more AI.  No worrying about AI slop submitters changing github usernames.  No need to close off projects or end bug bounties.  Just put a small, refundable cost to submitters, and watch the slop submissions drop dramatically.  Even if I'm wrong and it doesn't stop the problem, at least you can fund more maintainers to review the slop!</p>
<h3 id="footnote">Footnote</h3>
<p><sup class="footnote-reference"><a href="#integration">1</a></sup> I mean, &quot;that's it&quot; conceptually.  Building this application and integrating it with social coding platforms would take effort.  But they would be rewarded with the transaction fees.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Don&#x27;t Conform Your Data to the Model. Do the Opposite.</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Sun, 02 Mar 2025 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          
          
          <category><![CDATA[ab testing]]></category>
          <category><![CDATA[bad practices]]></category>
          <category><![CDATA[winsorizing]]></category>
          
          <link>https://dfd.github.io/dont-conform-data-to-the-model/</link>
          <guid>https://dfd.github.io/dont-conform-data-to-the-model/</guid>
          <description><![CDATA[In this post, I'll be responding to an idea I saw on LinkedIn.  See my guidelines on naming names.
The Problem
I recently saw a post on LinkedIn which serves as a nice example of how the null hypothesis mindset leads people to poor statistical decision making.  The author works a…]]></description>
          <content:encoded><![CDATA[<p>In this post, I'll be responding to an idea I saw on LinkedIn.  See my guidelines on <a href="https://dfd.github.io/guidelines-on-naming-names/">naming names</a>.</p>
<h2 id="the-problem">The Problem</h2>
<p>I recently saw <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.linkedin.com/posts/tyler-buffington-82a1a212a_not-all-outlier-handling-approaches-are-created-activity-7285002769231425536-mty1/">a post on LinkedIn</a> which serves as a nice example of how the null hypothesis mindset leads people to poor statistical decision making.  The author works at an AB testing as a service company.  Here is the post:</p>
<p><img src="https://dfd.github.io/dont-conform-data-to-the-model/post.png" alt="png" /></p>
<p>I agree with most of the points listed, but I want to focus on the suggestion to Winsorize the data and the plots at the bottom of the post.</p>
<p>The issue here is that the author wants to be able to compare the effect of a treatment on a continuous variable.  The problem arises presumably because the author wants to conduct a two-sample t-test on the observations against a null hypothesis, but the skewed distribution of the observables will mean the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Student&#x27;s_t-test#Assumptions">assumption of normality on the sampling distribution of the means</a> won't be valid at lower sample sizes, and hence will decrease the power of the test.</p>
<h2 id="the-proposed-solution">The Proposed Solution</h2>
<p><a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Winsorizing">Winsorizing</a> is used to clip the highest values to reduce the skewness in the observed data; so instead of a long tail, the empirical distribution now has a small peak at whatever the threshold was.</p>
<p>If we suppose a frequentist null hypothesis test is what they want, there are problems with using a t-test here:</p>
<ol>
<li>If we don't Winsorize the data, then the normality assumptions on the means do not hold until larger sample sizes (using <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Central_limit_theorem">CLT</a>).  So for sample sizes at which CLT does not apply (and what is that cutoff exactly?), the &quot;frequentist guarantees&quot; don't apply.</li>
<li>If we do Winsorize, then we've changed our data so the frequentist guarantees also don't apply.  How could they?  We're throwing out data so we won't have accurate estimates in the difference of our means.</li>
</ol>
<p>The post frames this as a bias/variance trade-off.  While this is better than insisting on unbiasedness above all else, it seems to ignore the typical reasons given for using the null hypothesis test in the first place: those guarantees.</p>
<p>Even if the method of analysis isn't a null hypothesis test, Winsorizing data still throws out information that might change our conclusion, had we not capped it.  The value at which to cap it will necessarily be arbitrary. If we cap it ahead of time, then we may not know where a good cap is; if we cap it after the fact, how do we know our bias hasn't influenced the choice of cap?</p>
<p>In the <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.linkedin.com/posts/ronnyk_abtest-activity-7254731531049488384-hh6J/">Ron Kohavi post linked to in the screenshot</a>, he discusses using the technique to reduce skewness and increase power.  But increased power to compare what? If the continuous metric is what you care about, it's no longer clear what you're comparing with the new capped metric. He even gives an example of reducing required sample size by 50% through capping, and suggests capping at different levels to compare results.  But how to decide which results to use?  Maybe he explains it in his course that he links to, but it's hard to imagine a principled approach to deciding, and providing multiple options risks cherry-picking a convenient false positive.</p>
<h2 id="the-alternative-solution">The Alternative Solution</h2>
<p>Instead of trying to make our data conform to the assumption of our model (the t-test relies on a model of the null hypothesis), we should do the opposite and find a model that describes the data well.  Looking back at the post in the screenshot, we see an illustrative sample provided by the author.  It looks like it would be well approximated by a log-distribution, and in my experience, this is quite common in revenue or customer lifetime value metrics.  I agree with the author that we should not simply log the observations, but we could model the samples as log-normal distributions.  Here are some benefits of doing so:</p>
<ol>
<li>We don't throw out information.  Assuming our measurements aren't flawed, we should try to retain all available information from these tests.  Driving higher revenue value is valuable to the company.  If we use the same Winsor cutoff on both samples, then whichever sample drove more users above the cutoff will be punished in the comparison. </li>
<li>The log-normal distribution naturally discounts the impact of outliers in the sample.  It does this by estimating its parameters on the log scale.  So we don't need to worry if one sample &quot;got lucky&quot; with a random draw far out in its tail; we won't be using its raw value in a mean calculation.</li>
<li>But this is different than just comparing the logged values.  The mean of a <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Log-normal_distribution">log-normal distribution</a> is a function of its log mean and standard deviation parameters: $\exp(\mu + \frac{\sigma^2}{2})$. So we can take into account how both the mean and spread of the distribution on the log scale impact the metric we car about in its original scale.</li>
<li>If the log-normal assumptions are a good match for the data, then it is a sample-efficient way to estimate the parameters.</li>
</ol>
<p>Using Bayesian methods, we can estimate the parameters and create derived quantities using the equation above and have a posterior distribution over the difference, which we can use to estimate the probability of the treatment having a higher mean revenue or whatever we would like to know.</p>
<p>Of course, this may not work well in all situations; not all businesses have revenue distributions that are well modeled by log-normals.  But perhaps other distributions will work well in those cases.  When we conform our modeling assumptions to the nature of our data, we can make better comparisons about the things we care about in sample efficient ways.  No need to cap.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Blog Guidelines on Naming Names</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Site]]></category>
          
          
          <category><![CDATA[guidelines]]></category>
          
          <link>https://dfd.github.io/guidelines-on-naming-names/</link>
          <guid>https://dfd.github.io/guidelines-on-naming-names/</guid>
          <description><![CDATA[I occasionally read items that inspire ideas for blog posts.  Sometimes they are items I disagree with, and sometimes those items are behind logins.  I also want to avoid strawman arugments, so I'll want to link to examples of what I'm referring to.
In these situations, the guide…]]></description>
          <content:encoded><![CDATA[<p>I occasionally read items that inspire ideas for blog posts.  Sometimes they are items I disagree with, and sometimes those items are behind logins.  I also want to avoid strawman arugments, so I'll want to link to examples of what I'm referring to.</p>
<p>In these situations, the guideline I plan to follow is to not name the author of the post which I am critical of, unless that person is otherwise somewhat famous in their field (e.g. the author of a book).</p>
<p>In any case, when I am critiquing someone's ideas, I never intend to imply anything negative about them as a person, scholar, or employee, unless explicitly stated.  Usually in these cases, the person has simply posted an example of the type of thinking I disagree with.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Can Primary Elections Cause Polarization?  A Toy Model</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Political Science]]></category>
          
          
          <category><![CDATA[voting]]></category>
          <category><![CDATA[toy model]]></category>
          
          <link>https://dfd.github.io/primaries-polarization/</link>
          <guid>https://dfd.github.io/primaries-polarization/</guid>
          <description><![CDATA[Introduction
There has been much speculation that Americans have become more politically polarized in recent years.  But the evidence seems to point to political parties become more polarized rather than the population itself.  In other words, there is more ideoleogical sorting i…]]></description>
          <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>There has been much speculation that Americans have become more politically polarized in recent years.  But the evidence seems to point to political parties become more polarized rather than the population itself.  In other words, there is more ideoleogical sorting in party membership than before.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://statmodeling.stat.columbia.edu/2024/10/15/things-are-getting-so-politically-polarized-we-cant-measure-how-politically-polarized-things-are-getting/">Andrew Gelman recently blogged about this</a> (citing a <a rel="noopener nofollow noreferrer" target="_blank" href="https://madeinamericathebook.wordpress.com/2024/06/20/things-are-getting-so-politically-polarized-we-cant-measure-how-politically-polarized-things-are-getting/">post by Claude Fischer</a>), and I agree with his observation that it seems like people tend to cast votes more against the other side than for their own these days.</p>
<p>Allen Downey <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2022/09/11/polarization-and-partisan-sorting/">looked at survey results a couple of years ago and came to a similar conclusion</a>.  He also points out that &quot;the percentage of Nonpartisans has increased to the point where they are now the plurality.&quot;</p>
<p>In this post, I will present a toy model that extends the median voter theorem to show how primary presidential elections might lead to these results.  To be clear, I will prove nothing here.  I won't even try to fit a model to empirical data.  This is more in the spirit of <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Schelling%27s_model_of_segregation">Schelling's segregation model</a>, which showed how slight in-group preferences could lead to highly segregated housing.  He never claimed this was the only cause or even the primary cause of observed, real-life segregation; merely that it was a plausible mechanism through which housing segregation could occur.  Similarly, I'll just try to give an intuition of how primary elections might lead to political polarization and growing independents under some assumptions.</p>
<h2 id="the-median-voter-theorem-and-hotelling-s-law">The median voter theorem and Hotelling's law</h2>
<p><a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Median_voter_theorem">The median voter theorem</a> supposes that voters have preferences along a one-dimensional spectrum and that voters cast votes for the candidate closest to their position, and then states voters will elect the candidate whose position is closest to the median voter.  I'm also supposing <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Hotelling%27s_law#Political_science">Hotelling's law</a> behavior, whereby politicians will position themselves at the median to win elections. I'm simplifying a bit, but that's a good enough intuition in a &quot;one person, one vote&quot; system.</p>
<figure class="image">
  <img src="Median_voter.png">
  <figcaption>An illustration of the median voter theorem.<br>Attribution:
  <a href="https://commons.wikimedia.org/wiki/File:Median_voter.png">Colin.champion</a>, <a href="https://creativecommons.org/licenses/by-sa/4.0">CC BY-SA 4.0</a>, via Wikimedia Commons
</figcaption>
</figure>
<h2 id="extension-and-repetition">Extension and repetition</h2>
<p>In this post, I will suppose the following:</p>
<ul>
<li>Political positions of the electorate are well described by a Gaussian distribution</li>
<li>The winning candidate of party's primary positions his or herself at the median vote of that party</li>
<li>Voters use presidential nominees as flagpoles of each party's position</li>
<li>Voters estimate the midpoint between flagpoles of each party, and this midpoint become the flagpole for independents</li>
<li>Voters then categorize themselves according to the flagpole that is closest to them</li>
</ul>
<p>In other words, this model represents people choosing to affiliate with whatever is closest to their views: one of two parties or the midpoint between them.</p>
<p>We will develop a toy model for this, and see how it leads to growing polarization between parties and a growing number of independents over time.</p>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#f92672dd;">import </span><span>numpy </span><span style="color:#cc7833;">as </span><span>np
</span><span style="color:#f92672dd;">import </span><span>matplotlib.pyplot </span><span style="color:#cc7833;">as </span><span>plt
</span><span>
</span><span style="color:#95815e;"># generate first voters, who each belong to a party to start
</span><span>np.random.seed(</span><span style="color:#a5c261;">3</span><span>)
</span><span>party1 </span><span style="color:#cc7833;">= </span><span>np.random.normal(</span><span style="color:#a5c261;">0</span><span>, </span><span style="color:#a5c261;">1</span><span>, </span><span style="font-style:italic;color:#fd971f;">size</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">500</span><span>)
</span><span>party2 </span><span style="color:#cc7833;">= </span><span>np.random.normal(</span><span style="color:#a5c261;">0</span><span>, </span><span style="color:#a5c261;">1</span><span>, </span><span style="font-style:italic;color:#fd971f;">size</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">500</span><span>)
</span><span>
</span><span>all_voters </span><span style="color:#cc7833;">= </span><span>np.hstack([party1, party2])
</span><span>
</span><span>party1_flagpole </span><span style="color:#cc7833;">= </span><span>np.median(party1)
</span><span>party2_flagpole </span><span style="color:#cc7833;">= </span><span>np.median(party2)
</span><span>independent_flagpole </span><span style="color:#cc7833;">= </span><span>(party1_flagpole </span><span style="color:#cc7833;">+ </span><span>party2_flagpole)</span><span style="color:#cc7833;">/</span><span style="color:#a5c261;">2
</span><span>
</span><span>party_array </span><span style="color:#cc7833;">= </span><span>np.array([independent_flagpole, party1_flagpole, party2_flagpole])
</span></code></pre>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span>rounds </span><span style="color:#cc7833;">= </span><span style="color:#a5c261;">7
</span><span>results </span><span style="color:#cc7833;">= </span><span>np.empty((rounds, </span><span style="color:#da4939;">len</span><span>(all_voters)))
</span><span>flagpoles </span><span style="color:#cc7833;">= </span><span>np.empty((rounds, </span><span style="color:#a5c261;">3</span><span>))
</span><span style="color:#cc7833;">for </span><span>i </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(rounds):
</span><span>    flagpoles[i, :] </span><span style="color:#cc7833;">= </span><span>party_array
</span><span>    closest_indices </span><span style="color:#cc7833;">= </span><span>np.abs(all_voters[:, np.newaxis] </span><span style="color:#cc7833;">- </span><span>party_array).argmin(</span><span style="font-style:italic;color:#fd971f;">axis</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">1</span><span>)
</span><span>    results[i, :] </span><span style="color:#cc7833;">= </span><span>closest_indices
</span><span>    </span><span style="color:#95815e;"># now group into new parties
</span><span>    party1 </span><span style="color:#cc7833;">= </span><span>all_voters[closest_indices</span><span style="color:#cc7833;">==</span><span style="color:#a5c261;">1</span><span>]
</span><span>    party2 </span><span style="color:#cc7833;">= </span><span>all_voters[closest_indices</span><span style="color:#cc7833;">==</span><span style="color:#a5c261;">2</span><span>]
</span><span>
</span><span>    party1_flagpole </span><span style="color:#cc7833;">= </span><span>np.median(party1)
</span><span>    party2_flagpole </span><span style="color:#cc7833;">= </span><span>np.median(party2)
</span><span>    independent_flagpole </span><span style="color:#cc7833;">= </span><span>(party1_flagpole </span><span style="color:#cc7833;">+ </span><span>party2_flagpole)</span><span style="color:#cc7833;">/</span><span style="color:#a5c261;">2
</span><span>    party_array </span><span style="color:#cc7833;">= </span><span>np.array([independent_flagpole, party1_flagpole, party2_flagpole])
</span><span>    
</span></code></pre>
<p>We can look at party membership over time</p>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span>plt.plot((results</span><span style="color:#cc7833;">==</span><span style="color:#a5c261;">0</span><span>).sum(</span><span style="font-style:italic;color:#fd971f;">axis</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">1</span><span>), </span><span style="font-style:italic;color:#fd971f;">label</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">&#39;Independents&#39;</span><span>)
</span><span>plt.plot((results</span><span style="color:#cc7833;">==</span><span style="color:#a5c261;">1</span><span>).sum(</span><span style="font-style:italic;color:#fd971f;">axis</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">1</span><span>), </span><span style="font-style:italic;color:#fd971f;">label</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">&#39;Party 1&#39;</span><span>)
</span><span>plt.plot((results</span><span style="color:#cc7833;">==</span><span style="color:#a5c261;">2</span><span>).sum(</span><span style="font-style:italic;color:#fd971f;">axis</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">1</span><span>), </span><span style="font-style:italic;color:#fd971f;">label</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">&#39;Party 2&#39;</span><span>)
</span><span>plt.xticks(</span><span style="font-style:italic;color:#fd971f;">ticks</span><span style="color:#cc7833;">=</span><span style="color:#da4939;">range</span><span>(results.shape[</span><span style="color:#a5c261;">0</span><span>]), </span><span style="font-style:italic;color:#fd971f;">labels</span><span style="color:#cc7833;">=</span><span style="color:#da4939;">range</span><span>(</span><span style="color:#a5c261;">1</span><span>, results.shape[</span><span style="color:#a5c261;">0</span><span>] </span><span style="color:#cc7833;">+ </span><span style="color:#a5c261;">1</span><span>))
</span><span>plt.legend()
</span><span>plt.gca().set_ylabel(</span><span style="color:#a5c261;">&#39;Voter Counts&#39;</span><span>)
</span><span>plt.gca().set_xlabel(</span><span style="color:#a5c261;">&#39;Election Number&#39;</span><span>)
</span><span>plt.show();
</span></code></pre>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_9_0.png" alt="png" /></p>
<p>And look at the flagpole positions over time.</p>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span>plt.plot(flagpoles)
</span><span>plt.gca().set_ylabel(</span><span style="color:#a5c261;">&#39;Flagpole position&#39;</span><span>)
</span><span>plt.gca().set_xlabel(</span><span style="color:#a5c261;">&#39;Election Number&#39;</span><span>)
</span><span>plt.xticks(</span><span style="font-style:italic;color:#fd971f;">ticks</span><span style="color:#cc7833;">=</span><span style="color:#da4939;">range</span><span>(results.shape[</span><span style="color:#a5c261;">0</span><span>]), </span><span style="font-style:italic;color:#fd971f;">labels</span><span style="color:#cc7833;">=</span><span style="color:#da4939;">range</span><span>(</span><span style="color:#a5c261;">1</span><span>, results.shape[</span><span style="color:#a5c261;">0</span><span>] </span><span style="color:#cc7833;">+ </span><span style="color:#a5c261;">1</span><span>)) 
</span><span>plt.show();
</span></code></pre>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_11_0.png" alt="png" /></p>
<p>Let's look at how it evolves over time with voters and their relative positions.</p>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span>party_colors </span><span style="color:#cc7833;">= </span><span>[</span><span style="color:#a5c261;">&#39;C0&#39;</span><span>, </span><span style="color:#a5c261;">&#39;C1&#39;</span><span>, </span><span style="color:#a5c261;">&#39;C2&#39;</span><span>]
</span><span>party_names </span><span style="color:#cc7833;">= </span><span>[</span><span style="color:#a5c261;">&#39;Independents&#39;</span><span>, </span><span style="color:#a5c261;">&#39;Party 1&#39;</span><span>, </span><span style="color:#a5c261;">&#39;Party 2&#39;</span><span>]
</span></code></pre>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#cc7833;">for </span><span>round_idx </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(rounds):
</span><span>    plt.figure(</span><span style="font-style:italic;color:#fd971f;">figsize</span><span style="color:#cc7833;">=</span><span>(</span><span style="color:#a5c261;">8</span><span>, </span><span style="color:#a5c261;">2</span><span>))
</span><span>
</span><span>    </span><span style="color:#cc7833;">for </span><span>party </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(</span><span style="color:#a5c261;">3</span><span>):
</span><span>        party_voters </span><span style="color:#cc7833;">= </span><span>all_voters[results[round_idx] </span><span style="color:#cc7833;">== </span><span>party]
</span><span>        plt.scatter(
</span><span>            party_voters,
</span><span>            np.zeros_like(party_voters),
</span><span>            </span><span style="font-style:italic;color:#fd971f;">color</span><span style="color:#cc7833;">=</span><span>party_colors[party],
</span><span>            </span><span style="font-style:italic;color:#fd971f;">marker</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">&#39;|&#39;</span><span>,
</span><span>            </span><span style="font-style:italic;color:#fd971f;">s</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">100</span><span>,
</span><span>            </span><span style="font-style:italic;color:#fd971f;">label</span><span style="color:#cc7833;">=</span><span>party_names[party],
</span><span>            </span><span style="font-style:italic;color:#fd971f;">alpha</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">0.2</span><span>,
</span><span>        )
</span><span>
</span><span>    </span><span style="color:#cc7833;">for </span><span>party </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(</span><span style="color:#a5c261;">3</span><span>):
</span><span>        flagpole_pos </span><span style="color:#cc7833;">= </span><span>flagpoles[round_idx, party]
</span><span>        plt.plot(
</span><span>            [flagpole_pos, flagpole_pos],
</span><span>            [</span><span style="color:#a5c261;">0</span><span>, </span><span style="color:#a5c261;">0.2</span><span>],
</span><span>            </span><span style="font-style:italic;color:#fd971f;">color</span><span style="color:#cc7833;">=</span><span>party_colors[party],
</span><span>            </span><span style="font-style:italic;color:#fd971f;">linestyle</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">&#39;-&#39;</span><span>,
</span><span>            </span><span style="font-style:italic;color:#fd971f;">linewidth</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">2</span><span>,
</span><span>            </span><span style="font-style:italic;color:#fd971f;">label</span><span style="color:#cc7833;">=</span><span style="color:#c1be91;">&quot;_nolegend_&quot;</span><span>,
</span><span>        )
</span><span>
</span><span>    plt.title(</span><span style="font-style:italic;color:#6e9cbe;">f</span><span style="color:#a5c261;">&#39;Round </span><span>{round_idx </span><span style="color:#cc7833;">+ </span><span style="color:#a5c261;">1</span><span>}</span><span style="color:#a5c261;">&#39;</span><span>)
</span><span>    plt.gca().set_xlabel(</span><span style="color:#a5c261;">&#39;Political Position&#39;</span><span>)
</span><span>    plt.yticks([])
</span><span>    plt.legend()
</span><span>
</span><span>    plt.show()
</span><span>
</span></code></pre>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_14_0.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_14_1.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_14_2.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_14_3.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_14_4.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_14_5.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_14_6.png" alt="png" /></p>
<p>Now instead of showing it by political position on the x-axis, let's give each voter equal width on the x-axis to better show how the proportions change.  We'll align the flagpoles with the closest voter.</p>
<pre data-lang="python" style="background-color:#383838;color:#e6e1dc;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#cc7833;">for </span><span>round_idx </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(rounds):
</span><span>    plt.figure(</span><span style="font-style:italic;color:#fd971f;">figsize</span><span style="color:#cc7833;">=</span><span>(</span><span style="color:#a5c261;">8</span><span>, </span><span style="color:#a5c261;">2</span><span>))
</span><span>
</span><span>    </span><span style="color:#95815e;"># voter positions and the corresponding party votes for this round
</span><span>    voters_positions </span><span style="color:#cc7833;">= </span><span>all_voters
</span><span>    parties </span><span style="color:#cc7833;">= </span><span>results[round_idx] 
</span><span>
</span><span>    </span><span style="color:#95815e;"># sort voters and their corresponding party by their position
</span><span>    sorted_indices </span><span style="color:#cc7833;">= </span><span>np.argsort(voters_positions)
</span><span>    sorted_voters </span><span style="color:#cc7833;">= </span><span>voters_positions[sorted_indices]
</span><span>    sorted_parties </span><span style="color:#cc7833;">= </span><span>parties[sorted_indices].astype(</span><span style="font-style:italic;color:#6e9cbe;">int</span><span>)
</span><span>
</span><span>    </span><span style="color:#95815e;"># generate evenly spaced x-values for the voters, from 0 to 1
</span><span>    x_values </span><span style="color:#cc7833;">= </span><span>np.linspace(</span><span style="color:#a5c261;">0</span><span>, </span><span style="color:#a5c261;">1</span><span>, </span><span style="color:#da4939;">len</span><span>(sorted_voters))
</span><span>
</span><span>    
</span><span>    </span><span style="color:#cc7833;">for </span><span>i </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(</span><span style="color:#da4939;">len</span><span>(sorted_voters) </span><span style="color:#cc7833;">- </span><span style="color:#a5c261;">1</span><span>):
</span><span>        plt.plot(
</span><span>            x_values[i:i </span><span style="color:#cc7833;">+ </span><span style="color:#a5c261;">2</span><span>], [</span><span style="color:#a5c261;">0</span><span>, </span><span style="color:#a5c261;">0</span><span>],
</span><span>            </span><span style="font-style:italic;color:#fd971f;">color</span><span style="color:#cc7833;">=</span><span>party_colors[sorted_parties[i]],
</span><span>            </span><span style="font-style:italic;color:#fd971f;">lw</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">2</span><span>, </span><span style="font-style:italic;color:#fd971f;">alpha</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">0.2
</span><span>        )
</span><span>
</span><span>    </span><span style="color:#95815e;"># flagpoles
</span><span>    </span><span style="color:#cc7833;">for </span><span>party </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(</span><span style="color:#a5c261;">3</span><span>):
</span><span>        flagpole_pos </span><span style="color:#cc7833;">= </span><span>flagpoles[round_idx, party]
</span><span>        nearest_voter_idx </span><span style="color:#cc7833;">= </span><span>np.abs(sorted_voters </span><span style="color:#cc7833;">- </span><span>flagpole_pos).argmin()
</span><span>        nearest_voter_x </span><span style="color:#cc7833;">= </span><span>x_values[nearest_voter_idx]
</span><span>
</span><span>        plt.axvline(
</span><span>            </span><span style="font-style:italic;color:#fd971f;">x</span><span style="color:#cc7833;">=</span><span>nearest_voter_x,
</span><span>            </span><span style="font-style:italic;color:#fd971f;">color</span><span style="color:#cc7833;">=</span><span>party_colors[party],
</span><span>            </span><span style="font-style:italic;color:#fd971f;">linestyle</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">&#39;--&#39;</span><span>,
</span><span>            </span><span style="font-style:italic;color:#fd971f;">linewidth</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">2</span><span>,
</span><span>            </span><span style="font-style:italic;color:#fd971f;">label</span><span style="color:#cc7833;">=</span><span style="color:#c1be91;">&quot;_nolegend_&quot; 
</span><span>        )
</span><span>
</span><span>    </span><span style="color:#cc7833;">for </span><span>party </span><span style="color:#cc7833;">in </span><span style="color:#da4939;">range</span><span>(</span><span style="color:#a5c261;">3</span><span>):
</span><span>        plt.plot([], [], </span><span style="font-style:italic;color:#fd971f;">color</span><span style="color:#cc7833;">=</span><span>party_colors[party], </span><span style="font-style:italic;color:#fd971f;">lw</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">2</span><span>, </span><span style="font-style:italic;color:#fd971f;">label</span><span style="color:#cc7833;">=</span><span>party_names[party])
</span><span>
</span><span>    plt.title(</span><span style="font-style:italic;color:#6e9cbe;">f</span><span style="color:#a5c261;">&#39;Round </span><span>{round_idx </span><span style="color:#cc7833;">+ </span><span style="color:#a5c261;">1</span><span>}</span><span style="color:#a5c261;">&#39;</span><span>)
</span><span>    plt.gca().set_xlabel(</span><span style="color:#a5c261;">&#39;Voter Rank (Equal Spacing)&#39;</span><span>)
</span><span>    plt.yticks([])  </span><span style="color:#95815e;"># Remove y-axis
</span><span>
</span><span>    plt.legend(</span><span style="font-style:italic;color:#fd971f;">loc</span><span style="color:#cc7833;">=</span><span style="color:#a5c261;">&#39;upper right&#39;</span><span>)
</span><span>
</span><span>    plt.show()
</span></code></pre>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_16_0.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_16_1.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_16_2.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_16_3.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_16_4.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_16_5.png" alt="png" /></p>
<p><img src="https://dfd.github.io/primaries-polarization/primaries-polarization_files/primaries-polarization_16_6.png" alt="png" /></p>
<p>So that illustrates the intuition behind this toy model, how it could lead to political polarization between parties and growth in independents.  However, one might notice that most of the polarization happened quite quickly toward the beginning (this is in part because I started independents as such a small group).</p>
<p>One could imagine extending this model such that affiliation switching is done with a lag as voters wonder, &quot;has my party really left me?&quot; and verify over several cycles that they are now closer to the center than to their party.  Or perhaps switching could be done probabilistically according to a function of how far the party has drifted from them and over how many cycles.</p>
<h2 id="conclusion">Conclusion</h2>
<p>So, what have we proved?  Nothing.  As mentioned up top, this is just a toy model and there are other plausible causes of party polarization.  But I encourage you to go look at Allen Downey's <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2022/09/11/polarization-and-partisan-sorting/">post on the topic</a>, where we can see that this move toward modern polarization took place shortly after <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/United_States_presidential_primary#National_binding_primary_era_(1972%E2%80%93present)">national binding primary era started in 1972</a>.  In the absence of primaries, <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Median_voter_theorem#Hotelling-Downs_median_voter_model">Hotelling's law suggests</a> political parties would be more focused on nominating candidates positioned toward the median of the general electorate (or at least of the swing states), rather than the median of their own party.  Hopefully this post gives some intuition as to how such polarization might arise under a few simplifying assumptions about party primaries and affiliation.</p>
]]></content:encoded>
      </item>
      <item>
          <title>A Solution to Gerrymandering: Virtual Districts</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Political Science]]></category>
          <category><![CDATA[Game Theory]]></category>
          
          
          <category><![CDATA[voting]]></category>
          <category><![CDATA[government]]></category>
          
          <link>https://dfd.github.io/virtual-districts/</link>
          <guid>https://dfd.github.io/virtual-districts/</guid>
          <description><![CDATA[Introduction
First, let’s define it: gerrymandering is the manipulation of arbitrary geographic boundaries of voting districts by a dominant political party to increase its representation in government.  This happens all over the world, but I’ll focus my discussion on the United …]]></description>
          <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>First, let’s define it: gerrymandering is the manipulation of arbitrary geographic boundaries of voting districts by a dominant political party to increase its representation in government.  This happens all over the world, but I’ll focus my discussion on the United States, as I’m most familiar with it.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Gerrymandering_in_the_United_States#Examples_of_gerrymandered_U.S._districts">Wikipedia provides some nice illustrative examples</a> of the issue, where we can see spiraling borders on Congressional districts to dilute the voters of a state’s minority party to help the majority party win more seats in Congress.  It’s a nice, visual example of politics at its worst: suppressing the voice of opponents.  The question is, what can be done about it?</p>
<figure class="image">
<img src="Maryland_US_Congressional_District_3.tif">
  <!--<figcaption>Examples of gerrymandering Congressional districts in the US.</figcaption>
</figure>
<figure class="image">-->
<img src="Illinois_District_4_2004.png">
  <!--<figcaption>Examples of gerrymandering Congressional districts in the US.</figcaption>
</figure>
<figure class="image">-->
  <img src="North_Carolina_12th_Congressional_District.gif">
  <figcaption>Examples of gerrymandering Congressional districts in the US.</figcaption>
</figure>
<p>Even restrictions on the type of boundaries will not satisfactorily solve the problem.  Allowing fewer degrees of freedom helps, but even normal looking shapes <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.quantamagazine.org/how-math-has-changed-the-shape-of-gerrymandering-20230601/">still may be used to create heavily disproportional representation with modern computers</a>.  You could even end up with skewed representation by chance.  For instance, imagine making a rule that Congressional districts will be defined by only longitudinal lines and state borders, such that each district holds $1/k$ proportion of the state voting age population, where $k$ is the number of Congressional seats.  It could just so happen that due to the distribution of residents that this leads to disproportional representation of the state’s voters, even with no ill intent.</p>
<h2 id="the-goal">The goal</h2>
<p>Let’s define a goal and then devise a system that achieves that goal.  We’re going to make some simplifying assumptions to make this tractable.  Let’s suppose we are in a two party political system, and all voters are in those two parties.  Further, we are in a state with $k$ Congressional seats and $n$ voters with $m$ share in the minority party and $(1 - m)$ in the majority party.  Another constraint is that each district must represent roughly the same number of eligible voters (for simplicity, we’ll assume $k$ is a factor of $n$ and that each district will have $n/k$ voters).</p>
<p>We want to devise a system in which no fewer than $\left\lfloor(k \times m) \right\rfloor$ Congressional seats are allocated to the minority party, assuming all voters vote along party lines.</p>
<h2 id="virtual-districts">Virtual districts</h2>
<p>A way out of this is to get rid of the arbitrary geographic borders and let the people self-organize.  So, do we just let voters sort themselves into $k$ virtual districts and call it a day?</p>
<p>Well, no.  Suppose there are 1,000 voters, 400 of which are in the minority party, and 10 virtual districts.  It will require at least 51 out of 100 votes in a district to win a seat.  A well organized majority party could immediately place 60 voters in each virtual district and win all of them!  Zero is far fewer than our objective of at least 4 seats for the minority party in this case.  We need to solve an adversarial problem and assume that any system we propose will be exploited if possible.</p>
<h2 id="the-solution">The solution</h2>
<p>Here’s how we will avoid this sort of issue:</p>
<p>Have two rounds of self-organization.  Start with $2 \times k$ virtual districts, each up to a limit of $n/k$ voters and which voters can voluntarily join.  After a deadline, eliminate the least populous $k$ virtual districts from the first round, and let each of the displaced voters join one of the remaining $k$ districts with available capacity in the second round.  Does this achieve our goal?</p>
<p>First, let’s revisit the illustrative example from above.  Does the majority party have the ability to guarantee the minority party has less than 4 seats in that example?  No.  If they place 60 voters in each of 10 virtual districts in the first round, the minority party members have the freedom to move to other open districts (there would be 10 empty ones), and the majority party doesn’t have enough members to place over 50 members in each of the $2 \times k$ virtual districts, since that would require more than $2 \times 10 \times 50 = 1,000$ members (in fact, it would require at least 1,010 to avoid ties).  In other words, it would require more members than total voters to guarantee a majority in each of the first round virtual districts.  Meanwhile, the minority party could try to stack itself into 4 virtual districts with 100 each and guarantee victory in them.</p>
<p>But would the majority party let them do that?  Let’s make some more assumptions before proceeding. First, let’s assume that voter virtual district choices can be centrally coordinated by the party.  Also, we’ll give the option to the majority party move first in each round, and each party can respond instantaneously to the other party’s moves.  In other words, we won’t allow a party to run out of time before the round 1 or round 2 deadlines to execute their strategy.  Finally, we’ll suppose that each voter’s party affiliation is visible, such that both parties know the party affiliation counts within each district.</p>
<p>Let’s examine some possible strategies.  Suppose the majority party spreads themselves evenly across the districts.  This won’t help them: they would have 30 members in each of the 20 districts, while the minority party would stack up with 70 each in 5 districts and 50 in the 6th.  All of those districts would survive the first round (10 of the districts with only 30 voters from the majority party would be removed and need to reallocate), with the minority party able to win 5 seats and a tie for the 6th, earning overrepresentation.</p>
<p>Let’s start again: suppose the majority party spreads themselves evenly with 30 in each of the 20 districts, and the minority party places themselves with 51 each in 7 districts and the remaining 43 in an 8th district.  Could the minority party win 7 districts this way if the majority party is allowed to respond?  No, because the majority party would see the minority party’s move and shift its voters out of those 7 districts and into 10 open districts with 60 members each, causing all the minority party districts to be cut after the first round, unless they reallocate further.  How could they reallocate to avoid this?  They could try reallocating to 6 districts with 61 members in each (and some members who would lose the 7th), but they know the majority party will react again.  Taken to its logical conclusion, the minority party would simply stuff 4 districts with 100 voters each to guarantee securing 4 virtual districts in the first round, while the majority will take the other 6, which is perfectly in line with their proportion.  The excess districts and mobility guarantee an inability to take more districts than a party’s proportion (to the precision of a rounded partial district).</p>
<h2 id="endgame">Endgame</h2>
<p>So what would happen?  A premise of game theory is that participants can reason through future moves and opponent responses.  In this case, having double the virtual districts in round one and facing reallocation of the lowest half of districts by population makes it impossible to suppress the other’s representation: the other side can always find enough space to take majority in at least $\left\lfloor(m \times k) \right\rfloor$ districts, so each side must be focused on not <em>losing</em> their fair share due to the culling of least populated districts.  The only way to guarantee at least $\left\lfloor (m \times k) \right \rfloor$ for the minority and $\left\lfloor((1 - m) \times k) \right\rfloor$ for the majority is for both sides to fill up as many districts as they can fully.</p>
<p>We’ve created a 2-player, fixed-sum deterministic game of perfect information.  This <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/watch?v=EqcSEKRtklE&amp;list=PLPhC147aCdDHQyAxcFYR1HSDtYlL0ARSI&amp;index=9">leads us to minimax strategies</a>, and the first round culling mechanism prevents one party from locking in more districts than their fair share.</p>
<p>In some cases when the party membership does not evenly fill an integer number of districts, the minority party can achieve $\left\lceil (k \times m) \right\rceil$, but this depends on the specific numbers of the situation.  For instance, with 10 districts, 99 voters in the minority party and 901 in the majority party, the minority would place 99 in a district and the majority would place 99 voters in 9 districts and only have 10 left over for a 10th, which is not enough to cull the minority district after round 1.  One district is higher than $\left\lfloor(m \times k) \right\rfloor = 0$.   On the other hand, with 10 districts, 949 voters in the majority and 51 in the minority, the minority will put their 51 voters in a single district and the majority party can place at least 94 voters in 10 districts in round 1 to force a culling of the minority district.  In the general case, if there would be a split district, the majority will win the extra district if</p>
<p>$$
(1-m)n / \left\lceil (1-m)k \right\rceil &gt; (1-m)n / \left\lceil (m)k \right\rceil
$$</p>
<p>In other words, if the majority party has more voters per district while extending itself to capture the remaining district than the minority party does while trying to extend itself for the remaining district, it can avoid getting culled and win the district.  Vice versa for the minority party.  Any attempt at extending to grab extra districts by a party will result in a countermove to cull districts of the greedy party and they will have <em>less</em> than their fair share as a result.</p>
<p>It should also work for any number of parties.  The key is having double the districts in round one such that, no matter a party’s majority or even a <em>coalition’s majority</em>, they can’t impose their will on any party with a sufficient number of voters.  If people just want to vote along party lines, they can always achieve at least $\left\lfloor (k \times p_i) \right\rfloor$ where $p_i$ is their proportion of the state’s voters.</p>
<h2 id="realistic-assumptions">Realistic assumptions?</h2>
<p>For the endgame to work out as desired, we had to make an assumption about perfect information to get minimax behavior.  This means that both parties knew what state they were in; in other words, they knew the number of voters from each party in each virtual district.  Today, we know they must have pretty good information about party affiliation, because they use it for effective gerrymandering.  The other assumption to make this work is that the party was centrally coordinating the voter movements of its members.  That also seems like a fine approximation; either the voters could have given the party permission to sign up for virtual districts, or they are given instructions centrally by the parties.  That’s not much of a stretch, considering how much the parties invest in campaign operations already.</p>
<p>Perhaps the biggest disconnect from real life is the assumption of voter participation.  Not all eligible voters will participate in the process, so we would need to randomly assign voters who do not participate at the end, and this would also weaken a party’s ability to obtain its share of districts if its members are less active than the other’s.</p>
<p>We could also expect parties to start engaging in deception with respect to party affiliation to violate the &quot;perfect information&quot; assumption.</p>
<h2 id="even-better-in-practice">Even better in practice?</h2>
<p>Above, we have assumed that voters just want to vote along party lines.  In practice it might lead to even richer coalitions and more accurate, issue-based representation.  One could imagine the moderate members of two parties forming a coalition to advance some shared goal in a virtual district.  For instance, imagine members of both parties join a district and agree that some small number of issues are their top concerns.  They announce that they will be voting for whichever candidate can best deliver on their top three concerns.  Eligible candidates who most closely line up with those goals would enter that race, and ones that do not would find better fits for them.</p>
<p>If issue-based, cross-party coalitions form, then we may not be so worried about party representation.  But the $2 \times k$ districts still serve a useful purpose in round 1, in that it gives an opportunity for grassroots coalitions to form and to see which issues have enough support to warrant representation.  If a cause can’t gather enough voters, that’s a signal to them that their issues aren’t as important to others, and they can seek to join some other coalition with enough support that suits them.</p>
<h2 id="continuity-between-elections">Continuity between elections</h2>
<p>In repeated elections, it would make sense to start voters in their virtual districts from the previous election.  If a coalition is happy with their representative, they can have continuity.  But we would add another $k$ empty districts to start round 1 again.  So if members of some virtual districts are unhappy, they can disband and disburse to new virtual districts.  Plus, inevitably, a state will have residents who left or joined the state since the last election, so spots in existing districts will open up and new residents will have to be placed.</p>
<h2 id="some-extreme-viewpoints">Some extreme viewpoints?</h2>
<p>One consequence of virtual districts would be that larger states with more Congressional seats could lead to more extreme viewpoints.
For instance, California has 52 seats, New York has 45, and Texas has 38.
This means that it would take less than 3% of a state’s population to form a virtual district, and it’s more likely this would lead to what others would perceive as extreme viewpoints winning Congressional seats.  However, these extreme viewpoints would still be marginalized in the context of a large Congress.  So those people may be accurately represented, but they will be unlikely to move the needle on policy.</p>
<h2 id="missing-the-original-purpose">Missing the original purpose?</h2>
<p>Would we be missing the point of Congressional districts if we removed the locality of them?  An original intent of Congressional districts was to ensure that the people would have representatives familiar with their local concerns.  However, as can be seen by the extreme gerrymandering examples, some Congressional districts stretch the definition of local communities already.  Plus, not all people in a locality necessarily agree on federal policy anyway, and those who vote for the losing candidate probably don’t feel well represented.  And of course, nothing would stop a local community from trying to form a virtual district on their own anyway; the difference is that they would be drawing the boundary themselves, and it would be opt-in for voters.</p>
<p>One drawback I can think of is when regional representation is more important than issue representation, like after natural disasters or accidents that the federal government can address.  In such cases, virtual districts may dilute regional representation even more than gerrymandering.</p>
<h2 id="realistic-solution">Realistic solution?</h2>
<p>Might this happen in real life?  Probably not, because the party in power of a state’s districts will not want to give up gerrymandering power, and it seems unlikely that the beneficiaries of gerrymandering in Congress will pass a federal law to ban it.  Also, there may be questions of whether such a scheme is too complex for some voters to navigate or understand.  But it's interesting to think of solutions that could work better in theory.  And if it had enough support behind it, who knows what might happen.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Choosing a Linux Distribution</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Wed, 25 Sep 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Linux]]></category>
          
          
          <category><![CDATA[opinion]]></category>
          <category><![CDATA[deep dive]]></category>
          
          <link>https://dfd.github.io/linux-distro/</link>
          <guid>https://dfd.github.io/linux-distro/</guid>
          <description><![CDATA[
  
  I'm only half kidding.

Introduction
Recently my laptop died, and after 12 years of using Ubuntu/Kubuntu, I decided to consider other Linux distributions.  What follows is a long, meandering post sharing recent trends in the Linux desktop, their trade-offs, as well as some …]]></description>
          <content:encoded><![CDATA[<figure class="image">
  <img src="./linux_decision_tree.png" alt="A decision tree Choosing a Linux Distro">
  <figcaption>I'm only half kidding.</figcaption>
</figure>
<h2 id="introduction">Introduction</h2>
<p>Recently my laptop died, and after 12 years of using <a rel="noopener nofollow noreferrer" target="_blank" href="https://ubuntu.com/desktop">Ubuntu</a>/<a rel="noopener nofollow noreferrer" target="_blank" href="https://kubuntu.org/">Kubuntu</a>, I decided to consider other Linux distributions.  What follows is a long, meandering post sharing recent trends in the Linux desktop, their trade-offs, as well as some recommendations.  It will be meandering because I survey several distributions, and I have two disparate audiences in mind:</p>
<ol>
<li>Data scientists who are new to Linux</li>
<li>People like me who have used Ubuntu-based distributions for awhile and are looking for a change</li>
</ol>
<p>Some of this post will be too much information for those in (1) and some will be too basic for those who fall into (2), so I suppose no one will really enjoy the whole thing.  And if you don’t fall into either category, you might consider finding a better use of your time, like standing in line at the DMV.</p>
<p>A caveat is that while I’ve been daily driving Linux for over twelve years, no one will confuse me for a kernel developer or professional sysadmin.  Heck, I don’t promise everything I write here is even true.  Some of it will be just <em>my impressions</em>, including possible misconceptions, based on limited time to deep dive.  Plus, things can change fast.  So you’ll want to check any claims that are important to your own decision.</p>
<h2 id="motivation-to-use-linux">Motivation to use Linux</h2>
<p>One piece of advice I give to early career data scientists is to use Linux on their personal computers, if they can.  While I wouldn’t recommend it to someone still ramping up on fundamental skills for fear that they would be distracted by wrestling with a new operating system, I think it helps develop technical breadth for the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/T-shaped_skills">T-shaped skills</a> a data scientist is expected to have.</p>
<p>Specifically, the benefits of using Linux for data scientists include:</p>
<ul>
<li>Getting comfortable at the Linux command line</li>
<li>Having a good development environment</li>
<li>Gain a better understanding of Linux, which is almost certainly your production environment</li>
</ul>
<p>Now, if you just use your personal laptop to check email and watch youtube, using Linux may not build many skills.  But mucking around on the command line, tinkering with my system, and troubleshooting issues on my personal computers over the years has helped me to work more independently as a data scientist, communicate better with engineers, and even take on some work that might normally require engineers.</p>
<p>For instance, I recently resolved a GPU driver/software package mismatch in AWS with only cryptic error messages to guide me, whereas many in data science may have kicked it back to the infra team and delayed their progress.  In another example, I was at a company where the IT team had a Windows background, but had some on-prem Linux servers before we started moving to the cloud.  Before I hired a software engineering team that stood up our production JupyterLab environment in AWS (before Sagemaker existed), I personally set up a Jupyter environment for my team on an on-prem Linux server to hold us over until we had the longer term solution.</p>
<p>I’m not suggesting using a Linux desktop is the only way to learn such skills, but I think a regular user will internalize Linux concepts better than someone who tries to learn them as needed.  Besides, I genuinely enjoy using the Linux desktop more than the alternatives.  While the setup and maintenance can take longer, I have more control of the experience and am convinced it enables me to get more done.</p>
<h2 id="my-history-with-linux">My history with Linux</h2>
<p>I first installed Red Hat Linux, the precursor to Fedora, as a dual boot with Windows on a custom PC build.  At the time, Linux was not my daily driver, but I took away that it was a good development environment and there was a surprising amount of good free software available.</p>
<p>In 2012, I installed Ubuntu as a dual boot option on a new Windows laptop and immediately started using it as my daily driver, until I switched to Kubuntu in 2017.  Recently, my 4+ year old laptop started breaking down in March of 2024, and I got a new desktop and later also a laptop to replace it.  For some reason, the Kubuntu installer failed for me (possibly user error), and I went down the rabbit hole of exploring Linux options, and hence we have this post.</p>
<p>But first let’s discuss some popular alternatives.</p>
<h2 id="why-not-windows">Why not Windows?</h2>
<p>First, is Windows’ <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.digitaltrends.com/computing/best-versions-windows-of-all-time/">history of sporadically terrible UIs</a>. Recently, they've been found to have <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/watch?v=IT4vDfA_4NI">copious amounts of telemetry</a>.  Just this year, they almost pushed <a rel="noopener nofollow noreferrer" target="_blank" href="https://arstechnica.com/gadgets/2024/08/microsoft-will-try-the-data-scraping-windows-recall-feature-again-in-october/">frequent, unencrypted, automated screenshots of users’ desktops</a> to them, which would be on by default.  They have <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.reddit.com/r/technology/comments/1cbujm9/start_menu_ads_are_officially_here_with_the/">ads in the Start menu</a>.  And <a rel="noopener nofollow noreferrer" target="_blank" href="https://answers.microsoft.com/en-us/windows/forum/all/how-do-i-get-windows-11-to-stop-with-msn-and-its/1ef07d91-c513-4a55-ac88-8cacb8d3131a">MSN ads</a>.  And the Edge browser <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.theregister.com/2023/02/23/microsoft_edge_banner_chrome/">desperately begs you to use it</a>.</p>
<p>In short, Windows appears to have become an ineffective advertising platform cleverly disguised as a bloated operating system.</p>
<p>But what about the <a rel="noopener nofollow noreferrer" target="_blank" href="https://learn.microsoft.com/en-us/windows/wsl/install">WSL</a>? That can be useful for involuntary Windows users, but I've heard it has a low performance ceiling (relative to Linux on bare metal) and requires extra effort to pass things (like Jupyter notebooks) through from WSL to Windows.</p>
<p>Gaming may be a reason for some to stay on Windows.  Linux gaming performance is now much improved and roughly on par with (and in many cases outperforming) Windows according to recent benchmarks, but Windows still offers more choice, especially for games that require <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.gamingonlinux.com/2024/09/grand-theft-auto-v-gets-battleye-anti-cheat-breaks-online-play-on-steam-deck-linux/">anti-cheat on Windows</a>.</p>
<h2 id="why-not-mac">Why not Mac?</h2>
<p>I have been using a Mac for work for the past three years.  It’s OK, but I don’t think I’d want it for personal use.  For one thing, there’s no way to leverage CUDA locally on a Mac.  Granted, you can use <a rel="noopener nofollow noreferrer" target="_blank" href="https://colab.google/">colab</a> for free, but I don’t like the colab workflow (no ssh anymore, and limited control over dependencies), and they give you slow GPUs.  I recently ran an example deep learning model on Colab Pro and locally on an RTX 4070, and my training time was nearly 4X faster locally (71 vs 18 minutes).  Plus there are no timeouts or limits on usage locally.  I don't mean to criticize Colab, which has a very generous free tier, and not everyone will want to shell out for a new Nvidia card (which will depreciate in relative performance over time), but I think there are some real advantages to experimenting locally.</p>
<p>Macs are based on BSD, so it’s also a Unix, but I find them just different enough from Linux to be confusing.  I recently saw that <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.techradar.com/pro/the-mythical-security-status-of-macos-is-no-more-report-finds-apple-devices-fare-the-worst-when-it-comes-to-full-takeover-risks">MacOS may not be particularly secure</a>.  Plus Docker Desktop needs to <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.docker.com/blog/the-magic-behind-the-scenes-of-docker-desktop/">run a Linux VM</a> just to start a container.</p>
<p>The defaults are clearly aimed at non-technical users (yes, I want to change the extension of that text file…).  The primary purpose of the multitouch gestures seems to be to accidentally “show desktop” several times per week, and it requires a third party app to have reasonable keyboard-driven task switching.   The defaults can be changed, but they give you a sense of who this nanny operating system is primarily targeting.</p>
<p>The advantage of a Mac is that the vertical integration means <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/watch?v=qmPq00jelpc">it just works</a>, but it also has a large price markup relative to its performance and throws you into a <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.tomsguide.com/news/ios-15-and-macos-monterey-apples-walled-garden-just-got-even-higher">walled garden</a> (although it may <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.theverge.com/24141929/apple-iphone-imessage-antitrust-dma-lock-in">be weakening</a>).  Enough developers prefer it that I suppose it must provide a good enough dev environment, but I’ve never been tempted to get one for myself.</p>
<h2 id="bork-insurance-and-prevention">Bork insurance and prevention</h2>
<p>Historically, Linux users were famous for rendering their own systems unusable through tinkering. I’ve personally had to rescue my laptops from the unbootable brink a few times, and once was forced to do a fresh install because of self-inflicted problems.  However, some innovations have been gaining traction that help mitigate that risk.</p>
<p>The first are <a rel="noopener nofollow noreferrer" target="_blank" href="https://btrfs.readthedocs.io/en/latest/Subvolumes.html">btrfs snapshots</a> that are bootable from grub.  This seems to be easiest to set up with <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/Snapper">snapper</a>, but can also be done with <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/Timeshift">timeshift</a>.  While <a rel="noopener nofollow noreferrer" target="_blank" href="https://arstechnica.com/gadgets/2021/09/examining-btrfs-linuxs-perpetually-half-finished-filesystem/">btrfs has some issues</a>, my impression is that it works well for a typical desktop's root filesystem.</p>
<p>Another innovation is <a rel="noopener nofollow noreferrer" target="_blank" href="https://kairos.io/blog/2023/03/22/understanding-immutable-linux-os-benefits-architecture-and-challenges/">atomic/immutable</a> distributions.  These are read-only root filesystems that prevent the users from making destablizing changes and provide some security benefits.  Then system updates are applied, and if the new system doesn’t work, the user can rollback to the old one.   This is how ChromeOS works, but there are several Linux distros that use this now, like the <a rel="noopener nofollow noreferrer" target="_blank" href="https://universal-blue.org/">Universal Blue</a> / <a rel="noopener nofollow noreferrer" target="_blank" href="https://bazzite.gg/">Bazzite</a>/<a rel="noopener nofollow noreferrer" target="_blank" href="https://projectbluefin.io/">Bluefin</a> spins of Fedora or <a rel="noopener nofollow noreferrer" target="_blank" href="https://nixos.org/">NixOS</a>.  I’ve heard that these distros require more reboots than traditional systems to use newly installed software, although less so with NixOS.</p>
<p><a rel="noopener nofollow noreferrer" target="_blank" href="https://www.flatpak.org/">Flatpaks</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://snapcraft.io/">Snaps</a>, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://appimage.org/">AppImages</a> are ways to package software that have gained traction in recent years.  By bundling codecs and dependencies with the apps, they help users avoid dependency conflicts between applications known as <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Dependency_hell">dependency hell</a>.  I recently tried some Flatpak browsers, and I experienced graphics glitches, although perhaps that was specific to my system (using Nvidia with X11).  Snaps, created by Canonical (the maker of Ubuntu), have stirred controversy for a variety of reasons.  Snaps require <a rel="noopener nofollow noreferrer" target="_blank" href="https://systemd.io/">systemd</a>, which is not present in all Linux distributions, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://forum.snapcraft.io/t/snapd-still-requires-out-of-tree-apparmor-patches-for-strict-confinement/19632/20">out-of-tree kernel patches are needed to run them securely</a>.  They also used to launch painfully slow, although that's improving (and all these formats launch slowly compared to native applications).  Flatpaks and Snaps are containerized, while AppImages are not.  Additionally, Flatpaks and Snaps are typically tracked by a package manager to stay up-to-date, whereas AppImages are installed as one-off downloads that the user must update themselves.</p>
<p>Finally, two other popular aids are <a rel="noopener nofollow noreferrer" target="_blank" href="https://distrobox.it/">Distrobox</a>, which lets you run a tightly integrated Linux container from various distributions on a host, and the cross-platform <a rel="noopener nofollow noreferrer" target="_blank" href="https://nixos.org/guides/how-nix-works/">nix package manager</a>, which helps to build and manage isolated dependencies.</p>
<h2 id="in-praise-of-unofficial-spins">In praise of unofficial spins</h2>
<p>At a high level, there are three types of Linux distributions:</p>
<ol>
<li><strong>Independent:</strong> These are not derived from other distributions. There are a relatively small number of them, such as <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.debian.org/">Debian</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://archlinux.org/">Arch</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://fedoraproject.org/">Fedora</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.opensuse.org/">openSUSE</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.gentoo.org/">Gentoo</a>, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://voidlinux.org/">Void</a>.</li>
<li><strong>Derivatives:</strong> These extend a parent distro and add to or modify it. For instance, <a rel="noopener nofollow noreferrer" target="_blank" href="https://ubuntu.com">Ubuntu</a> is a derivative of Debian and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.linuxmint.com/">Linux Mint</a> is a derivative of Ubuntu.</li>
<li><strong>Spins (or flavors):</strong> These are variants of a distribution.  For instance, <a rel="noopener nofollow noreferrer" target="_blank" href="https://kubuntu.org/">Kubuntu</a> is an official flavor of Ubuntu that uses KDE Plasma as its desktop environment. <a rel="noopener nofollow noreferrer" target="_blank" href="https://fedoraproject.org/spins/xfce">Fedora Xfce</a> is an official spin of Fedora (but not the “flagship” distribution).</li>
</ol>
<p><a rel="noopener nofollow noreferrer" target="_blank" href="https://geckolinux.github.io/">GeckoLinux</a> is an <em>unofficial spin</em> of Tumbleweed that provides an easy installation and configuration of openSUSE Tumbleweed.  Once it's done, you are running Tumbleweed with no dependencies left on GeckoLinux.  But the production of GeckoLinux itself is unaffiliated with openSUSE, which helps make it easier to provide proprietary drivers and codecs out of the box (it also makes some strange choices, like setting up the Skype repo, but that's easy to undo).  <a rel="noopener nofollow noreferrer" target="_blank" href="https://spirallinux.github.io/">SpiralLinux</a> is another unofficial spin from the maintainer of GeckoLinux that provides an easy installation for Debian plus drivers and btrfs and snapper, like openSUSE, as well as a backported testing kernel, for better support on recent hardware.  Unofficial spins are great because they can take care of the painpoints of some distributions while removing the risk associated with using a smaller project.</p>
<h2 id="trade-offs-abound">Trade-offs abound</h2>
<p>One of the best aspects of the Linux desktop is the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/List_of_Linux_distributions">diversity of options</a>.  While the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/List_of_Linux_distributions#/media/File:Linux_Distribution_Timeline.svg">sheer number can be overwhelming</a>, each distro comes with trade-offs, and knowing what you want can help narrow it down quite a bit.  So I’ll cover some of the common trade-offs I’ve noticed.  Some of the distros mentioned will have various options available, so if I attach a particular trait to a distro, I’m referring to what I view as the most common out-of-the-box configuration.</p>
<h3 id="minimal-vs-batteries-included-vs-fully-loaded">Minimal vs batteries included vs fully loaded</h3>
<p>Minimal distributions, such as <a rel="noopener nofollow noreferrer" target="_blank" href="https://archlinux.org/">Arch</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://voidlinux.org/">Void</a>, treat the OS like Lego blocks for the user to build with.  As a result, these distros tend to attract more experienced users, who like to customize and understand all the components of their system.  There’s also a security benefit of fewer installed packages in that they provide fewer opportunities for vulnerabilities or malicious code to enter your system (assuming the user also installs some security related packages that do not come by default).  While these distros target expert users, beginners who are willing to invest time and learn can jump in.  Their communities <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/Arch_terminology#RTFM">tend to do less handholding</a>, however.</p>
<p>Because the users choose the components of their system, a job normally done by maintainers of “batteries included” distros, they need to stay on top of the evolving Linux ecosystem.  Often these distro installers come with fewer desktop environment options, and nothing distinguishes the installer for a <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Headless_computer">headless server</a> vs a desktop.</p>
<p>Alternatively, there are what might be called “batteries included” and “fully loaded” distros.  Batteries included distro maintainers make decisions about your operating system so you don’t have to worry about keeping up with all the details (like transitions from <a rel="noopener nofollow noreferrer" target="_blank" href="https://bbs.archlinux.org/viewtopic.php?id=273969">PulseAudio to Pipewire</a>).  The installation comes with a choice of desktop environments so your first boot doesn’t just land you in a terminal.  Think of openSUSE or Debian in this category (you can install as headless, but they give options for installing desktop environments during installation).</p>
<p>“Fully loaded” distros are “batteries included” plus a suite of applications, either for a general use case, like Linux Mint, or a specific one, like Nobara for gaming.  The line between “batteries included” and “fully loaded” is a gray one (especially since I just made up those definitions), and some installers come with options for more or less applications.  It can be helpful for new users though, so they don’t have to research Linux calculator options, for instance.  Experienced users may call these “bloated” since they may come with software they won’t use.</p>
<h3 id="rolling-vs-point-release">Rolling vs point release</h3>
<p>Distros can either follow a point release model with periodic major updates or offer rolling updates. Rolling releases gradually update software over time, meaning users face smaller risks more frequently with each update. Point release distros (also called 'fixed release') prioritize stability within each major version, but when it's time to upgrade, users must make larger jumps, which can carry more risk—either during the upgrade process or if the new version turns out to be buggy.  For instance, the rule of thumb I’ve seen with Fedora is to wait a month after a new major release before upgrading.  And Ubuntu is still having issues with <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.omgubuntu.co.uk/2024/09/canonical-halts-ubuntu-24-04-lts-upgrades-again">upgrades to 24.04, about 5 months after its initial release</a>.  You may also see the term “semi-rolling release,” which refers to distros that keep some components stable while providing rolling updates to others, but I’ll ignore that distinction here.  Arch, Gentoo, Tumbleweed, and Void are examples of rolling releases, while Debian, Ubuntu, and Fedora follow point releases.</p>
<h3 id="up-to-date-vs-stable">Up-to-date vs stable</h3>
<p>Distros can provide software that is more or less up-to-date. Keeping up with the latest packages gives the user access to newer features and improvements, along with more opportunities for customization and tinkering. However, these users may encounter more bugs and some <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/XZ_Utils_backdoor">security risks</a> associated with newer, less-tested code. Arch and Gentoo are examples of distros that integrate upstream updates quickly into their official repos. </p>
<p>&quot;Stable&quot; distros emphasize testing and less frequent updates. If attached to point releases, then they will typically limit updates to security patches and bug fixes for a given release.  While newer packages are often found in rolling releases, Void is an example of a rolling release that delays and curates updates to prioritize a more reliable and stable user experience.</p>
<p>Debian is the poster child for stable distros, and its derivatives, including Ubuntu and its own derivatives, follow suit.  Another benefit is doing blind updates with reasonable confidence.  That is, while rolling release distros that stay on the bleeding edge require the user to be aware of important updates about their distros, someone using a point release of Debian or Ubuntu can just trigger updates, usually without worrying about the consequences if they've stuck to the official repos.</p>
<p>The downside is using outdated software.  It’s not uncommon for Debian users to be two years behind the current version of software packages, and this can sometimes cause problems for newer hardware.  For instance, in late 2019, I installed Kubuntu on a new laptop and had to tether to my cell phone in order to download the driver needed for the wifi card and install it myself.  And software developers <a rel="noopener nofollow noreferrer" target="_blank" href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=819703#10">don’t always love fixing new bugs found on old releases</a>.</p>
<p>Debian and the LTS versions of Ubuntu (and its flavors) are only released every two years, and the software used in Debian is only included after thorough testing, so isn’t even new upon release.  While I ran LTS versions of Ubuntu/Kubuntu, I was sometimes unable to install newer software I wanted because I couldn't match the required dependencies.</p>
<h3 id="software-availability">Software availability</h3>
<p>Debian, Arch, Fedora, and NixOS seem to have the largest software selection in their official repos, although I’ve been pleasantly surprised to see how much Void and openSUSE have as well (although some notable exceptions for Void).  Unofficial repos, like the <a rel="noopener nofollow noreferrer" target="_blank" href="https://aur.archlinux.org/">AUR</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://copr.fedorainfracloud.org/">COPR</a>, or <a rel="noopener nofollow noreferrer" target="_blank" href="https://build.opensuse.org/">OBS</a> can provide supplementary packages as well.  Smaller selections may lead the user to install more from source code and manage those updates themselves.</p>
<p>[Edit: 2024-12-08] <a rel="noopener nofollow noreferrer" target="_blank" href="https://getsol.us/">Solus</a> is a good example of how the size of a project can effect software availability.  In a lot of ways, Solus is appealing: it is a rolling release with software freshness that is just a bit behind Tumbleweed with some curation like Void.  However, it's a smaller project, so its limited software availability makes it less appealing to me.</p>
<h3 id="beginner-friendly-vs-customizable">Beginner friendly vs customizable</h3>
<p>Some distros have a reputation for being beginner-friendly.  Attributes of these include:</p>
<ul>
<li><strong>Ease of installation:</strong> GUI installers are typically more friendly than command line or chroot installation, although even a GUI installer can present an overwhelming number of choices to new users. Arch used to be famously challenging for new users to install, although they now have an easier <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/Archinstall">archinstall</a> option. </li>
<li><strong>Less reliance on the command line:</strong> Some offer GUI package management or configuration alternatives to command line interfaces or config file editing.</li>
<li><strong>Ease of printer setup:</strong> This was generally painful when I first started, but now is often done automatically. However, some distros make it unnecessarily hard still, like openSUSE.</li>
<li><strong>How easy it is to install proprietary drivers and codecs:</strong> Distros often don’t include proprietary drivers out of the box for legal or ideological reasons.</li>
<li><strong>The size and helpfulness of their communities:</strong> Some communities like Linux Mint expect and welcome new users.  Others are more geared toward experts who may encourage <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/RTFM">self-help</a>.</li>
</ul>
<p>Note there may be other trade-offs that could matter to some, like distros being steered by companies vs communities or <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.gnu.org/distros/optionally-free-not-enough.html">philosophical differences in how “free” the software needs to be</a>.</p>
<h2 id="efficient-frontiers">Efficient frontiers</h2>
<p>A useful idea when it comes to considering trade-offs is the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Efficient_frontier">efficient frontier</a>.  This refers to the set of options that offer the best possible trade-offs for the qualities you value most. In finance, for example, the efficient frontier is composed of the investments that provide the highest return for each given level of risk. While this is universally favorable in finance, operating systems present a more subjective case: different users have different preferences. For instance, an experienced user might find a beginner-friendly distribution too restrictive or bloated. Ultimately, each user's unique preferences will shape their own version of the efficient frontier.</p>
<p>An example of a distro on the efficient frontier for many beginners was Ubuntu, which leveraged the stability of Debian to provide many beginner-friendly features.  Here I will try to identify some distributions which I believe fall on my efficient frontier.</p>
<h2 id="my-use-cases-and-preferences">My use cases and preferences</h2>
<h3 id="primary-use-cases">Primary use cases</h3>
<ul>
<li>Personal data science projects, sometimes requiring CUDA</li>
<li>Software development</li>
<li>Web browsing </li>
<li>Reading pdfs or ebooks</li>
<li>Youtube / streaming TV</li>
<li>Occasional multimedia editing</li>
</ul>
<h3 id="desktop-environments-and-window-manager-preferences">Desktop environments and window manager preferences</h3>
<p>The UI is an important part of any PC operating system experience.  We’ll take a quick tour of the main Linux desktop environments, and I’ll share my preferences.</p>
<h4 id="kde-plasma-vs-gnome">KDE Plasma vs Gnome</h4>
<p>These are the two heavyweights of the Linux desktop environments (DEs).  I personally prefer <a rel="noopener nofollow noreferrer" target="_blank" href="https://kde.org/plasma-desktop/">Plasma</a>, which has a nice out-of-box experience, but is also highly customizable, and provides Windows-like keyboard shortcuts, but many distros use <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.gnome.org/">Gnome</a> as their flagship DE.</p>
<p>There are also some lighter weight DEs (<a rel="noopener nofollow noreferrer" target="_blank" href="https://lxqt-project.org/">LXQT</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://xfce.org/">Xfce</a>).  They’re particularly good for resource-constrained VMs and older hardware, but some people prefer them even more generally.  The <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/Cinnamon">Cinnamon desktop</a> is used by Linux Mint and preferred by those who think computing peaked with Windows 7.</p>
<h4 id="hyprland-vs-sway">Hyprland vs Sway</h4>
<p>Aside from fully fledged desktop environments are window managers (or compositors under <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/Wayland">Wayland</a>).  They tend to be more bare bones and require more configuration from the user.  They can be lighter weight, unless the user loads them up with features.</p>
<p>I recently tried out these compositors.  I first tried <a rel="noopener nofollow noreferrer" target="_blank" href="https://swaywm.org/">Sway</a>, because it had a reputation for being more stable and lighter weight, but I ran into some bugs and didn't like the default window placements.  So I tried out <a rel="noopener nofollow noreferrer" target="_blank" href="https://hyprland.org/">Hyprland</a> and really like it.  The main benefit of Hyprland is that it dynamically places your newly open windows in sensible, unoverlapping places.  It also looks very nice and has animations, which I first thought to be superfluous, but I think the visual cues enhance usability.</p>
<p>Hyprland works particularly well with bleeding edge (or close to it) distros like Arch, NixOS, and openSUSE Tumbleweed; with a bit more work, it seems like people are able to get it to run on other distros, even if they can’t use every new plugin.  It mostly works very well, although I occasionally run into rough edges with the clipboard manager or have to search for something that KDE provides easy access to.</p>
<p>I would not recommend new users jump straight into window managers or compositors.  For those who want to try it out, I would recommend they first configure their own installations before using anyone else’s dotfiles.  Some nicely pre-configured installations are available from <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/JaKooLit/Hyprland-Dots">JKoolit</a> (several distros) and <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/mylinuxforwork/dotfiles">MyLinuxForWork</a> (for Arch based distros). I would also recommend keeping a more stable desktop environment as a backup, as hyprland has fast paced development, and dependencies can go out of sync.</p>
<h4 id="coming-soon-cosmic">Coming soon: Cosmic</h4>
<p>A new DE called <a rel="noopener nofollow noreferrer" target="_blank" href="https://system76.com/cosmic">Cosmic</a> looks to have what I would call “smart configuration,” where a smaller number of choices lead to a set of changes that go well together.  It also provides a dynamic tiling window manager within a full desktop environment.  It’s also developed in Rust, which should reduce an entire class of bugs and security vulnerabilities.  But it’s still in alpha, so I doubt whether I would consider this seriously for another year or two.</p>
<h3 id="summary-of-preferences">Summary of preferences</h3>
<p>In short, for this recent distro search, I preferred:</p>
<ul>
<li>Good KDE Plasma support
<ul>
<li>And (after I tried it) good hyprland support</li>
</ul>
</li>
<li>Rolling releases to avoid my inertia with point release upgrades</li>
<li>Relatively easy installation, although I don’t mind some extra work or decisions</li>
<li>Batteries included &gt; minimal &gt; fully loaded</li>
<li>Large software selection, ideally in official repos</li>
<li>Blind updates that are relatively reliable and trivial to recover from</li>
<li>Making use of <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/Unified_Extensible_Firmware_Interface/Secure_Boot">secure boot</a> without too much effort or maintenance</li>
<li>Good Nvidia compatibility, particularly for CUDA
<ul>
<li>this rules out Musl in favor of glibc, which is required for CUDA</li>
</ul>
</li>
</ul>
<h2 id="the-stopgap">The stopgap</h2>
<h3 id="tuxedo-os">Tuxedo OS</h3>
<p>When I first got my new desktop in March, I installed <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.tuxedocomputers.com/en/TUXEDO-OS_1.tuxedo">Tuxedo OS</a> to buy time while I researched further.  It’s like Kubuntu without <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.gamingonlinux.com/2023/02/ubuntu-flavours-to-drop-flatpak-by-default-and-stick-to-snaps/">Canonical shoving Snaps down your throat</a>.  The OS is based on the Ubuntu LTS versions with KDE Plasma, via <a rel="noopener nofollow noreferrer" target="_blank" href="https://neon.kde.org/">KDE Neon</a>, by a <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.tuxedocomputers.com/en">company that sells hardware for Linux</a>.  They replaced Snaps with Flatpaks and optimized it for their own hardware, although I’ve also <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.phoronix.com/review/tuxedo-os-1">seen benchmarks that suggest it outperforms vanilla Ubuntu on other hardware</a> as well.  I found it to be very user friendly, exemplified by pushing instructions in a desktop notification for how to upgrade to Plasma 6.  Nvidia drivers were installed without extra effort, although Nvidia did not work well on wayland, due to driver issues (not Tuxedo’s fault beyond being slow to adopt drivers that worked well).</p>
<p>I had audio issues though, which were probably due to its older kernel and new hardware that differs from Tuxedo’s.  The audio would crackle for about a minute any time I adjusted the volume, a problem that did not persist on the next OS I installed with a newer kernel.  A big downside of Tuxedo OS is that it is far downstream from Debian and even Ubuntu.  The Ubuntu LTS version comes out once every two years, plus Tuxedo needs to wait for KDE Neon to come out before it rebases to that.  So the Ubuntu base it’s using can be nearly two and a half years old before it rebases.  But, if you are using older hardware (or Tuxedo’s newer hardware) and want KDE Plasma, it seems like a reasonable choice for a beginner.</p>
<h2 id="my-efficient-frontier">My efficient frontier</h2>
<h3 id="opensuse-tumbleweed-via-geckolinux">openSUSE Tumbleweed via GeckoLinux</h3>
<p>When the new laptop arrived in May, I installed <a rel="noopener nofollow noreferrer" target="_blank" href="https://get.opensuse.org/tumbleweed/">openSUSE Tumbleweed</a> (via <a rel="noopener nofollow noreferrer" target="_blank" href="https://geckolinux.github.io/">GeckoLinux Rolling</a>) on it.  This distro is aimed at professionals (developers and sysadmins).  It’s much closer to the “bleeding edge” than Debian or Ubuntu.  Packages are typically only delayed about a few days to a week from their upstream releases, and they use automated testing to help provide stability.   As a rolling release, you get multiple Tumbleweed releases and hundreds of package updates per week.  It comes configured with snapper for btrfs snapshots that can be booted directly from grub.  Interestingly, the GeckoLinux installer was last released in August of 2022 and does not do a network install.  The Zypper package manager successfully completed nearly 2 years worth of updates without issue, which really impressed me.  Tumbleweed has a good software selection, although not quite as complete as Debian, Arch, and Fedora.  I've seen cases where Void has a package in its official repos that Tumbleweed doesn't, but it’s pretty rare that I run into any availability issues I care about.  Tumbleweed detected the CPU microarchitecture on the laptop automatically, and it feels very snappy; easily the best laptop experience I’ve had.</p>
<p>As impressive as Zypper is at resolving updates, it’s also slow, largely because it <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/openSUSE/zypper/issues/104">does not support parallel downloads</a>.  And sometimes packages have unusual names that depart from common conventions on other distros, which can add some uncertainty when searching.</p>
<p>Because of its extensive testing, Tumbleweed is known as a “stable rolling release.”  However, during my time with it, there have certainly been exceptions that have led users (including me) to have to rollback to an earlier btrfs snapshot.  Issues in just the past few months included AMD graphics issues, Intel wifi issues, and hyprland getting out of sync with one of its dependencies.</p>
<p>openSUSE (or <a rel="noopener nofollow noreferrer" target="_blank" href="https://forums.opensuse.org/t/project-rebrand/176691">whatever it will be called</a>) has another offering that is in beta right now: <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.opensuse.org/Portal:Slowroll">Slowroll</a>, which offers monthly releases with security updates in between.  This sounds pretty appealing, as rollbacks still inevitably suck up time with research and checking forums to see when issues have been resolved.  Slowroll should be able to avoid most of the issues that pop up on Tumbleweed, and being just one to two months behind upstream releases is still better than point release distros.</p>
<h3 id="cachyos">CachyOS</h3>
<p>Eventually, after I was feeling good about rolling releases with Tumbleweed, I wanted to replace Tuxedo OS on the desktop.  I had seen CachyOS early in my research, but dismissed it since it was Arch-based, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.reddit.com/r/linuxmemes/comments/1rzlhp/every_time_people_say_arch_linux_is_too_unstable/">Arch has a reputation</a> for <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.reddit.com/r/linuxmemes/comments/tcvq3w/arch_updating_meme/">being hard to maintain</a>.  But after trying several other distributions on VMs, I gave it a closer look and really liked what I saw, so installed it on my desktop in mid-August.</p>
<p>CachyOS has to be on the efficient frontier for ease of use and up-to-date software, plus it’s <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.phoronix.com/review/cachyos-linux-perf">one of the highest benchmarking OSes out there</a>. The installation was very easy.  It's not as minimal as Arch but installed fewer packages than Tumbleweed.  It provides shortcuts to setup btrfs snapshots with snapper (you need to choose grub as the bootloader and install <code>btrfs-grub</code> to enable booting directly into them), as well as enabling AppArmor and secure boot.  CUDA also just worked right out of the box.  And it’s <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.cachyos.org/features/optimized_repos">optimized for your CPU microarchitecture</a>, which it automatically detects on installation, with an <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.cachyos.org/cachyos_basic/why_cachyos/#advanced-cpu-scheduler-support">improved scheduler</a>.</p>
<p>It provides a GUI to trigger common package management, system update, and maintenance tasks.  In short, it provides <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/watch?v=lcnCo1bYwUA&amp;t=1439s">Arch on easy mode</a>, although it's probably a good idea to <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/System_maintenance">learn more about maintaining an Arch system</a>.  And it occasionally requires manual intervention, just like Arch.  However, despite its Arch roots, I don’t feel qualified to <a rel="noopener nofollow noreferrer" target="_blank" href="https://knowyourmeme.com/memes/btw-i-use-arch">“btw”</a> people.</p>
<p>This distro was the biggest surprise to me. Despite <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.cachyos.org/desktop_environments/hyprland/">Cachy’s warnings about hyprland</a>, it has worked just fine—especially after I switched to running Hyprland on the iGPU, leaving the Nvidia GPU reserved for CUDA-related tasks.  In short, it’s ease of use and btrfs snapshots have made the bleeding edge feel relatively easy and stable so far.  And if you're into gaming, it's <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/watch?v=zMpVYZJvBiI&amp;t=87s">apparently one of the best options</a>.</p>
<h3 id="void">Void</h3>
<p>I installed this one on an old laptop (after some VMs) to play around and thought it would end up on my desktop.  It’s a minimal, lightweight distro without systemd, and is on the efficient frontier of stability for a rolling release, but at a more conservative point than Tumbleweed/Slow Roll.  Package releases appear to be more curated; for instance they didn’t make Plasma 6 available for months while they felt it still had too many bugs.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/void-linux/void-packages/commits/master/srcpkgs/linux/template">Void’s default kernel hasn’t been updated in nine months</a>, which is a bit unusual compared to the two to five month cadence they had prior to that.  The 6.6 kernel that it defaults to as of this writing wouldn't fully support either of my new computers, although they do allow users to use a &quot;mainline&quot; kernel that keeps rolling, but with lowered compatability expectations.</p>
<p>It has very high user satisfaction, including the <a rel="noopener nofollow noreferrer" target="_blank" href="https://distrowatch.com/dwres.php?resource=ranking">highest average user rating on distrowatch</a>, although it probably has a high self-selection bias, attracting mostly experienced users.  It has surprising software availability for small user base, with some major caveats.  For instance, it has no browser forks; so no derivatives of chromium or firefox, leaving users to compile for themselves or to use a 3rd party repo.  It also <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/void-linux/void-packages/issues/37544">does not have hyprland in its official repos</a>, and the installation process looks <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/watch?v=-tCdeY1Jqk0">non-trivial</a> (although this may change with <a rel="noopener nofollow noreferrer" target="_blank" href="https://hyprland.org/news/independentHyprland/">hyprland’s recent breakup with wlroots</a>).  It provides templates for installing packages from source in git repos.  It provides the choice between glibc and musl, but you’ll <a rel="noopener nofollow noreferrer" target="_blank" href="https://docs.voidlinux.org/installation/musl.html">need glibc version (opposed to musl) to use Nvidia proprietary drivers and CUDA</a>.</p>
<p>Along the same lines as Tumbleweed, Void is considered a “stable rolling release,” and Void is considered even more stable.  However, I’ve realized there is no such thing as a truly stable rolling release, and it appears that <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.reddit.com/r/voidlinux/comments/1crhzeu/upgrade_to_kde_plasma_6_is_now_working/">Void had a bit of trouble in the transition from KDE Plasma 5 to 6</a>.  And oddly enough, Plasma 6 is still available through a package called <code>kde5</code> ([Edit: 2024-12-08] this package was renamed to <code>kde-plasma</code> sometime after the initial publication of this post).</p>
<p>As a minimal distro, it presents a good way to learn more about Linux.  And while the documentation is pretty good, you’re on your own if you want to go off the beaten path, which may require a <a rel="noopener nofollow noreferrer" target="_blank" href="https://docs.voidlinux.org/installation/guides/chroot.html">chroot installation</a>.  I see the appeal of Void, and I may tinker with it a bit more to see if I can get a setup I like.  But for now, it seems like a bit too much work compared with CachyOS.  This makes me think there may be room for an unofficial spin to help install with encryption, secure boot, snapper, btrfs snapshots, grub, and a wrapper around xbps to automate the snapshots.</p>
<h2 id="also-considered">Also considered</h2>
<p>These are some other distros I tried out on VMs, but did not bother to install on real hardware.</p>
<h3 id="fedora">Fedora</h3>
<p>This distro is popular with developers, but <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/watch?v=x9qCqRTEVz0&amp;t=682s">tends to be experimental</a>, moving to new technologies quickly.  It has a relatively frequent, six month point release schedule.  Fair or not, KDE Plasma is often portrayed as a second class citizen to them ([Edit: 2024-12-08] <a rel="noopener nofollow noreferrer" target="_blank" href="https://pagure.io/Fedora-Council/tickets/issue/504">Fedora has promoted KDE Plasma</a> release to an &quot;edition&quot; instead of a &quot;spin&quot;, so this may get better.]).  In my VM installation, the KDE spin used by far the most RAM of any distro I tried.</p>
<h3 id="nobara">Nobara</h3>
<p>Nobara is a distro focused on gaming, which is not my main use case, but I had heard that it fixed up a lot of frustrations with Fedora (like making Nvidia drivers easy and good KDE Plasma support), so I took a look.  Ultimately, I decided the point releases were too much of a drawback and I didn’t want all the gaming packages.  I think I read that Nobara uses some of Cachy’s kernel patches for performance, and Cachy offers some gaming packages from Nobara’s creator, so for gamers it may come down to a preference between Fedora and Arch as a base for these two.</p>
<h2 id="honorable-mentions">Honorable mentions</h2>
<p>These are some other distros that I never installed on hardware or VMs, but are worth mentioning.</p>
<h3 id="nixos">NixOS</h3>
<p>NixOS seems to have leapfrogged in popularity in recent years.  My impression is that it is a minimal distro, which is good for tinkering, has up-to-date software, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://nixos.org/manual/nixos/stable/#sec-configuration-syntax">build-via-config file</a> leads to easily repeatable installations.</p>
<p>However, needing to learn a new configuration programming language was a deterrent.  It also departs from the Unix <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard">filesystem hierarchy standard</a>, which can lead to compatibility issues, and even <a rel="noopener nofollow noreferrer" target="_blank" href="https://discourse.nixos.org/t/shebang-locations/28992">requires a different shebang on shell scripts by default</a>.  So, a little like Macs, this seems to be off from the rest of the Linux world.  I may tinker with this on VMs in the next few years.</p>
<p>[Edit: 2024-12-08] It looks like NixOS is a <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.reddit.com/r/NixOS/comments/1drkymj/python_is_a_nightmare_on_nixos/">particularly bad fit for Python development</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.reddit.com/r/NixOS/comments/1fv4hyg/anyone_using_python_uv_on_nixos/">uv also requires some workarounds</a>.  It may be too much of a hassle for my use case.</p>
<h3 id="gentoo-and-arch">Gentoo and Arch</h3>
<p>I never really considered using these, as they seem to require too much attention from their users, but I want to include them for completeness.  These are rolling, minimalist, bleeding edge distros aimed at people who know what they’re doing (or are willing to spend a lot of time ramping up).  Gentoo even required compiling source code locally, giving its users greater control over compilation options to get exactly what they want, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.gentoo.org/news/2023/12/29/Gentoo-binary.html">although they recently added optional binary packages as well</a>.  Their user bases are extremely knowledgeable and active, as evidenced by their <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/Main_page">extensive</a> <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.gentoo.org/wiki/Main_Page">wikis</a>.  Despite its reputed propensity for borking after updates, my impression is that Arch has become more reliable, which also benefits Cachy.</p>
<h2 id="tips-for-new-users">Tips for new users</h2>
<p>Some may hesitate to replace the OS that came with their computer.  Switching it is not as scary as it seems, particularly if you choose a distro with a friendly GUI installer.  If you’re coming from Windows, you can always reinstall Windows if you’re not happy.  Windows ties licenses to hardware for OEM installations, which makes it especially easy to switch back if needed, or you could <a rel="noopener nofollow noreferrer" target="_blank" href="https://wiki.archlinux.org/title/Dual_boot_with_Windows">dual boot</a>.  Also practice installing distros on VMs first, and look up anything in the process you’re unsure about.  Take notes during installation and configuration, both during the VM and real hardware installations.</p>
<p>Just use the <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.gnu.org/software/bash/">bash shell</a> to start.  Some users will advocate for <a rel="noopener nofollow noreferrer" target="_blank" href="https://fishshell.com/">fish</a> or <a rel="noopener nofollow noreferrer" target="_blank" href="https://zsh.sourceforge.io/">zsh</a>, but you’ll have the easiest time if you just start with bash.  Fish can be especially troublesome in how it veers from <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/POSIX">POSIX</a> standards.</p>
<p>Try to stick to official distro repos for software, as these tend to be the most reliably maintained and in sync with each other.  Some distributions offer user contributed repos, such as the AUR (Arch), OBS (openSUSE), copr (Fedora), PPAs (Ubutntu) to supplement the official repos.  But you should use them only when needed, as they are more likely to lead to dependency conflicts and potential security issues, especially until you get the hang of what you’re doing.</p>
<p>Don’t chase extremes to the detriment of usability.  For instance, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.clearlinux.org/">Clear Linux</a> <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.phoronix.com/review/intel-ubuntu2404-fedora40">will show the best performance benchmarks</a>, but it’s not a user-friendly desktop OS.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.alpinelinux.org/">Alpine</a> is among the most minimal and secure operating systems, but would be a pain to use as a daily driver.  Both of these are made for enterprise use cases and are not intended for personal use.</p>
<p>Most servers you’ll work with are likely to be Debian-based (including Ubuntu) or Fedora-based (including Amazon Linux 2).  So if your main purpose is to get experience with a distro you are likely to work with on a server, choose a Debian or Fedora distro or derivative.</p>
<p>If you don’t know which distro to use, just try the live USBs for <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.linuxmint.com/download.php">Linux Mint</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://os.tuxedo.de/archive/">TuxedoOS</a> and pick the one you like better.  They are both Debian -&gt; Ubuntu derivatives.  If gaming is a big use case, then look at <a rel="noopener nofollow noreferrer" target="_blank" href="https://bazzite.gg/">Bazzite</a> (which is Fedora based) as well.</p>
<p>One common way new users bork their system is</p>
<ol>
<li>Google their problem online</li>
<li>Find a post they think matches their problem</li>
<li>Follow the solution by copying and pasting shell commands or configurations without understanding them</li>
<li>It turns out it was a subtly different problem than their own</li>
<li>Now the changes give them a new, worse problem</li>
</ol>
<p>So if you find a potential solution, don’t blindly copy and paste commands (which is also a security risk); go research the solution further to make sure it makes sense for your specific case.  Could be for a different distro, or a different distro version, or different graphics card, etc.</p>
<p>If you want resources to learn Linux, <a rel="noopener nofollow noreferrer" target="_blank" href="https://nostarch.com/tlcl2">The Linux Command Line</a> is a good one for a new user, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://nostarch.com/howlinuxworks3">How Linux Works</a> will teach users more about how their system works under the hood.</p>
<h2 id="tips-for-users-moving-to-a-rolling-distribution">Tips for users moving to a rolling distribution</h2>
<p>Rolling distributions like Arch require their users to stay on top of news that may require manual intervention, something users of Debian and its derivatives, can mostly ignore.  I've found for bleeding edge projects like CachyOS and hyprland that it suffices to join their Discord servers to receive announcements about updates.</p>
<p>The more up-to-date distros like Arch, CachyOS, and Tumbleweed will have software updates available every day.  Falling too far behind risks a botched update (although Tumbleweed seems to do fine).  It should suffice to update once per week at some point when the user has sufficient time to handle any inconveniences that may arise.  I recommend rebooting immediately after the system update to check if anything went wrong so you can rollback if needed.</p>
<p>Speaking of which: practice rolling back at least once after setting up snapper btfs snapshots on grub to make it sure it works.  You can simply install one package you don't intend to keep, reboot into the previous snapshot, rollback, and reboot again.</p>
<h2 id="most-useful-resources-for-learning-about-distros">Most useful resources for learning about distros</h2>
<p>Here I want to share some resources I’ve found helpful over the past six months or so as I’ve been looking around the Linux landscape to evaluate options.  You need to be a little careful with content creators, since they err to the side of too much content.  For instance, some may make distro-hopping appear more desirable than it really is, and they do so on studio machines that don’t really impact their own personal use.</p>
<ul>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://www.linuxuserspace.show/">Linux User Space</a> podcast
<ul>
<li>They spend one month with each distro they cover (except for <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.linuxuserspace.show/420">one year on GenToo</a>), which gives them good perspectives on the pros and cons of each distro.</li>
</ul>
</li>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/thelinuxcast">The Linux Cast</a>
<ul>
<li>Matt gives thorough distro reviews from a non-developer perspective, which helps give a sense for user-friendliness.  He also provides good reviews of other Linux software like file managers.</li>
</ul>
</li>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/A1RM4X">A1RM4X</a>
<ul>
<li>A1RM4X focuses on distros for gaming and content creation, which aren’t my main use cases, but I like his coverage of installations and usability, even if I skip over the fps benchmarks.  Plus, lots of CachyOS content.</li>
</ul>
</li>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://distrowatch.com/">Distrowatch</a>
<ul>
<li>I usually read the last year’s worth of reviews when researching a distro to see what themes come up, and I think the average user reviews are telling.  I wouldn’t really consider a distro with under an 8.0 average, and I would take an average rating with a grain of salt if it has fewer than 30 reviews.</li>
</ul>
</li>
</ul>
<h3 id="other-useful-linux-channels">Other useful Linux channels</h3>
<ul>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/c/BrodieRobertson">Brodie Robertson</a>
<ul>
<li>Brodie provides a very technical user perspective, but does not do that many distro reviews.  He does provide good reviews of other software like file managers.  He did <a rel="noopener nofollow noreferrer" target="_blank" href="https://youtu.be/NujVOhdBhAU?si=w81wlsWo538xfh5M">this video for new Linux users</a>. </li>
</ul>
</li>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/@ChrisTitusTech">Chris Titus</a>
<ul>
<li>Chris Titus brings a sysadmin perspective and also does <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/@TitusTechTalk">quite a few live streams</a>, which can be useful for seeing installations.  He also has some material geared toward beginners such as <a rel="noopener nofollow noreferrer" target="_blank" href="https://christitus.com/choose-linux-distro/">this one on choosing a Linux distro</a> with <a rel="noopener nofollow noreferrer" target="_blank" href="https://youtu.be/dL05DoJ0qsQ?si=1c0kbgm1SoB0RMrW">this accompanying video</a> and this <a rel="noopener nofollow noreferrer" target="_blank" href="https://christitus.com/linux-for-beginners/">Linux explainer for beginners</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://youtu.be/10f4899srvc?si=o6atcRojyRsC4dpj">its video</a>.</li>
</ul>
</li>
</ul>
<p>There are some other ones I’ve found that have helpful tips, cover recent news, or specialize in niche distros, but I think that’s enough to get a Linux newbie started.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I’ll sum up my thoughts from this recent Linux deep dive:</p>
<ul>
<li>btrfs + <code>snapper</code> + <code>btrfs-grub</code> = bork insurance</li>
<li>Bork insurance can give users more confidence in using up-to-date, rolling release distros</li>
<li>openSUSE Tumbleweed is great and GeckoLinux makes it easy to get started with (and the new <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/openSUSE/agama">agama installer</a> looks promising as well).  I suspect Slow Roll will be an even better choice for me once it's out of beta (and starts to leverage openSUSE's <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.opensuse.org/openSUSE:OpenQA">openQA service</a> for automated testing) and hyprland's development stabilizes.</li>
<li>Void’s curated, stable rolling release also looks appealing and may be where I eventually land on the desktop, but requires a fair amount of work to go outside the standard installation path.  I think an unofficial spin that adds bork insurance and other common needs, similar to SprialLinux, could be helpful here.</li>
<li>CachyOS has been the biggest surprise.  I would have never guessed that I would end up on CachyOS with hyprland back in March.  I never thought I would use an Arch-based distro due to active management it demands, but Cachy was just that easy to set up.  If it proves stable enough over time, I may stick with it over Void.</li>
<li>I’m also interested to see where the atomic/immutable distros go by the next time I need to upgrade.</li>
</ul>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: Bernoulli&#x27;s Fallacy</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          <category><![CDATA[Probability]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/clayton-bernoullis-fallacy/</link>
          <guid>https://dfd.github.io/clayton-bernoullis-fallacy/</guid>
          <description><![CDATA[Overview
Aubrey Clayton's Bernoulli's Fallacy is perhaps the most accessible introduction to Jaynes' version of &quot;logical probability.&quot;  But it goes further than that: Clayton provides the essential elements of the development and historical debates on the interpretation…]]></description>
          <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>Aubrey Clayton's <a rel="noopener nofollow noreferrer" target="_blank" href="https://aubreyclayton.com/bernoulli"><em>Bernoulli's Fallacy</em></a> is perhaps the most accessible introduction to Jaynes' version of &quot;logical probability.&quot;  But it goes further than that: Clayton provides the essential elements of the development and historical debates on the interpretation of probability and statistical inference through Fisher; gives numerous examples of how frequentist methods can fail; connects these issues to the academic research replication crisis; and suggests a way forward.</p>
<h2 id="interpreting-probability">Interpreting probability</h2>
<p>After motivating self-reflection within the reader through some example probability problems, Clayton enumerates the most common interpretations, including ancient notions of chance, frequentist, subjective, and axiomatic, while dissecting the appeal and problems of each one.  Finally he presents &quot;probability as logic,&quot; which will serve as author's point of view throughout the book.</p>
<p>To summarize, <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Probability_axioms#Kolmogorov_axioms">Kolmogorov's axiomatic probability</a> had put probability on a rigorous foundation, but did not solve the interpretation problem; all modern interpretations could adhere to the axioms.  The logical point of view then evolved with contributions from <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/A_Treatise_on_Probability">Keynes</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://arxiv.org/pdf/0804.3173">Jeffries</a>; <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Cox%27s_theorem">Cox showed</a> how to break probability free from the notion of proportions and frequencies and to represent plausibilities while still adhering to the axioms; and Jaynes completed the evolution with his book, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712"><em>Probability Theory: The Logic of Science</em></a>.  Clayton walks the reader quickly through this evolution, and the final intuition is as follows:</p>
<p>Probability is an extension of logical deduction.  While logical deduction is process for calculating implications of interest from propositions that have true or false values, probability allows us to do the same with propositions of uncertain truth values.  In fact, as Clayton relays, probability reduces to logical deduction when working exclusively with probabilities of 0s and 1s.  Another key point is that all probability is conditional; we are always determining probability from <em>some</em> information, and that two rational people with the same information will calculate the same probabilities.  Jaynes viewed probability as a way to process information, not as proportions or long run frequency.  Finally, probability is epistemology, not ontology.  That is, probability represents our state of knowledge rather than reality itself, and confusing our uncertainty with reality is the <em>mind projection fallacy</em>.</p>
<p>Following this, Clayton walks the reader through some classic probability problems using this view of the probability, pointing out along the way that there is no need in any of them to define probability as long run frequency.</p>
<h2 id="bernoulli-s-blunder">Bernoulli's blunder</h2>
<p>Clayton next turns our attention to the history of probability and statistical inference, starting with Bernoulli.  In his telling of the history, Bernoulli was the first to make a mistake that would haunt statistics to the present day: concluding that a statistic derived from a particular sample is probably close to its true value for any sampled value, rather than the correct conclusion that a yet-to-be-drawn sample statistic will probably be close to the true value for any true value.  Clayton explains that Bernoulli's fallacy came from confusing the sampling probability (the likelihood), with the probability of a hypothesis.  These two are related through Bayes theorem, so Bernoulli needed to include more information to complete the calculation for his claim, and it's not necessarily true.</p>
<p>Clayton then goes on to illustrate <em>why</em> these two conclusions are not equivalent, despite appealing to people's intuition.  Specifically, he arms the reader with a particular framework to define hypotheses, assign prior probabilities, assign the sampling probability (or likelihood), calculate the &quot;pathway probability&quot; (the unnormalized posterior of multiplying the previous two numbers), and finally the posterior probability for each hypothesis through normalization.  This is done in a similar manner to Allen Downey's <a rel="noopener nofollow noreferrer" target="_blank" href="https://docs.google.com/drawings/d/1dHkIa-RdmnLPze84gkcmYflWwA7xwtwFhXPs_Rd0oRM/edit">Bayesian worksheet</a> (example use case <a rel="noopener nofollow noreferrer" target="_blank" href="https://allendowney.blogspot.com/2017/02/a-nice-bayes-theorem-problem-medical.html">here</a>).</p>
<p>To end the chapter, Clayton covers <em>base rate neglect</em>, first presenting the canonical medical test example, followed by the <em>prosecutor's fallacy</em> with the tragic story of <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Sally_Clark#Conviction_for_murder">Sally Clark's conviction</a>, before explaining that these are in fact the same error, confusing $P[A | B]$ for $P[B | A]$; the same one made by Bernoulli himself. </p>
<h2 id="wrong-turn">Wrong turn</h2>
<p>The next chapter covers the bridge from applying statistics in the hard sciences to softer ones.  This can be traced back to Gauss and Laplace, who developed and justified the method of least squares regression in astronomy, and Quetelet, who looked to apply the methods to the development of &quot;social physics.&quot; Around this time, the common interpretation of probability also starts to shift toward frequency.  This is due to both empirical data seeming to adhere to probability distributions, and because of academics questioning the origins of prior probabilities.</p>
<p>Chapter Four covers Galton, Pearson, and Fisher's influence on statistics.  These three advanced frequentist statistical theory and applied it to comparisons of groups of people, for the purposes of eugenics.  In this pursuit, they wanted to develop statistical methods that could be seen as &quot;objective&quot; to bolster their claims.  Clayton wrote <a rel="noopener nofollow noreferrer" target="_blank" href="https://nautil.us/how-eugenics-shaped-statistics-238014/">a related piece in 2020</a> that overlaps a bit with this chapter.</p>
<p>Chapter Five starts with an illustrative dialogue between a fictitious student and frequentist bot.  The humorous conversation conveys all the typical confusions one encounters while trying to navigate the logic and interpretation of frequentist methods.  It sits in stark contrast to the consistency of the previously presented Bayesian analyses.</p>
<p>Clayton then turns his attention back to Fisher, and the development of disparate tests for various situations, presenting <a href="https://dfd.github.io/mcelreath-statistical-rethinking/#connecting-methods-to-science">a flow chart similar to the one presented by McElreath</a>.  But Clayton goes beyond this sight gag in his critique of frequentist methods, calling out issues in both theory and practice.  He also cites evidence that Fisher, despite his vitriol toward Bayesian methods, actually drew inspiration from them in his derivation of maximum likelihood and grew a soft spot for them in his later writings.  And finally, his zig zagging during the development of <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Fiducial_inference">fiducial inference</a> eventually led him to argue against his own earlier concepts of inference since the new paradigm would not work for his new paradigm, which was trying to draw Bayesian conclusions without use of priors.</p>
<p>The chapter concludes with nine example problems that require Bayesian reasoning to obtain sensible solutions and for which frequentist methods fail miserably.</p>
<h2 id="frequently-wrong">Frequently wrong</h2>
<p>Clayton then turns his attention to the replication crisis in academia, and particularly in the social and medical sciences.  After citing objections to null hypothesis testing over time by those who recognized its irrelevance and misuse, he presents the theory and evidence of why so many modern studies have failed to replicate. Then he recounts the curious case of Daryl Bem, who turned from ESP skeptic to <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Daryl_Bem#%22Feeling_the_Future%22_controversy">publishing &quot;statistically significant&quot; research</a> on the subject.  Bem's claims were implausible to most and yet used standard methods of analysis.  Clayton then shares rumors that Bem had pulled this off as metacommentary on research practices.</p>
<p>In the final chapter, Clayton proposes reform to research practices.  It will come as no surprise that the proposal includes abandoning frequentist methods, including null hypothesis testing, and instead embracing Bayesian methods.  He even goes so far as to suggest that universities dissolve their statistics departments, since Bayesian inference relies on only one theorem.  That seems a bit far to me, as Bayesian models and computation can become quite complex, and there are still regular developments in Bayesian methods and practice.  Plus, even if researchers across fields used Bayesian methods, many won't master them at an advanced level, so it's helpful to have some centralized academics who can help or guide empirical work.</p>
<h2 id="recommendation">Recommendation</h2>
<p>This is a unique book in its scope: a straightforward presentation of Logical Probability; an overview of the historical debates; hands-on examples of where frequentist methods fail and how Bayesian methods help; and insights and proposed remedies related to the academic replication crisis.  It's not as math-heavy or dense as a textbook, but there is some math sprinkled throughout.  The notation includes probability, summations, factorials, binomial coefficients, and exponential functions.  So while I think the concepts are well-explained, a reader will want to feel comfortable with that notation and related concepts to get the most out of it.</p>
<p>Recommended for those looking for an introduction to Jaynes or the &quot;statistics wars.&quot;  I had first read this over two years ago, and it was fun to revisit and flip through.  If you feel inspired to move on to Jaynes' book itself afterward, Clayton also has a <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/watch?v=rfKS69cIwHc&amp;list=PL9v9IXDsJkktefQzX39wC2YG07vw7DsQ_">video lecture series</a> that covers the book from start to finish.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: Statistical Rethinking</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Wed, 24 Jul 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/mcelreath-statistical-rethinking/</link>
          <guid>https://dfd.github.io/mcelreath-statistical-rethinking/</guid>
          <description><![CDATA[<p>I'll start with the conclusion: McElreath's <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/036713991X/ref=pd_sim_d_sccl_3_5/141-3103872-8081264?psc=1">Staistical Rethinking</a> is my favorite textbook.  In the rest of this post I'll explain why and offer advice on how to get the most out of this book.</p>
]]></description>
          <content:encoded><![CDATA[<p>I'll start with the conclusion: McElreath's <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/036713991X/ref=pd_sim_d_sccl_3_5/141-3103872-8081264?psc=1">Staistical Rethinking</a> is my favorite textbook.  In the rest of this post I'll explain why and offer advice on how to get the most out of this book.</p>
<span id="continue-reading"></span><h2 id="my-background-before-reading-this-book">My background before reading this book</h2>
<p>By the time I started reading the first edition of this book in late 2017, I was already leading a sizable data science and analytics team and had a masters degree in statistics.  For that degree, most courses used frequentist methods, although I also took the one Bayesian statistics course that was offered.  So I wasn't exactly a beginner, and yet I still claim that I learned at least as much from this book as I did from that degree program.  I later used this book to teach a course at my old company, so I've also seen how well it works for those with non-stats backgrounds.</p>
<p>In this post, I'll refer to the 2nd edition.</p>
<h2 id="connecting-methods-to-science">Connecting methods to science</h2>
<p>McElreath starts off by presenting the <a rel="noopener nofollow noreferrer" target="_blank" href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-01?slide=28">typical maze of statistical tests</a> students are presented with in a statistics curriculum, and explains why this often causes confusion.  He then goes on to argue that these tests are not enough for research because they are difficult to adapt to unique contexts and fail in unpredictable ways.</p>
<p>Chapter one continues on to describe why deductive falsification, which is often used as the justification for applying frequentist null hypothesis tests, is impossible in most scientific contexts.  This is because &quot;many models correspond to the same hypothesis, and many hypotheses correspond to a single model,&quot; which &quot;makes strict falsification impossible.&quot;  This idea is <a rel="noopener nofollow noreferrer" target="_blank" href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-01?slide=34">illustrated here</a>, which shows that many processes are consistent with stated hypotheses, and many processes may be consistent with a single statistical model. </p>
<p>Measurement errors and continuous hypotheses also make falsification challenging in practice, and because of these issues, falsification is consensual rather than logical.  That is, scientists debate the merits of the evidence and come to a consensus over time.</p>
<p>The remainder of the chapter is spent outlining the methods for doing better science that are covered in the book: Bayesian data analysis, model comparison, multilevel models, and graphical casual models.</p>
<h2 id="illustrative-explanations">Illustrative explanations</h2>
<p>I think people like this book so much because the explanations bring the concepts to life, giving the reader an intuition that goes well beyond the definitions.  These are given throughout the book, but here are two examples that have always stuck with me:</p>
<p>Chapter 2 contains perhaps the best illustration of a likelihood function that I've seen.  It does so by presenting a simple case, in which a bag contains four marbles, each of which is blue or white.  Three marbles are drawn with replacement, and their colors are the observed data.  The unknown parameter of interest, $p$, is the proportion of marbles that are blue, so this sets up a discrete parameter, discrete outcome case.  The likelihood is calculated by enumerating all of the possible ways data can be generated for each conjecture (each possible value of parameter $p \in \{ 0,\frac{1}{4}, \frac{1}{2}, \frac{3}{4}, 1 \}$ ) by taking three draws from the bag, and highlighting which possible outcomes match the observed outcome.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2022-lecture-02?slide=5">You can see the example play out over consecutive slides starting here.</a>  The lesson is simple: the likelihood is the relative number of ways that the observed data can occur for each value of the parameter.  While such literal counting is only feasible in the discrete/discrete case, it gives the right intuition for continuous cases as well. </p>
<p>Another explanation that stands out is for KL Divergence and demonstrating why it is not symmetric like a distance metric.  The example given concerns predicting whether we will land on land or water, using Earth to set expectations for landing on Mars and then using Mars to set expectations for Earth.  Mars' surface is almost entirely land (we count the ice caps as water on 1% of the surface for the sake of the example), while Earth's surface is mostly water but with a significant amount of land.  So if you randomly land on Mars while using Earth to guide expectations, you'll be a little surprised but not shocked to (almost certainly) touch down on land.  If you randomly land on Earth using Mars to guide your expectations, there's a good chance you'll land in water, which will be shocking if you were picturing the 1% water on Mars.  The extra surprise means that using the proportions of land and water on Mars to guide expectations about Earth produces larger KL divergence than vice versa.</p>
<h2 id="insightful-plots">Insightful plots</h2>
<p>A strength of this book is the plots used for model checking.  It conveys good habits for plotting relationships and comparing the model to data.  As every experienced practitioner knows, plotting adds valuable information far beyond what model fit metrics can provide alone.</p>
<h2 id="bayesian-philosophy">Bayesian philosophy</h2>
<p>While he points out advantages of Bayesian methods over frequentist ones, McElreath avoids making sweeping claims for Bayesian methods.  In fact, he argues that no statistical approach by itself is sufficient, and there's a healthy humility as he discusses the modeling process.</p>
<p>The book takes the &quot;logical&quot; view of probability (<a rel="noopener nofollow noreferrer" target="_blank" href="https://bayes.wustl.edu/etj/prob/book.pdf">see here</a> for a long explanation), often associated with Jaynes, and also does a nice job delineating epistemology from ontology.  The simulation of tadpoles in tanks and the role of hyperparameters in the <em>Models with Memory</em> chapter is a good example of clarifying this difference, as is the discussion of the i.i.d. assumption and Jaynes' <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Mind_projection_fallacy"><em>mind projection fallacy</em></a>.</p>
<h2 id="working-up-to-multilevel-modeling-and-beyond">Working up to multilevel modeling and beyond</h2>
<p>The book is sensibly organized to gradually build up to multilevel models with partial pooling, which is one of the Bayesian super powers.  Along the way, the book introduces essential and useful topics for a practicing statistician, including spurious association, masked relationships, confounding variables, interactions, and causal DAGs.  It also covers modeling with Gaussian, binomial, Poisson, categorical, zero-inflated, and ordinal response variables.</p>
<p>It consistently presents the formal specification of each model before moving onto the code.  It's a great practice to get into and helps to clarify and communicate a model's intention.  Plus, modern Bayesian libraries use declarative model syntax that closely aligns with this format, bridging conceptual understanding and assumptions to code.</p>
<p>Continuing a bit beyond multilevel models to some other advanced topics, the book includes covariance, Gaussian Processes, missing data, measurement error, and instrument variables.  After I used it to teach the class at work, our team members were able to tackle some pretty sophisticated problems where quantifying uncertainty or leveraging a flexible model structure were crucially important.</p>
<h2 id="traditional-topics-missing">Traditional topics missing</h2>
<p>One thing the book skips over, which is normally covered in an introductory Bayesian course, is solving simple problems with conjugate priors analytically.  In practice, it's only applicable to a very small set of problems.  But I think doing the math by hand for a few examples will help some learners better understand what the MCMC sampling is approximating.  I don't think this belongs in McElreath's book; it's just a suggestion to find another resource to try that out separately, perhaps before reading this book, or sometime around chapter 3 or 4.</p>
<p>Continuing along those lines, it's generally not a math heavy book.  The math is usually presented, but not required, since most problems are solved computationally.  I think this serves the book and its scope well, but for moving on to more advanced material afterwards, I think a mathematical statistics course or book would be helpful to supplement.  <a href="https://dfd.github.io/probabilistic-machine-learning/">Probabilistic Machine Learning</a> (PML) actually covers this material quite well (and also includes a section on conjugate prior models).  Having that background ahead of time would probably help comprehend this book a bit better, but I don't think it's necessary.</p>
<p>Another traditional topic is implementing your own <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis Hastings</a> samplers.  This is again not particularly practical with modern libraries for Bayesian inference that do the hard work for you (and tend to use <a rel="noopener nofollow noreferrer" target="_blank" href="https://arxiv.org/abs/1701.02434">Hamiltonian Monte Carlo</a>).  I don't necessarily think a reader needs to supplement with this, although diving into the sampling algorithms may help with diagnosing problems and understanding when reparameterization is helpful in more complex models.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/book2.html">PML2</a> covers MCMC sampling.</p>
<p>And finally another traditional topic not found in the book is <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Jeffreys_prior">Jeffreys priors</a>.  These are intellectually interesting, but rarely use in applied work.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/book2.html">PML2</a> covers Jeffreys priors.</p>
<h2 id="evolving-editions">Evolving editions</h2>
<p>As mentioned, I originally read the first edition of the book, which was already great.  I also bought a copy of the second edition, which added several more topics, but the main difference may be how it stresses causal diagrams (DAGs) throughout.  The forthcoming 3rd edition <a rel="noopener nofollow noreferrer" target="_blank" href="https://elevanth.org/2024_01_02_third_edition.html">will apparently feature more on the Bayesian workflow</a>.</p>
<h2 id="recommendation">Recommendation</h2>
<p>McElreath thoughtfully weaves together philosophical, conceptual, and computational considerations through motivating examples and insightful illustrations.  If you want to learn (or relearn) statistics in a way that will clarify your thinking and prepare you for a wide variety of modeling problems, this is a must read.  I'll describe how I went through the book, which is what I'd recommend to get the most out of it.</p>
<p>When I read this book, I wanted to make a collection of the code examples from it in Python.  It's such a popular book that the author's code, which uses an <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.rdocumentation.org/packages/rethinking/">R package the author created</a>, has been translated to several other probabilistic programming languages (PPLs).  I was using PyMC3 at the time (now replaced by PyMC version 5), so I grabbed an available translation for it, and started making my own notebooks.  However, I noticed there were some differences between results from the resource I found and the book's results.  I think doing the full translation from scratch myself would have been tedious, but figuring out how to correct some examples was helpful.</p>
<p>By the way, I don't think which framework you choose is that important.  In fact, when I taught the class, I stressed that our chosen framework was somewhat arbitrary and that those will change over time; what's important is learning the concepts and practicing with modern MCMC samplers.  In practice, I've used <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.pymc.io/welcome.html">pymc</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://num.pyro.ai/en/stable/index.html">numpyro</a>, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://mc-stan.org/">stan</a> for various projects.</p>
<p>I also reproduced every plot in the book with code.  Much of McElreath's plotting code was not published in the book, so figuring that out also helped my understanding of the mounds of samples the models were generating.  Plotting is so important for model improvement.</p>
<p>And finally, as preparation for teaching a class with the book, I selected a small number of problems to do from each chapter.  I recommend choosing at least one conceptual question, and one computational question (and ideally one from each major topic) from each chapter to help get hands on experience with the material in the book.  McElreath also publishes homework problems and solutions from his course, which is a good option.</p>
<p>At the end of it, I had an amazing set of code examples in a framework I wanted to use, along with a solid understanding of how to approach a wide variety of problems.  This book on its own can bring an active learner up to an intermediate level.  It's also great conceptual preparation for more dense books like <a href="https://dfd.github.io/probabilistic-machine-learning/">PML</a> or <a rel="noopener nofollow noreferrer" target="_blank" href="http://www.stat.columbia.edu/~gelman/book/">BDA3</a>.</p>
<p>Take your time with this one.  You'll get out of it what you put into it.</p>
<h2 id="extra-resources">Extra Resources</h2>
<p>McElreath publishes <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.youtube.com/@rmcelreath/playlists">lectures for the book here</a>.  You can just find the latest (or whichever one matches your book's edition) and follow along.  Or you can look for the latest class <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/rmcelreath?tab=repositories">github repo</a> with lectures and assigned homework and solutions here (specific example <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/rmcelreath/stat_rethinking_2024">here</a>)</p>
<p>I mentioned translations for various PPLs.  Here are some for the 2nd edition of the book.</p>
<ul>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/pymc-devs/pymc-resources/tree/main/Rethinking_2">Translation for PyMC.</a></li>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://fehiepsi.github.io/rethinking-numpyro/">Translation for numpyro.</a></li>
<li><a rel="noopener nofollow noreferrer" target="_blank" href="https://vincentarelbundock.github.io/rethinking2/">Translation for rstan.</a></li>
</ul>
<p>I haven't seen a complete translation for pystan, but stan itself is a language, so it should be relatively straightforward to convert to pystan for those with python data experience.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: The Theory That Would Not Die</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Thu, 18 Jul 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/mcgrayne-the-theory-that-would-not-die/</link>
          <guid>https://dfd.github.io/mcgrayne-the-theory-that-would-not-die/</guid>
          <description><![CDATA[Overview
I had put off reading Sharon Bertsch McGrayne's The Theory That Would Not Die for a long time, but I'm glad I finally got to it late last year.  Although I had previously read some books with &quot;history of statistics&quot; content, like Bernoulli's Fallacy and The Lad…]]></description>
          <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>I had put off reading Sharon Bertsch McGrayne's <a rel="noopener nofollow noreferrer" target="_blank" href="https://yalebooks.yale.edu/book/9780300188226/the-theory-that-would-not-die/">The Theory That Would Not Die</a> for a long time, but I'm glad I finally got to it late last year.  Although I had previously read some books with &quot;history of statistics&quot; content, like <a rel="noopener nofollow noreferrer" target="_blank" href="https://aubreyclayton.com/bernoulli">Bernoulli's Fallacy</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Lady-Tasting-Tea-Statistics-Revolutionized/dp/0805071342">The Lady Tasting Tea</a>, this book introduced a large number of new stories I hadn't heard before about the development and application of Bayesian methods as well as its detractors.</p>
<p>While <em>Bernoulli's Fallacy</em> primarily focused on a narrow lineage of academic thought for and against Bayesian methods up through Fisher, and <em>The Lady Tasting Tea</em> ran through all the big names in academic statistics in the 20th century without an overarching theme, McGrayne follows the thread of Bayesian statistics from its early development all the way through to its modern usage.  And whereas <em>Bernoulli's Fallacy</em> advocates and explains the Bayesian philosophy, McGrayne is focused on telling the full story with historical context, highlighting key figures, turning points, and historical use cases.</p>
<p>The book's long subtitle presents a nice synopsis: &quot;How Bayes' rule cracked the Enigma code, hunted down Russian submarines &amp; emerged triumphant from two centuries of controversy.&quot;</p>
<h2 id="new-to-me">New to me</h2>
<p>I learned something new in just about every chapter.  I was even embarrassed to realize I didn't know the motivating example of Bayes' original paper, using a Beta distribution to estimate the unknown location of a ball on a square table, using only information about whether other randomly thrown balls had landed to the left or right of it.  That really is clever, given the historical context.</p>
<p>There was some overlap between the sections on Laplace and Fisher from the aforementioned books, but many of the other stories I either hadn't seen at all or hadn't seen in as much depth.</p>
<p>For instance, I wasn't aware that Bayesian shrinkage had been imbued into actuarial practice as <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Credibility_theory">Credibility theory</a>, unbeknownst to many actuaries.  Or that there were proponents at Harvard Business School starting in the late 1950s.  There were many such stories that were completely new to me.</p>
<h2 id="world-war-ii">World War II</h2>
<p>I had long been familiar with Turing's work at Bletchley Park for cracking the Enigma Code with <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Banburismus">Banburismus</a> using early electro-mechanical computers, known as <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Bombe">bombes</a>, which enabled the Allies to navigate the Atlantic safely and evade U-boats.  McGrayne dedicates most of a chapter to this, as well as some other applications of Bayes to the war effort.  Helping to defeat the Nazis is a nice notch on Bayes' belt.</p>
<h2 id="nate-silver-s-prequel">Nate Silver's Prequel</h2>
<p>I hadn't previously heard the story of John Tukey, a well known statistician of Princeton and Bell Labs, joining NBC News to help call elections for 18 years with statistical methods, starting in 1960.  Some who helped with the programming claimed they were using empirical Bayes, although Tukey never admitted to using Bayesian methods.  The author shares speculation that Tukey's involvement with Cold War era national security, where Bayes' rule was widely used, may have been part of his reluctance to credit the method.</p>
<h2 id="biggest-surprise">Biggest surprise</h2>
<p>The most surprising thing I learned in this book was that the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman Filter</a> was <em>not</em> motivated by Bayes rule, and in fact Kalman disliked Bayesian methods.  I had previously assumed it was intended as an analytical solution to updating a Bayesian model with conjugate priors.  In any case, it does indeed have a <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Kalman_filter#Relationship_to_recursive_Bayesian_estimation">Bayesian interpretation</a>, like other filtering methods for localization.</p>
<h2 id="nitpicks">Nitpicks</h2>
<p>While McGrayne is not a mathematician or statistician herself, she seemed to have a pretty good handle on the Bayesian framework at a conceptual level.  However, there were two things that bothered me.  The first is that Bayes' rule is not a theory; it's a <em>theorem</em> that can be proven from the axioms of probability.  I think perhaps calling it a &quot;theory&quot; may have been in reference to using it for statistical inference, but it makes for a confusing book cover when &quot;theory&quot; appears to reference to Bayes' rule itself.</p>
<p>Second (and related to the first point), I kept waiting for her to state that Bayes' rule is <em>not at all controversial</em> in probability theory; it's only been debated in its application to estimating parameters in statistical models, where Bayesian practitioners apply probability to model parameters just as they would to any other uncertain quantity of interest.  I would also separate out the philosophical disagreements on the interpretation of probability from Bayes' rule itself.  Frequentist statisticians don't doubt the validity of Bayes' rule; it's a particular application of it they doubt.</p>
<h2 id="recommendation">Recommendation</h2>
<p>What I like about books like this and <em>Bernoulli's Fallacy</em> is that you can see the path dependence of history so clearly.  It's not as if people wake up each day and reevaluate what statistical methods they plan to use.  These things carry momentum and inertia with them, and wrong turns can last a long time.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Planck%27s_principle">&quot;Science progresses one funeral at a time,&quot;</a> as it were.</p>
<p>I've only covered a handful of stories told within this book; I was pleasantly surprised at its breadth and selectively chosen depth.  While it avoids the technical weeds, <em>The Theory That Would Not Die</em> does the most thorough job surveying the history of thought and application of Bayesian statistics as I've seen.  So if that interests you, I recommend it.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: Information Theory, Inference, and Learning Alogrithms</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Mon, 15 Jul 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Machine Learning]]></category>
          <category><![CDATA[Statistics]]></category>
          <category><![CDATA[Information Theory]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/mackay-itila/</link>
          <guid>https://dfd.github.io/mackay-itila/</guid>
          <description><![CDATA[Overview
David MacKay's classic textbook Information Theory, Inference, and Learning Algorithms was first published in 2003, and yet it holds up very well over 20 years later, seamlessly weaving together concepts across compression, noisy-channel coding, Bayesian statistics, and …]]></description>
          <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>David MacKay's classic textbook <a rel="noopener nofollow noreferrer" target="_blank" href="http://www.inference.org.uk/mackay/itila/book.html">Information Theory, Inference, and Learning Algorithms</a> was first published in 2003, and yet it holds up very well over 20 years later, seamlessly weaving together concepts across compression, noisy-channel coding, Bayesian statistics, and neural networks.  By the end, a diligent reader will have a very good understanding of fundamentals that are vital to more advanced work in Bayesian statistics and deep learning.</p>
<h2 id="a-unique-perspective">A unique perspective</h2>
<p>MacKay uniquely presents the material, drawing parallels between seemingly distinct topics.  As he states in Chapter 2, &quot;One of the themes of this book is  that data compression and data modeling are one and the same, and that they should both be addressed... using inverse probability.&quot;</p>
<p>One chapter presented a unique idea for model comparison using Occam's Razor.  He presents an &quot;Occam's Factor&quot; which &quot;provides the ratio of the poterior accessible volume of $\mathcal{H}_i$'s hypothesis space to the priori accessible volume, or the factor by which $\mathcal{H}_i$'s hypothesis space collapses when the data arrive.&quot;  It's logarithm &quot;is a measure of the amount of information we gain about the model's parameters when the data arrive.&quot;</p>
<p>So models with many parameters and few constraints will be penalized by a stronger Occam's factor than a simpler model because they had too much &quot;wiggle room&quot; to overfit the data.  It's similar in spirit to AIC or BIC, except it can be applied to the full distributions over parameters, as opposed to just the maximium likelihood point estimation models.  Later he goes so far as to say it can be used as a substitute for validation sets in Bayesian machine learning.  I'm not sure I'm ready to commit to that, but I'm interested to explore it further.  It seemes like it could be gamed.  For instance, if you know of an overfit model and its parameter values for a set of training data, and you then set your priors to be equal to these, then the parameters wouldn't change and the Occam's Factor of that model would be small.  So this seems to be relying on restricting oneself to truly using priors before seeing the data.</p>
<h2 id="diverse-topics">Diverse topics</h2>
<p>As if the diversity of high level topics weren't enough, perhaps the most interesting sequence in the book are chapters 18 and 19.  He applies the tools from the Information Theory section first to crossword puzzles, and then presents a simplified version of the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Cryptanalysis_of_the_Enigma">Enigma Code breaking done at Bletchley Park</a>.  Finally, he ends the section with an information theoretic point of view on the benefits of sexual reproduction with recombination using both analytical and simulation methods.</p>
<h2 id="relevance-to-nlp-and-deep-learning">Relevance to NLP and deep learning</h2>
<p>Even in Claude Shannon's original paper that introduced information theory, <a rel="noopener nofollow noreferrer" target="_blank" href="https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">A Mathemetical Theory of Communication</a>, he presented language models.  And if one studies natural language processing today, it's still filled with concepts from information theory.  Although this book came out 14 years before the transformer architecture was introduced in <a rel="noopener nofollow noreferrer" target="_blank" href="https://research.google/pubs/attention-is-all-you-need/">Attention is All You Need</a>, it will give readers an outstanding foundation from which to study modern approaches.  I worry too many practitioners just take the shortest path to the latest architectures without building the requisite knowledge first.</p>
<p>I also like how he distills neural networks into their architecture, activity rule, and learning rule.  The architecture specifies the variables and relationships within the network; the activity rule details how the outputs of neurons change in response to each other; the learning rule defines how to update weights during training.</p>
<p>One other topic I found particularly interesting was on the information capacity of a single neuron (and relating it back to VC dimension), as well as the capacity of Hopfield networks.  Walking through these analyses gives the reader building blocks for reasoning about the capacity of larger, more complex networks.</p>
<p>While it's true that the Bayesian methods for which MacKay advocates are not the dominate paradigm used in deep learning today, the probabilistic perspective is broad enough to interpret parameters with point estimates.  Regularization and drop out also have Bayesian interpretations.</p>
<h2 id="comparison-to-frequentist-methods">Comparison to frequentist methods</h2>
<p>MacKay mostly focuses on presenting the material at hand, but occasionally contrasts to frequentist methods.  One example comes in Chapter 24, where he discusses the estimators for $\sigma$ in a Gaussian distribution.  As he states:</p>
<p>Given data $D = \{x_n\}_{n=1}^N$, and 'estimator' of $\mu$ is</p>
<p>$$
\bar{x} \equiv \sum_{n=1}^N x_n / N,
$$
and two estimators of $\sigma$ are:
$$
\sigma_N = \sqrt{\frac{\sum_{n=1}^N (x_n - \bar{x})^2}{N}}
\;\text{and}\;
\sigma_{N-1} = \sqrt{\frac{\sum_{n=1}^N (x_n - \bar{x})^2}{N-1}}
$$</p>
<p>He discusses how they invent estimators in the frequentist paradigm and choose the one that best meets some criteria of sampling properties.  After pointing out that there is no clear principle for deciding which criterion to use, and given most criteria, there's no systematic way to produce an optimal estimator, he then explains the frequentist interpretation of these estimators.  The estimator $(\bar{x}, \sigma_N)$ is the maximum likelihood estimator, but $\sigma_N$ is biased.  That is, averaging over repeated sampling, $\sigma_N$ will not equal to $\sigma$.  The $\sigma_{N-1}$ version is unbiased.</p>
<p>In contrast, the Bayesian view arrives at these quantities as maximum a posteriori estimates of $\sigma$ with different conditioning.  The maximum of $P(\sigma | D)$ is at $\sigma_{N-1}$ and the maximum of $P(\sigma | D, \mu = \bar{x})$ is $\sigma_N$, using uninformative priors.  In other words, $\sigma_{N-1}$ is when we are jointly estimating our uncertainty in $\mu$ and $\sigma_N$ is when we hold $\mu$ fixed at $\bar{x}$.  There are nice supporting visuals on page 321.</p>
<p>One of the more lively and entertaining chapters is 37, <em>Bayesian Inference and Sampling Theory</em>.  In this short chapter, he offers very simple examples to demonstrate that frequentist methods calculate unhelpful quantities to the decision at hand or are sensitive to irrelevant information.</p>
<h2 id="nitpick">Nitpick</h2>
<p>Perhaps the one thing I didn't care for in the book was the early insistence on approximating $x!$ and ${n \choose x}$, which seemed like a bit of an unnecessary distraction.  This did not last long, however.</p>
<h2 id="problems">Problems</h2>
<p>The book offers a lot of problems, each with a difficulty rating between 1 and 3 next to it.  Some of the problems are quite challenging.  I recognized one problem as being a slight reframing of a <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/William_Lowell_Putnam_Mathematical_Competition">Putnam</a> A6 question, and it was only marked with a difficulty of 2.  Luckily, he also marked a small portion as recommended problems, and these problems were thoughtfully chosen to reinforce the content.</p>
<h2 id="recommendation">Recommendation</h2>
<p>Overall, I see this book as being a good one to read after Kevin Murphy's <a href="https://dfd.github.io/probabilistic-machine-learning/">Probabilistic Machine Learning</a>.  They draw from a similar perspective, but MacKay's goes deeper within what it covers.</p>
<p>So my recommendation is to read the whole book, after PML, and do all of the recommended problems.  The book is comprised of 50 relatively short chapters.  I think readers could alternate between reading a chapter and then doing its recommended problems, and complete the book in 100 days without much problem.  Very motivated readers could probably finish in 50 days by reading a chapter and completing the problems each day.  Consider it a &quot;must read&quot; if you want to go deep into NLP.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: Probabilistic Machine Learning: An Introduction</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Sat, 30 Mar 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Machine Learning]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/probabilistic-machine-learning/</link>
          <guid>https://dfd.github.io/probabilistic-machine-learning/</guid>
          <description><![CDATA[<p>I'm surprised people aren't making a bigger deal about Kevin Murphy's new textbooks, the first of which I'll review here.</p>
]]></description>
          <content:encoded><![CDATA[<p>I'm surprised people aren't making a bigger deal about Kevin Murphy's new textbooks, the first of which I'll review here.</p>
<span id="continue-reading"></span><h2 id="overview">Overview</h2>
<p><a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/book1.html">Probabilistic Machine Learning: An Introduction</a> covers an incredible breadth and surprising depth of machine learning and statistics topics.  It can be thought of as a &quot;best of&quot; from <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Bishop</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="http://www.inference.org.uk/mackay/itila/">MacKay</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://hastie.su.domains/ElemStatLearn/">The Elements of Statistical Learning</a>, and others, along with a view of recent, relevant research, all of which is pulled together under a probabilistic persepctive and consistent notation.  The book is almost entirely self-contained, with an extensive <em>Foundations</em> section covering most prerequisite topics<sup class="footnote-reference"><a href="#prereqs">1</a></sup> before moving on to linear models, deep neural networks, nonparametric models, unsupervised learning, and a few other topics.</p>
<h2 id="clear-exposition">Clear exposition</h2>
<p>The book takes a Bayesian view of probability (that we can treat all unknown quantities, such as future outcomes and parameters, as random variables and model them with probability distributions) and applies it to statistical and machine learning models.  In doing so, Murphy is able to apply a consistent framework and common concepts across a very wide range of topics, regardless of how they were originally motivated or conceived.  This approach turns the exploration of various models into a seamless composition of building blocks, rather than a series of jumps between the origin stories of each model. </p>
<p>Perhaps surprisingly, the book manages to cover the most important<sup class="footnote-reference"><a href="#important">2</a></sup> concepts one would encounter in graduate statistics coursework.  In fact, even after exiting the aforementioned <em>Foundations</em> section, it spends a good chunk of the book on linear models, much of which would overlap with a statistics curriculum.  This is helpful for two reasons:</p>
<ol>
<li>First, it's important to apply the right tool for the job.  While complex ML models increasingly do amazing things, there are still many applications and contexts when we need the imposed structure of GLMs and other simpler models.</li>
<li>More importantly, it introduces the structure of Murphy's approach, which extends naturally from the <em>Foundations</em> section, to relatively simple models before moving on to more complex topics.  This allows the reader to more easily adapt and generalize the framework.</li>
</ol>
<p>A common pattern is:</p>
<ol>
<li>Motivate  the model and define the likeliihood, such as
$$
p(y| \mathbf{x}; \mathbf{\theta}) = \text{Ber}(y|\mathbf{\sigma}(\mathbf{w}^\intercal \mathbf{x} + b))
$$
in the case of logistic regression, adding definitions, priors, or nested functions if applicable.</li>
<li>Define and derive the objective function, such as the negative log likelihood.</li>
<li>Show how the optimization of this objective is done in practice.</li>
</ol>
<p>The book extensively borrows from earlier textbooks and papers for explanations, figures, and problems, always with proper attribution. In fact, it may have the most citations per page I've encountered.  Murphy has meticulously adapted the clearest plots and explanations for each subject and selected problems that provoke deep insights into the material.  Additionally, the extensive use of self-references within the book help to facilitate refreshing an earlier topics as needed.</p>
<h2 id="practical-theory">Practical theory</h2>
<p>In practice, I find the the probabilistic perspective to be much more useful than the theory emphasized in some other ML books.  For instance, while I understand the role of <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension">VC-dimension</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">PAC learning</a> in the development of statistical learning theory, I'm rarely utilizing those ideas in applied work.<sup class="footnote-reference"><a href="#SLT">3</a></sup>  Murphy's book explains them in under two pages, while some other books devote entire chapters to each of these concepts.</p>
<p>Also, as he notes, the probabilistic perspective lends itself to optimal decision making and is shared across science and engineering disciplines, which is what helps fuel the breadth of the two books.</p>
<h2 id="breadth-and-depth">Breadth and depth</h2>
<p>Have a look at the <a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/toc1.pdf">table of contents</a>.  I don't know of another textbook that covers linear Gaussian systems, GLMs, and transformers, not to mention Gaussian processes, factorization machines, and graph embeddings (plus the <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/probml/pml2-book/blob/main/toc2-long-2023-01-19.pdf">even longer sequel book on &quot;advanced topics&quot;</a> <em>and</em> the <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/probml/pml-book/blob/main/supp2.md">supplemental material</a> to that!).  The power of his approach is that it allows him to write succintly, yet clearly and precisely, on a variety of subjects, adding color and intuition when needed.</p>
<p>The book examines the most crucial and fundamental topics, offering detailed proofs and derivations, unveiling important results, and comparing the performance of competing methods across various problems. For example, linear regression methods are thoroughly explored over forty pages. Furthermore, the text dedicates three chapters to deep neural networks, addressing structured data, images, and sequences specifically. In contrast, it touches briefly on many specialized and niche topics, subsequently directing readers to one or more citations for further investigation.</p>
<h2 id="source-code">Source code</h2>
<p>While the text focuses on the theory, concepts, and math, Murphy makes available the <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/probml/pyprobml/tree/master/notebooks/book1">code to reproduce most of the figures</a>.  This often includes experiments and model fitting, for those who want to try out models from the book.  I have not used the code much myself, although I did find <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/probml/pyprobml/issues/1113">an issue with the mixture of linear experts code</a>.</p>
<h2 id="corrections">Corrections</h2>
<p>I preordered a hard copy of the first printing and, while meticulously working through it, encountered several typos, some even within the mathematical content.  For each typo discovered, I referred to the  <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/probml/pml-book/releases/latest/download/book1.pdf">latest draft version</a> to verify if it had already been corrected.  Many had been previously identified and fixed, yet I was still able to report <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/probml/pml-book/issues?q=is%3Aissue+author%3Adfd+">several new ones</a>, all of which were subsequently addressed.  Contributing in this way to such an important book was an unexpected perk.</p>
<p>In a weird way, the typos actually enhanced my experience, since catching them was a nice way to validate I understood the material well (your mileage may vary).  Collecting corrections via github probably increased reader participation, and I would assume the current draft is in great shape as a result.</p>
<h2 id="recommendation">Recommendation</h2>
<p>Probabilistic Machine Learning is one of two introductory ML textbooks I would recommend.  I think its perspective is the perfect complement to the inductive bias<sup class="footnote-reference"><a href="#inductive">4</a></sup> lens found in Tom Mitchell's classic <a rel="noopener nofollow noreferrer" target="_blank" href="http://www.cs.cmu.edu/~tom/mlbook.html">Machine Learning</a> textbook.</p>
<p>If you're looking to develop expertise in machine learning, I recommend going through this book cover to cover and attempting each problem provided.  You might not be able to solve them all, so time box each attempt.  You can check <a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/solns-public.pdf">solutions to many of the problems</a>, but give each of them a shot on your own first.</p>
<p>I was able to move through it relatively quickly, but it helped that I already had graduate degrees focused on statistics and machine learning.<sup class="footnote-reference"><a href="#experience">5</a></sup>  If you find some of your math background is shaky or that the <em>Foundations</em> section is too dense, there are plenty of online resources available to help out.  I particularly like the <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.coursera.org/specializations/mathematics-machine-learning">Mathematics for Machine Learning</a> course from Imperial College London for linear algebra and multivariate calculus.  If you need to supplement with such outside sources, I would still use the structure within PML to guide what specific topics to brush up on, rather than committing to an endless math curriculum before moving on to ML.  This would also be an ideal book for readers with a Bayesian statistics background to dive into machine learning, since they will be familiar with the framing.</p>
<p>If you find a particular subject interesting, grab the <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/probml/pyprobml/tree/master/notebooks/book1">associated code</a> and apply it to some new problems or run experiments.  If you feel like you've mastered a chapter of the book, try to generate data sets that deliberately demonstrate the comparative strengths or weaknesses of the models within it.</p>
<h2 id="conclusion">Conclusion</h2>
<p>This has become my go-to reference on ML.  Whenever I encounter a question about ML or need to refresh my memory on a topic, I usually go straight to this book (or <a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/book2.html">its sequel</a>) over google, wikipedia, or other books.</p>
<p>For a  long time, it's been common for study groups to read through <a rel="noopener nofollow noreferrer" target="_blank" href="https://hastie.su.domains/ElemStatLearn/">ESL</a> together while learning ML.  I think some of its sections are good, but it never appealed to me in the same way and relies too much on null hypothesis testing and p-values for my taste.  <em>Probabilistic Machine Learning</em> now offers a better, modern alternative to cultivate a way of thinking that extends well beyond what many people narrowly think of as &quot;machine learning.&quot;</p>
<p>The book is <a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/book1.html">freely available online</a>; however, considering its extensive length, opting for a hard copy allows for a much-needed respite from computer screens.  You can <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Probabilistic-Machine-Learning-Introduction-Computation/dp/0262046822">purchase it here</a>.  And if you want to hear more about the author's perspective, you <a rel="noopener nofollow noreferrer" target="_blank" href="https://learnbayesstats.com/episode/68-probabilistic-machine-learning-generative-models-kevin-murphy/">can listen to a podcast interview here</a>.</p>
<p>I also have the <a rel="noopener nofollow noreferrer" target="_blank" href="https://probml.github.io/pml-book/book2.html">second book</a>, but haven't yet had time to read it (although I've already used it as a reference on several topics).  Hopefully I will start it later this year.</p>
<h2 id="footnotes">Footnotes</h2>
<div class="footnote-definition" id="prereqs"><sup class="footnote-definition-label">1</sup>
<p>The book assumes knowledge of basic set theory and calculus, and comfort with math notation will help, although there is an appendix for that.  It <em>very briefly</em> covers derivatives and matrix calculus, so that might be challenging for someone new to those subjects.  Integration is not covered within the book, so I think it's fair to say that a reader should know calculus well before starting.</p>
</div>
<div class="footnote-definition" id="important"><sup class="footnote-definition-label">2</sup>
<p>While it briefly describes null hypothesis testing, it does not <a rel="noopener nofollow noreferrer" target="_blank" href="https://speakerdeck.com/rmcelreath/statistical-rethinking-2023-lecture-01?slide=28">devolve into various statistical tests</a>.  I said <em>important</em>, not <em>common</em>.</p>
</div>
<div class="footnote-definition" id="SLT"><sup class="footnote-definition-label">3</sup>
<p>These concepts may help develop an intuition about machine learning, but they are certainly not the concepts I start with when facing a new prediction problem.</p>
</div>
<div class="footnote-definition" id="inductive"><sup class="footnote-definition-label">4</sup>
<p>Inductive bias describes how a learning algorithm conducts its search over a hypothesis space, and can be further broken down into restriction bias (which hypotheses are considered) and preference bias (which hypotheses are preferred) for analysis.  Mitchell's book covers much more than this, of course, but it's the unique aspect I find most useful.</p>
</div>
<div class="footnote-definition" id="experience"><sup class="footnote-definition-label">5</sup>
<p>Even with this background, I still learned quite a bit from the book.</p>
</div>
]]></content:encoded>
      </item>
      <item>
          <title>Book Review: Probably Overthinking It</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Sun, 17 Mar 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Statistics]]></category>
          <category><![CDATA[Causal Inference]]></category>
          
          
          <category><![CDATA[book review]]></category>
          
          <link>https://dfd.github.io/probably-overthinking-it/</link>
          <guid>https://dfd.github.io/probably-overthinking-it/</guid>
          <description><![CDATA[Overview
Allen Downey’s Probably Overthinking It describes and resolves a number of statistical fallacies and paradoxes in an accessible way.  The subtitle is How to use data to answer questions, avoid statistical traps, and make better decisions, and an overarching theme is the …]]></description>
          <content:encoded><![CDATA[<h2 id="overview">Overview</h2>
<p>Allen Downey’s <a rel="noopener nofollow noreferrer" target="_blank" href="https://greenteapress.com/wp/probably-overthinking-it/">Probably Overthinking It</a> describes and resolves a number of statistical fallacies and paradoxes in an accessible way.  The subtitle is <em>How to use data to answer questions, avoid statistical traps, and make better decisions</em>, and an overarching theme is the various ways sampling bias can impact inference.  He illustrates the issues visually, demonstrates each with multiple examples, and shows adjustments that can overcome and explain them.</p>
<p>This book is well suited for data scientists, analysts, and mathematically inclined consumers of quantitative analyses.  While a background in probability and statistics would help get more out of the book, it’s not particularly technical; Downey does not dive deeply into the mathematical details, leaving much of the explanation to well designed plots and stories that accompany them.  It’s primarily a book on how to <em>reason</em> about observational data and evaluate models.</p>
<p>Having read <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/">Downey’s blog</a> (and <a rel="noopener nofollow noreferrer" target="_blank" href="http://allendowney.blogspot.com/">his previous blog</a> before that) for the past 12 years or so, I was familiar with many themes in the book, although some of the examples were new.  Topics include (among others) the <a rel="noopener nofollow noreferrer" target="_blank" href="http://allendowney.blogspot.com/2015/08/the-inspection-paradox-is-everywhere.html">Inspection Paradox</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2021/04/07/berkson-goes-to-college/">Berkson’s Paradox</a>, <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2021/05/25/in-search-of-simpsons-paradox/">Simpson’s Paradox</a>, and the <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/AllenDowney/BiteSizeBayes/blob/master/05_test.ipynb">base rate fallacy</a>.</p>
<h2 id="what-makes-the-book-unique">What makes the book unique</h2>
<p>One thing I like about the book is its emphasis on choosing appropriate probability distributions and plotting methods.  Students tend to come out of university statistics courses overemphasizing the Gaussian distribution, and a big lesson from this book is that the choice of distribution can matter a lot to a model’s quality and the decisions it informs. He also explains circumstances under which the Gaussian distribution <em>is</em> an appropriate choice and how to compare the fit of different modeling distributions.  He does quite a bit of modeling with the log-normal distribution, which I've used extensively in industry, and even the log-t distribution, which I've also applied in practice. </p>
<p>The book also focuses on comparing model predictions to observable data, rather than on parameters, standard errors, or hypothesis tests.  This is done with widespread use of ECDFs and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2022/10/03/the-long-tail-of-disaster/">log-log complementary CDFs when we want to emphasize tail behavior</a> (these are <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.cs.cmu.edu/~christos/courses/826-resources/PAPERS+BOOK/0412004-newman05.pdf">widely used for power law distributions</a>, often found in networks).  It also touches on survival analysis, which many analysts and data scientists don’t seem to encounter in school.</p>
<h2 id="most-interesting-chapters">Most interesting chapters</h2>
<p>Perhaps the most interesting topic is Berkson’s Paradox, for which the following example is given:</p>
<ul>
<li>Babies of mothers who smoked were about 6% lighter at birth</li>
<li>Smokers were about twice as likely to have babies lighter than 2,500 grams, which is considered “low birthweight”</li>
<li>Low-birthweight babies were much more likely to die within a month of birth</li>
<li>But the mortality rate of low-birthweight babies of smokers was lower than that of non-smokers</li>
</ul>
<p>This led some to conclude that smoking was protective for low-birthweight babies.  I won't spoil the solution here, but he posted <a rel="noopener nofollow noreferrer" target="_blank" href="https://docs.google.com/presentation/d/e/2PACX-1vRfp93z9-0v4d4xbhw73zgBO-oz0QMguXrBCUI3JQsmpv2RzUDlClyRS2kZMy-EMZTCYI-38HNMrPSP/pub?start=false&amp;loop=false&amp;delayms=3000">some slides on the subject</a>.  The chapter also introduces causal diagrams, but they are only covered to the extent needed by the chapter.</p>
<p>Another chapter I particularly enjoyed was <em>Chasing the Overton Window</em>, where he explains the following phenomena:</p>
<ul>
<li>People are more likely to identify as conservative as they get older.</li>
<li>Order people hold more conservative views, on average.</li>
<li>However, as people get older, their views do not become more conservative.</li>
</ul>
<p>Again, I won't spoil it, but you can see <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2023/04/24/the-overton-paradox/">some of his analysis here</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2022/11/09/chasing-the-overton-window/">watch a talk on it here</a>.</p>
<p>Although Downey is a <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.allendowney.com/blog/2021/05/07/founded-upon-an-error/">card-carrying Bayesian</a>, the book doesn't emphasize Bayesian analysis, beyond the chapter on the base rate fallacy, which uses the <a rel="noopener nofollow noreferrer" target="_blank" href="https://allendowney.github.io/BiteSizeBayes/05_test.html">uncontroversial application of Bayes rule to diagnostic tests</a>, although I don't recall seeing the word &quot;Bayes&quot; at all.  An important point he makes is that information available before the test is important to establishing the relevant base rate, e.g. random screening implies a different base rate for a disease than someone being tested because of symptoms.  He then moves on to applying base rates to the <a rel="noopener nofollow noreferrer" target="_blank" href="https://arxiv.org/abs/1707.01195">fairness impossibility theorem</a>.</p>
<p>Another interesting chapter I'll mention was about the inspection paradox.  He gives several examples, but the most illustrative is about running a distance race.  A given runner will observe more runners who are either significantly faster or slower than themselves, but almost none who are running at about the same pace.  Not only does Downey do a nice job explaining why this occurs, but also shows how to adjust for the sampling bias that leads to the paradox.  You can see a <a rel="noopener nofollow noreferrer" target="_blank" href="http://allendowney.blogspot.com/2015/08/the-inspection-paradox-is-everywhere.html">version of the analysis here</a>.</p>
<h2 id="expanding-on-chapter-1">Expanding on Chapter 1</h2>
<p>The basic premise of chapter 1 is that no one is “normal” (defined as being within 0.3 standard deviations of the average) with respect to even a modest number of bodily measurements or personality traits.  He shows through a couple of examples that very few people are &quot;normal&quot; on numerous metrics.  One specific example uses the Big Five personality traits, which we are told are roughly independent, and pretty well approximated by Gaussian distributions.  For each trait, he defines normal as being in the middle 20-28% of the distribution.</p>
<p>He then presents data on the traits and normality within them and in combinations.  I'm combining some information from two tables here to show the percentage who are considered &quot;normal&quot; for each trait, followed by the counts and percentage of the total people who make it through each successive &quot;normal&quot; filter.</p>
<table><thead><tr><th><strong>Trait</strong></th><th style="text-align: right"><strong>Pct &quot;Normal&quot;</strong></th><th style="text-align: right"><strong>Count</strong></th><th style="text-align: right"><strong>Pct Remain</strong></th></tr></thead><tbody>
<tr><td>Extroversion</td><td style="text-align: right">23.4</td><td style="text-align: right">204,077</td><td style="text-align: right">23.4</td></tr>
<tr><td>Emotional stability</td><td style="text-align: right">20.9</td><td style="text-align: right">46,988</td><td style="text-align: right">5.4</td></tr>
<tr><td>Conscientiousness</td><td style="text-align: right">20.2</td><td style="text-align: right">10,976</td><td style="text-align: right">1.3</td></tr>
<tr><td>Agreeableness</td><td style="text-align: right">21.1</td><td style="text-align: right">2,981</td><td style="text-align: right">0.3</td></tr>
<tr><td>Openness</td><td style="text-align: right">28.3</td><td style="text-align: right">926</td><td style="text-align: right">0.1</td></tr>
</tbody></table>
<p>In other words, only 0.1% of those in the sample data are in a &quot;normal&quot; range in each trait.  There are at least two ways to explain why this is an expected result.</p>
<p>The first is <a rel="noopener nofollow noreferrer" target="_blank" href="https://juanitorduz.github.io/exploring-the-curse-of-dimensionality-part-ii./">the curse of dimensionality</a>, which offers a geometric explanation for this.  Suppose each of $d$ traits were an independent standard Gaussian distribution.  The mean squared distance from the origin for points in the distribution is $d$, which follows from the <a rel="noopener nofollow noreferrer" target="_blank" href="https://en.wikipedia.org/wiki/Chi-squared_distribution">chi-squared distribution</a>.  In other words, the more independent traits we add, the further from the mean we expect a sampled point in the distribution to be.  And more to the point, as $d$ grows, the probability increases that at least one of the draws from the independent Gaussian traits will be far away from the mean.</p>
<p>From a geometric point of view, if a point is far from the mean in any dimension, then we would say it is on the &quot;edge&quot; of the distribution.  The curse of dimensionality tells us that as $d$ grows large, eventually most of our data will be found on the edge of the data set, i.e. a value in at least one dimension for each point will be far from the mean (not &quot;normal&quot; aka &quot;weird&quot;).  And with enough dimensions, all data will be at an edge in approximately the same number of attributes.  Downey alludes to this idea when he writes, &quot;In the limit, if we consider the nearly infinite ways people vary, we find we are all equally weird,&quot; but without naming the concept directly.  So there may have been an opportunity to connect the chapter 1 lesson back to a more broadly applicable concept that readers may have heard about in other contexts of statistics or machine learning. See <a rel="noopener nofollow noreferrer" target="_blank" href="https://hastie.su.domains/ElemStatLearn/">section 2.5 of Elements of Statistical Learning</a> for more on the idea.</p>
<p>A simpler reason this result is expected is that if you multiply roughly independent probabilities between 0.2 and 0.3, you can get a small probability quickly.  And the product gets smaller faster when the events are independent.  For instance, suppose we treat each of those percent normal for each trait as probabilities of independent events and just multiply them.  We would get the results in the far right of the following table, which isn't too much different from the actual results.</p>
<table><thead><tr><th><strong>Trait</strong></th><th style="text-align: right"><strong>Pct &quot;Normal&quot;</strong></th><th style="text-align: right"><strong>Pct Remain</strong></th><th style="text-align: right"><strong>Pct if ⫫</strong></th></tr></thead><tbody>
<tr><td>Extroversion</td><td style="text-align: right">23.4</td><td style="text-align: right">23.4</td><td style="text-align: right">23.4</td></tr>
<tr><td>Emotional stability</td><td style="text-align: right">20.9</td><td style="text-align: right">5.4</td><td style="text-align: right">4.9</td></tr>
<tr><td>Conscientiousness</td><td style="text-align: right">20.2</td><td style="text-align: right">1.3</td><td style="text-align: right">1.0</td></tr>
<tr><td>Agreeableness</td><td style="text-align: right">21.1</td><td style="text-align: right">0.3</td><td style="text-align: right">0.2</td></tr>
<tr><td>Openness</td><td style="text-align: right">28.3</td><td style="text-align: right">0.1</td><td style="text-align: right">0.1</td></tr>
</tbody></table>
<p>Notice this result does not have anything to do with being &quot;normal&quot; or any concept of distance at all.  We could have been talking about the probabilities of being in the farthest left portions of the trait distributions, or simply assigned the probabilities to a set of independent Bernoulli trials.  To simplify further, let's suppose the probability of being &quot;normal&quot; for any trait is $p$.  Then the probability that an instance is normal in all $d$ independent traits is just $p^d$, which will become very small as $d$ grows large, if $p \lt 1$.  Here, we can get roughly the same result by setting $p=0.234$ for each of 5 indepdent traits.</p>
<h2 id="verdict">Verdict</h2>
<p>Overall, I recommend this book to producers and consumers of data analysis to learn a useful set of concepts through examples.  At the very least, readers will learn to ask good questions with respect to common issues in observational data analysis, and readers with the right background may learn some new methods to apply.  While it eschews the technical density of a textbook, it demands more intellectual engagement than a typical pop science book, drawing readers in with its broad scope of topics and colorful storytelling.</p>
<p>All the notebooks used to generate the analyses and plots are <a rel="noopener nofollow noreferrer" target="_blank" href="https://allendowney.github.io/ProbablyOverthinkingIt/intro.html">available here</a>, which can aid practitioners looking to apply the lessons to their own work.  You can <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.amazon.com/Probably-Overthinking-Questions-Statistical-Decisions/dp/0226822583">buy the book here</a>, and he also has a number of high quality technical books <a rel="noopener nofollow noreferrer" target="_blank" href="https://greenteapress.com/wp/">available for free here</a>.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Changing Zola&#x27;s Content Structure</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Development]]></category>
          
          
          <category><![CDATA[zola]]></category>
          
          <link>https://dfd.github.io/changing-zola-structure/</link>
          <guid>https://dfd.github.io/changing-zola-structure/</guid>
          <description><![CDATA[<p>A note for my future self on how to edit the Zola content structure.</p>
]]></description>
          <content:encoded><![CDATA[<p>A note for my future self on how to edit the Zola content structure.</p>
<span id="continue-reading"></span>
<p>This wasn't that hard, but in case I want to do it again, I'll leave a note here about how I changed the Zola content structure for this blog.</p>
<p>I was originally using the structure from <a rel="noopener nofollow noreferrer" target="_blank" href="https://deepthought-theme.netlify.app/docs/">Deep Thought</a>, but decided that I wanted the posts to show up at the root, with the former home page in an <code>About</code> section.   To do this, I had to switch the <code>index.html</code> with the <code>section.html</code> in <code>templates</code>.  Then in the <code>content</code> directory, I created an <code>about</code> directory, and dropped the original <code>_index.md</code> there, then I dumped the files from the <code>posts</code> directory in the root directory.  Then I had to edit the <code>navbar_items</code> in the <code>config.toml</code> to map the section names to their new locations.  The last part, which I didn't realize until I previewed the deployment, was that I needed to change the <code>avatar</code> from <code>config.toml</code> from <code>images/avatar.png</code> to <code>/images/avatar.png</code>.</p>
]]></content:encoded>
      </item>
      <item>
          <title>Creating this blog</title>
          <dc:creator><![CDATA[Dave]]>
        </dc:creator>
          <pubDate>Wed, 27 Dec 2023 00:00:00 +0000</pubDate>
          
          <category><![CDATA[Development]]></category>
          
          
          <category><![CDATA[zola]]></category>
          <category><![CDATA[front end]]></category>
          
          <link>https://dfd.github.io/creating-this-blog/</link>
          <guid>https://dfd.github.io/creating-this-blog/</guid>
          <description><![CDATA[<p>As is tradition, I will use the first post of my new blog to briefly describe how I set it up.</p>
]]></description>
          <content:encoded><![CDATA[<p>As is tradition, I will use the first post of my new blog to briefly describe how I set it up.</p>
<span id="continue-reading"></span><h2 id="choosing-a-framework-and-theme">Choosing a framework and theme</h2>
<p>Since I'm planning to make 2024 <em>the year of Rust</em>, I wanted to use a static site generator written in Rust, and <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.getzola.org/">Zola</a> seemed to be the best choice.  I wanted to find a theme that used the CSS framework <a rel="noopener nofollow noreferrer" target="_blank" href="https://bulma.io/">Bulma</a>, which offers the right level of customization for something like this.  I found <a rel="noopener nofollow noreferrer" target="_blank" href="https://deepthought-theme.netlify.app/docs/">DeepThought</a>, which had the high level features I wanted and seemed well-organized.  However, I thought the design was a little too busy; I wanted a simpler, flatter style, so I started hacking away at the sass/css and <a rel="noopener nofollow noreferrer" target="_blank" href="https://keats.github.io/tera/docs/">Tera templates</a> until I got something I liked well enough.</p>
<p>The templates were relatively easy to understand; they're very similar to <a rel="noopener nofollow noreferrer" target="_blank" href="https://palletsprojects.com/p/jinja/">jinja templates</a> in Python.  I hadn't worked with <a rel="noopener nofollow noreferrer" target="_blank" href="https://sass-lang.com/">sass</a> before, so that took some exploration and consultation with ChatGPT at times.</p>
<p>It was mostly a good experience, although I think there may be an <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/getzola/zola/issues/2393">issue</a> with next/previous page directions in Zola.</p>
<h2 id="styling">Styling</h2>
<p>I got rid of the 3D box shadows on each post, instead adding a backlight effect to each flat item on <code>hover</code>.  I went with a grayscale palette with just a couple of colors, adding a brown for icons and table of contents highlighting to complement the dark blue used for links.  Adobe offers a <a rel="noopener nofollow noreferrer" target="_blank" href="https://color.adobe.com/create/color-wheel">nice tool for selecting color palettes</a>.  The framework by default includes a search function, but since it's a static site, this is done by pushing a large javascript file with all the sites indexed content to the browser.  That seemed heavy, and google site searches are probably good enough.  Fully removing it was not as easy as just setting the <code>build_search_index = false</code> however; the search icon was hard coded into the navbar within the <code>base.html</code> template.  So I had to remove that, but doing so broke the <code>site.js</code> including the dark mode toggle.  So I had to remove all the javascript related to search before it worked properly.</p>
<p>One issue I couldn't figure out was how to get the DeepThought <code>navbar</code> to have a fixed position at the top.  As I described in <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/RatanShreshtha/DeepThought/issues/87">this github issue</a>, it seems to work in light mode but not dark mode, and that happened even on the out-of-the-box DeepThought content.  Hopefully that can be resolved.</p>
<p>I'm still not sold on the colors, particularly for the links; I tried a blue with lower saturation, but it became harder to distinguish the links.  I also don't know that I like the DeepThought theme's default behavior of turning hovered links back to the plain text color, so I may revisit the styling later.</p>
<p>One note is that while you can extend a theme by replacing its templates, once you start modifying the sass/css, it appears you no longer want to work with it within the theme directory.  In other words, just move the files from the theme directory into the root level equivalents.</p>
<h2 id="deployment">Deployment</h2>
<p>I had initially deployed to github pages with a different theme using shalzz's <a rel="noopener nofollow noreferrer" target="_blank" href="https://github.com/shalzz/zola-deploy-action">github action</a>, and that worked pretty well (zola 0.17.2 had some issues, but 0.18.0 worked), but when I updated to using my own theme, the build failed silently.  <a rel="noopener nofollow noreferrer" target="_blank" href="https://developers.cloudflare.com/pages/">Cloudflare pages</a> offer a better free tier with <a rel="noopener nofollow noreferrer" target="_blank" href="https://developers.cloudflare.com/pages/framework-guides/deploy-a-zola-site/">Zola support</a> and <a rel="noopener nofollow noreferrer" target="_blank" href="https://developers.cloudflare.com/pages/how-to/web-analytics/">free, privacy-first analytics</a>, although I found I needed to <a rel="noopener nofollow noreferrer" target="_blank" href="https://www.getzola.org/documentation/deployment/cloudflare-pages/#change-the-build-system-version-to-v1">change the build system back to v1</a> for it to work.</p>
<p>One odd part of the process is that you have little say in the name of the subdomain from cloudflare.  They automatically assign the name of the repo as the project name.  I think there was an opportunity to edit the project name, but it was not obvious.  So my repo name was <code>blog</code> and I was assigned <code>blog-eng</code> as the subdomain.  That's not the worst subdomain, but I would have appreciated having a choice.</p>
<p>Update: I was able to get the github actions to work for github.io.  Although I think Cloudflare has the nicer free tier, I'll stick with github.io for now.</p>
]]></content:encoded>
      </item>
    </channel>
</rss> 
